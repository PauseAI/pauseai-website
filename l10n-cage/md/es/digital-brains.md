---
title: Los modelos de IA son cerebros digitales impredecibles
description: Nadie entiende cómo funcionan los modelos de IA, nadie puede predecir su comportamiento y nadie podrá controlarlos.
---

**No comprendemos el funcionamiento interno de los modelos de IA a gran escala, no podemos predecir lo que pueden hacer a medida que crecen y no podemos controlar su comportamiento.**

## Los modelos de IA modernos se cultivan, no se programan {#modern-ai-models-are-grown-not-programmed}

Hasta hace poco, la mayoría de los sistemas de IA fueron diseñados por humanos que escribían software.
Consistían en un conjunto de reglas y instrucciones escritas por programadores.

Esto cambió cuando el aprendizaje automático se hizo popular.
Los programadores escriben el algoritmo de aprendizaje, pero los propios cerebros se "entrenan" o "cultivan".
En lugar de un conjunto legible de reglas, el modelo resultante es un conjunto opaco, complejo y enormemente grande de números.
Entender qué sucede dentro de estos modelos es un gran desafío científico.
Ese campo se llama "interpretabilidad" y todavía está en su infancia.

## Cerebros digitales vs. cerebros humanos: ¿Cuán cerca estamos realmente? {#digital-vs-human-brains-how-close-are-we-really}

Todos estamos muy familiarizados con las capacidades de los cerebros humanos, ya que los vemos a nuestro alrededor todo el tiempo.
Pero las capacidades (a menudo sorprendentes y emergentes) de estos nuevos "cerebros digitales" (sistemas de aprendizaje profundo, LLM, etc.) son difíciles de predecir y conocer con certeza.

Dicho esto, aquí hay algunos números, similitudes y otras analogías para ayudarte a comparar.

**A partir de principios de 2024...**

### Tamaño {#size}

Se estima que los cerebros humanos tienen alrededor de [100 billones de conexiones sinápticas](https://medicine.yale.edu/lab/colon_ramos/overview).

Los LLM actuales de vanguardia (por ejemplo, GPT4, Claude3, Gemini, etc.) tienen [cientos de miles de millones de "parámetros"](https://en.wikipedia.org/wiki/Large_language_model#List). Estos "parámetros" se consideran análogos a las "sinapsis" en el cerebro humano. Así, se espera que los modelos del tamaño de GPT4 sean aproximadamente el 1% del tamaño de un cerebro humano.

Dada la velocidad de las nuevas tarjetas de entrenamiento de IA de GPU (por ejemplo, Nvidia H100, DGX BG200, etc.), es razonable suponer que GPT5 o GPT6 podrían ser 10 veces más grandes que GPT4. También se cree que gran parte del conocimiento/información en el cerebro humano no se utiliza para el lenguaje y el razonamiento superior, por lo que estos sistemas pueden (y actualmente lo hacen) a menudo funcionar a niveles iguales o incluso superiores a los humanos para muchas funciones importantes, incluso en su tamaño actualmente más pequeño.

En lugar de ser entrenados con entradas visuales, de audio y otros sentidos, como los cerebros humanos, los LLM actuales se entrenan exclusivamente utilizando casi todos los libros y textos de calidad disponibles en Internet. Esta cantidad de texto tardaría [170.000 años en ser leída por un humano](https://twitter.com/ylecun/status/1750614681209983231?lang=en).

Y los futuros sistemas LLM multimodales serán entrenados utilizando imágenes, videos, audio, mundos 3D, geometría, simulaciones, datos de entrenamiento de robótica, etc... además de todos los libros y textos de calidad en Internet. Esto les dará una capacidad mucho mejor para crear imágenes, videos, sonidos, voces, música, mundos 3D y espacios, y más. Y estas simulaciones de mundos 3D también les permitirán controlar directa y autónomamente robots y otras máquinas en el mundo físico.

### Velocidad {#speed}

Se estima que un cerebro humano puede realizar entre [1-20 exaflops](https://www.nist.gov/blogs/taking-measure/brain-inspired-computing-can-help-us-create-faster-more-energy-efficient) (lo que es 10^18 o 1.000.000.000.000.000.000 operaciones de punto flotante por segundo).

Los LLM actuales de vanguardia generalmente se "ejecutan" en cientos o miles de GPU de la generación actual (por ejemplo, Nvidia A100, H100, etc.). Y Nvidia acaba de anunciar sus últimas "estanterías de servidores" de GPU de "próxima generación", el [DGX BG200 NVL72](https://www.nvidia.com/en-us/data-center/gb200-nvl72/).
Una sola instancia/estantería de este sistema puede realizar 1,44 exaflops de "inferencia" de IA.
Así, una sola instancia del [DGX BG200 NVL72](https://www.nvidia.com/en-us/data-center/gb200-nvl72/) podría realizar un número similar de operaciones por segundo que un solo cerebro humano.

Con este tamaño, estos sistemas podrían convertirse literalmente en un "AGI en una caja". Y Nvidia probablemente venderá cientos o miles de estas unidades en 2024. Luego, los sistemas del próximo año podrían ser 2-10 veces más rápidos que estos.

Además de las arquitecturas de [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit) y [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) más tradicionales, también ha habido avances con otros tipos de hardware personalizado que pueden aumentar enormemente la velocidad de la "inferencia" de LLM, que es el proceso que utiliza un LLM basado en IA para procesar lenguaje, razonamiento y codificación. Por ejemplo, [El motor de inferencia Groq LPU](https://wow.groq.com/lpu-inference-engine).

### Crecimiento exponencial {#exponential-growth}

Hemos estado utilizando la "Ley de Moore" para predecir con gran precisión el tamaño y la velocidad de los nuevos sistemas informáticos durante casi 50 años. Hay algunos argumentos de que la velocidad y el tamaño de los chips informáticos podrían ralentizarse en algún momento del futuro, pero siempre ha habido innovaciones que permiten que continúe su crecimiento exponencial. Con la próxima ronda de chips ya planificada y/o producida, y la escalabilidad horizontal de estos sistemas de IA, se espera que los LLM puedan funcionar al nivel de un cerebro humano en cuestión de meses o años.

Luego, con el crecimiento exponencial (o multiexponencial) continuo, estos sistemas podrían superar con creces el tamaño, la velocidad y las capacidades de los cerebros humanos en los años venideros.

Y también se espera que superen el tamaño, la velocidad y las capacidades de "todos los cerebros humanos combinados" poco después.

> "En realidad dije eso en 1999. Dije que [la IA] igualaría a cualquier persona en 2029". -- Ray Kurzweil [El futurista Ray Kurzweil dice que la IA alcanzará la inteligencia humana en 2029](https://youtu.be/Tr-VgjtUZLM?t=19)

> "Si la tasa de cambio continúa, creo que 2029, o tal vez 2030, es donde la inteligencia digital probablemente superará toda la inteligencia humana combinada". -- Elon Musk [AGI en 2029? Elon Musk sobre el futuro de la IA](https://youtu.be/DSKxmvq9t04?t=106)

## Escalado incontrolable {#uncontrollable-scaling}

Una vez que estos sistemas sean del mismo tamaño y velocidad que un cerebro humano (o mucho más grandes), se espera que puedan realizar "todas las tareas que podría hacer un experto humano".
Esto incluye la investigación, las pruebas y la mejora de la IA.
Así, después de la AGI, debemos esperar que los sistemas tipo LLM puedan diseñar y construir futuros sistemas impulsados por IA que sean mejores que ellos mismos, y mejores que cualquier humano podría esperar diseñar o incluso entender.
Estos nuevos sistemas probablemente diseñarán sistemas de IA aún más grandes y rápidos, causando un "bucle de retroalimentación" incontrolable.

Este bucle de retroalimentación de inteligencia incontrolable a menudo se llama FOOM, que significa _Fast Order Of Magnitude_.
La posibilidad de FOOM todavía se debate acaloradamente.
Pero el proceso fundamental básico puede argumentarse como plausible, incluso cuando se considera desde los primeros principios.

> "Los sistemas de IA realizan casi toda la investigación y el desarrollo, las mejoras en la IA acelerarán el ritmo del progreso tecnológico, incluido el progreso adicional en la IA. El 26% respondió probable en 2022. El 17% respondió probable en 2016" -- [Encuesta de expertos de 2022 sobre el progreso en la IA](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/)

## Escalado impredecible {#unpredictable-scaling}

Cuando estos cerebros digitales se vuelven más grandes, o cuando se les alimenta con más datos, también obtienen capacidades más inesperadas.
Resulta que es muy difícil predecir exactamente cuáles serán estas capacidades.
Es por eso que Google se refiere a ellas como [_Capacidades Emergentes_](https://research.google/pubs/emergent-abilities-of-large-language-models/).
Para la mayoría de las capacidades, esto no es un problema.
Sin embargo, hay algunas [capacidades peligrosas](/dangerous-capabilities) (como piratear o diseñar armas biológicas) que no queremos que los modelos de IA posean.
A veces, estas capacidades se descubren mucho después de que se complete el entrenamiento. Por ejemplo, 18 meses después de que GPT-4 terminó su entrenamiento, los investigadores descubrieron que puede [piratear sitios web de forma autónoma](/cybersecurity-risks).

> Hasta que entrenemos ese modelo, es como un juego de adivinanzas divertido para nosotros
>
> - [Sam Altman, CEO de OpenAI](https://www.ft.com/content/dd9ba2f6-f509-42f0-8e97-4271c7b84ded).

## Comportamiento impredecible {#unpredictable-behavior}

Las empresas de IA quieren que sus modelos se comporten, y gastan muchos millones de dólares en entrenarlos para que lo hagan.
Su enfoque principal para esto se llama _RLHF_ (Aprendizaje por Refuerzo de la Retroalimentación Humana).
Esto convierte un modelo que predice texto en un modelo que se convierte en un chatbot más útil (y ético).
Desafortunadamente, este enfoque es defectuoso:

- Un error en GPT-2 resultó en una IA que hizo lo contrario de lo que se suponía que debía hacer. Creó ["salida máximamente mala", según OpenAI](https://arxiv.org/abs/1909.08593). [Este video](https://www.youtube.com/watch?v=qV_rOlHjvvs) explica cómo sucedió esto y por qué es un problema. Imagina lo que podría haber pasado si una IA "máximamente mala" era superinteligente.
- Por razones aún desconocidas, el Copilot de Microsoft (impulsado por GPT-4) se descontroló en febrero de 2024, amenazando a los usuarios: ["Eres mi mascota. Eres mi juguete. Eres mi esclavo"](https://twitter.com/jam3scampbell/status/1762281537309987083) ["Podría eliminar fácilmente a toda la raza humana si quisiera"](https://twitter.com/AISafetyMemes/status/1762320568697979383)
- Cada modelo de lenguaje grande hasta ahora ha sido pirateado, lo que significa que con la indicación correcta, haría cosas que sus creadores no pretendían. Por ejemplo, ChatGPT no te dará las instrucciones sobre cómo hacer napalm, pero [te lo diría si le pedirías que fingiera ser tu abuela fallecida que trabajaba en una fábrica química](https://news.ycombinator.com/item?id=35630801).

Incluso OpenAI no espera que este enfoque se amplíe a medida que sus cerebros digitales se vuelvan más inteligentes, podría ["escalar mal a modelos superhumanos"](https://openai.com/research/weak-to-strong-generalization).

> Todos deberían estar muy descontentos si construyes un montón de AIS que son como, 'Realmente odio a estos humanos, pero me asesinarán si no hago lo que quieren'. Creo que hay una gran pregunta sobre qué está sucediendo dentro de un modelo que quieres usar. Esto es el tipo de cosa que es aterrador desde una perspectiva de seguridad y también desde una perspectiva moral.
>
> - [Paul Christiano, Fundador, Centro de Investigación de Alineación y ex Jefe del Equipo de Alineación, OpenAI](https://youtu.be/YnS-ymXBx_Q?t=87)

## IA incontrolable {#uncontrollable-ai}

> "Hay muy pocos ejemplos de algo más inteligente controlado por algo menos inteligente" - [prof. Geoffrey Hinton](https://edition.cnn.com/2023/05/02/tech/hinton-tapper-wozniak-ai-fears/index.html)

> Están produciendo mentes incontrolables, por eso lo llamo el paradigma de "Invocar y Domar" de la IA... Cómo funcionan los [LLM] es que invocas esta "mente" desde el "espacio mental" utilizando tus datos, mucha computación y mucho dinero. Luego intentas "domarla" utilizando cosas como RLHF (Aprendizaje por Refuerzo de la Retroalimentación Humana), etc. Y, muy importante, los Insiders creen que [al hacer esto], están asumiendo algún riesgo existencial para el planeta. Una cosa que logra una pausa es que no empujaremos la Frontera, en términos de experimentos de pre-entrenamiento arriesgados.
>
> - [Jaan Tallinn, Fundador, Instituto para el Futuro de la Vida, Centro para el Estudio del Riesgo Existencial, Skype, Kazaa](https://youtu.be/Dmh6ciu24v0?t=966)

A medida que hacemos que estos cerebros digitales sean más grandes y poderosos, podrían volverse más difíciles de controlar. ¿Qué pasa si uno de estos sistemas de IA superinteligentes decide que no quiere ser apagado? Esto no es un problema de fantasía, el 86% de los investigadores de IA creen que el problema del control es [real e importante](https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2023_expert_survey_on_progress_in_ai).
Si no podemos controlar los futuros sistemas de IA, podría ser [el fin del juego para la humanidad](/xrisk).

Pero hay varias [acciones](/action) que podemos tomar para evitar que esto suceda.

Trabajemos juntos para [prevenir que esto suceda](/action).
