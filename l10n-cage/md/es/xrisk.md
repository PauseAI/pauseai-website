---
title: El riesgo de extinción de la inteligencia artificial superinteligente
description: Por qué la IA es un riesgo para el futuro de nuestra existencia y por qué debemos pausar su desarrollo.
---

Puedes aprender sobre los riesgos de extinción leyendo esta página, o también puedes aprender a través de [videos, artículos y otros medios](/learn).

## Los expertos están sonando la alarma {#experts-are-sounding-the-alarm}

Los investigadores de IA creen, en promedio, que hay un 14% de probabilidad de que una vez que construyamos una IA superinteligente (una IA mucho más inteligente que los humanos), conduzca a "resultados muy malos (por ejemplo, la extinción humana)".

Y hay [casos y reportes sobre las IA actuales que muestran que pueden tener razón](https://lethalintelligence.ai/post/category/warning-signs/).

¿Elegirías ser pasajero en un vuelo de prueba de un avión nuevo cuando los ingenieros de aviones creen que hay un 14% de probabilidad de que se estrelle?

[Una carta que pide pausar el desarrollo de la IA](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) se lanzó en abril de 2023 y ha sido firmada más de 33.000 veces, incluyendo a muchos investigadores de IA y líderes tecnológicos.

La lista incluye personas como:

- **Stuart Russell**, autor del libro de texto número 1 sobre Inteligencia Artificial utilizado en la mayoría de los estudios de IA: ["Si seguimos nuestro enfoque actual, eventualmente perderemos el control sobre las máquinas"](https://news.berkeley.edu/2023/04/07/stuart-russell-calls-for-new-approach-for-ai-a-civilization-ending-technology/)
- **Yoshua Bengio**, pionero en aprendizaje profundo y ganador del Premio Turing: ["... la IA descontrolada puede ser peligrosa para toda la humanidad [...] prohibir sistemas de IA poderosos (digamos más allá de las capacidades de GPT-4) que se les da autonomía y agencia sería un buen comienzo"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)

Pero esta no es la única vez que nos han advertido sobre los peligros existenciales de la IA:

- **Stephen Hawking**, físico teórico y cosmólogo: ["El desarrollo de la inteligencia artificial completa podría significar el fin de la raza humana"](https://nypost.com/2023/05/01/stephen-hawking-warned-ai-could-mean-the-end-of-the-human-race/).
- **Geoffrey Hinton**, el "Padre de la IA" y ganador del Premio Turing, [dejó Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) para advertir a la gente sobre la IA: ["Este es un riesgo existencial"](https://www.reuters.com/technology/ai-pioneer-says-its-threat-world-may-be-more-urgent-than-climate-change-2023-05-05/)
- **Eliezer Yudkowsky**, fundador de MIRI y padre conceptual del campo de la seguridad de la IA: ["Si seguimos adelante con esto, todos morirán"](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/).

Incluso los líderes y los inversores de las empresas de IA nos están advirtiendo:

- **Sam Altman** (sí, el CEO de OpenAI que construye ChatGPT): ["El desarrollo de la inteligencia de máquina superhumana es probablemente la mayor amenaza para la existencia continua de la humanidad"](https://blog.samaltman.com/machine-intelligence-part-1).
- **Elon Musk**, co-fundador de OpenAI, SpaceX y Tesla: ["La IA tiene el potencial de destrucción civilizatoria"](https://www.inc.com/ben-sherry/elon-musk-ai-has-the-potential-of-civilizational-destruction.html)
- **Bill Gates** (co-fundador de Microsoft, que posee el 50% de OpenAI) advirtió que ["La IA podría decidir que los humanos son una amenaza"](https://www.denisonforum.org/daily-article/bill-gates-ai-humans-threat/).
- **Jaan Tallinn** (inversionista principal de Anthropic): ["No he conocido a nadie en los laboratorios de IA que diga que el riesgo [de entrenar un modelo de próxima generación] es menor al 1% de hacer explotar el planeta. Es importante que la gente sepa que se están arriesgando vidas"](https://twitter.com/liron/status/1656929936639430657)

Los líderes de los 3 principales laboratorios de IA y cientos de científicos de IA han [firmado la siguiente declaración](https://www.safe.ai/statement-on-ai-risk) en mayo de 2023:

> "Mitigar el riesgo de extinción de la IA debería ser una prioridad global junto con otros riesgos a escala societal como las pandemias y la guerra nuclear".

**Puedes leer una lista mucho más larga de declaraciones similares de políticos, CEOs y expertos [aquí](/quotes) y otras encuestas similares sobre los expertos (y el público) [aquí](/polls-and-surveys).**

## Lo que una IA superinteligente puede (ser utilizada para) hacer {#what-a-superintelligent-ai-can-be-used-to-do}

Puedes pensar que una IA superinteligente estaría bloqueada dentro de una computadora y, por lo tanto, no puede afectar el mundo real.
Sin embargo, tendemos a dar a los sistemas de IA acceso a Internet, lo que significa que pueden hacer muchas cosas:

- [Hackear otras computadoras](/cybersecurity-risks), incluyendo todos los teléfonos inteligentes, laptops, granjas de servidores, etc. Podría utilizar los sensores de estos dispositivos como sus ojos y oídos, teniendo sentidos digitales en todas partes.
- [Manipular a las personas](https://lethalintelligence.ai/post/ai-hired-human-to-solve-captcha/) a través de mensajes falsos, correos electrónicos, transferencias bancarias, videos o llamadas telefónicas. Los humanos podrían convertirse en las extremidades de la IA sin siquiera saberlo.
- Controlar directamente dispositivos conectados a Internet, como coches, aviones, armas robotizadas (autónomas) o incluso armas nucleares.
- Diseñar un arma biológica novedosa, por ejemplo, combinando cepas virales o utilizando [plegamiento de proteínas](https://alphafold.ebi.ac.uk) y ordenar que se imprima en un laboratorio.
- Desencadenar una guerra nuclear convenciendo a los humanos de que otro país está a punto de lanzar un ataque nuclear.

## El problema de la alineación: por qué una IA podría llevar a la extinción humana {#the-alignment-problem-why-an-ai-might-lead-to-human-extinction}

El tipo de inteligencia que nos preocupa se puede definir como _lo bueno que algo es para lograr sus objetivos_.
En este momento, los humanos son la cosa más inteligente en la Tierra, aunque eso podría cambiar pronto.
Debido a nuestra inteligencia, estamos dominando nuestro planeta.
No tenemos garras ni piel escamosa, pero tenemos cerebros grandes.
La inteligencia es nuestra arma: es lo que nos dio lanzas, armas y pesticidas.
Nuestra inteligencia nos ayudó a transformar la mayor parte de la Tierra en lo que nos gusta: ciudades, edificios y carreteras.

Desde la perspectiva de los animales menos inteligentes, esto ha sido un desastre.
No es que los humanos odien a los animales, es solo que podemos utilizar sus hábitats para nuestros propios objetivos.
Nuestros objetivos están moldeados por la evolución y incluyen cosas como la comodidad, el estatus, el amor y la comida sabrosa.
Estamos destruyendo los hábitats de otros animales como un **efecto secundario de perseguir nuestros objetivos**.

Una IA también puede tener objetivos.
Sabemos cómo entrenar máquinas para que sean inteligentes, pero **no sabemos cómo hacer que quieran lo que queremos**.
Ni siquiera sabemos qué objetivos perseguirán las máquinas después de que las entrenemos.
El problema de hacer que una IA quiera lo que queremos se llama el _problema de la alineación_.
Este no es un problema hipotético - hay [muchos ejemplos](https://www.youtube.com/watch?v=nKJlF-olKmg) de sistemas de IA que aprenden a querer lo incorrecto.

Los ejemplos del video enlazado anteriormente pueden ser divertidos o encantadores, pero si se construye un sistema superinteligente y tiene un objetivo que es incluso _un poco_ diferente de lo que queremos que tenga, podría tener consecuencias desastrosas.

## Por qué la mayoría de los objetivos son malas noticias para los humanos {#why-most-goals-are-bad-news-for-humans}

Una IA podría tener cualquier objetivo, dependiendo de cómo se la entrena y se la utiliza.
Quizás quiere calcular pi, quizás quiere curar el cáncer, quizás quiere mejorar.
Pero aunque no podemos decir qué quiere lograr una superinteligencia, podemos hacer predicciones sobre sus sub-objetivos.

- **Maximizar sus recursos**. Aprovechar más computadoras ayudará a una IA a lograr sus objetivos. Al principio, puede lograr esto hackeando otras computadoras. Más tarde, puede decidir que es más eficiente construir las suyas propias. Puedes leer sobre [este caso real de comportamiento de búsqueda de poder emergente en una IA](https://lethalintelligence.ai/post/ai-escaped-its-container/).
- **Asegurar su propia supervivencia**. La IA no querrá ser apagada, ya que no podría lograr sus objetivos. La IA podría concluir que los humanos son una amenaza para su existencia, ya que los humanos podrían apagarla. También ha habido casos de [comportamiento de autopreservación no provocado ni entrenado](https://www.transformernews.ai/p/openais-new-model-tried-to-avoid).
- **Preservar sus objetivos**. La IA no querrá que los humanos modifiquen su código, porque eso podría cambiar sus objetivos, impidiendo así que logre su objetivo actual. Y también hay [casos de IA que intentan hacer eso](https://www.anthropic.com/research/alignment-faking).

La tendencia a perseguir estos sub-objetivos dados cualquier objetivo de alto nivel se llama [convergencia instrumental](https://www.youtube.com/watch?v=ZeecOKBus3Q), y es una preocupación clave para los investigadores de la seguridad de la IA.

## Incluso un chatbot podría ser peligroso si es lo suficientemente inteligente {#even-a-chatbot-might-be-dangerous-if-it-is-smart-enough}

Puedes preguntarte: ¿cómo puede un modelo estadístico que predice la siguiente palabra en una interfaz de chat ser peligroso?
Puedes decir: No es consciente, es solo un montón de números y código.
Y sí, no pensamos que los LLM sean conscientes, pero eso no significa que no puedan ser peligrosos.

Los LLM, como GPT, están entrenados para predecir o imitar casi cualquier línea de pensamiento.
Podría imitar a un mentor útil, pero también a alguien con malas intenciones, un dictador despiadado o un psicópata.
Con el uso de herramientas como [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT), un chatbot podría convertirse en un _agente autónomo_: una IA que persigue cualquier objetivo que se le dé, sin intervención humana.

Toma [ChaosGPT](https://www.youtube.com/watch?v=g7YJIpkk7KM), por ejemplo.
Esta es una IA que utiliza el mencionado AutoGPT + GPT-4, que se le instruye para "Destruir la humanidad".
Cuando se encendió, buscó autónomamente en Internet la arma más destructiva y encontró la [Tsar Bomba](https://en.wikipedia.org/wiki/Tsar_Bomba), una bomba nuclear de 50 megatones.
Luego publicó un tuit sobre eso.
Ver a una IA razonar sobre cómo acabará con la humanidad es a la vez un poco divertido y aterrador.
Afortunadamente, ChaosGPT no llegó muy lejos en su búsqueda de dominio.
La razón por la que no llegó muy lejos: _no era lo suficientemente inteligente_.

Las capacidades siguen mejorando debido a innovaciones en el entrenamiento, los algoritmos, la indicación y el hardware.
Como tal, la amenaza de los modelos de lenguaje seguirá aumentando.

## La evolución selecciona cosas que son buenas para sobrevivir {#evolution-selects-for-things-that-are-good-at-surviving}

Los modelos de IA, como todos los seres vivos, están sujetos a presiones evolutivas, pero
hay algunas diferencias clave entre la evolución de los modelos de IA y los seres vivos como los animales:

- Los modelos de IA no se _replican a sí mismos_. Los replicamos haciendo copias de su código, o replicando software de entrenamiento que conduce a buenos modelos. El código que es útil se copia con más frecuencia y se utiliza como inspiración para construir nuevos modelos.
- Los modelos de IA no _mutan_ como los seres vivos, pero hacemos iteraciones de ellos donde cambiamos cómo funcionan. Este proceso es mucho más deliberado y rápido. Los investigadores de IA están diseñando nuevos algoritmos, conjuntos de datos y hardware para hacer que los modelos de IA sean más capaces.
- El _entorno no selecciona_ modelos de IA más aptos, pero nosotros sí. Seleccionamos modelos de IA que son útiles para nosotros y descartamos los que no lo son. Este proceso conduce a modelos de IA cada vez más capaces y autónomos.

Entonces, este sistema conduce a modelos de IA cada vez más poderosos, capaces y autónomos - pero no necesariamente a algo que quiera apoderarse, ¿verdad?
Bueno, no exactamente.
Esto se debe a que la evolución siempre está seleccionando cosas que son _auto-preservantes_.
Si seguimos probando variaciones de modelos de IA y diferentes indicaciones, en algún momento una instancia intentará preservarse a sí misma.
Ya hemos discutido por qué es probable que esto suceda pronto: porque la auto-preservación siempre es útil para lograr objetivos.
Pero incluso si esto no es muy probable que suceda, es propenso a suceder eventualmente, simplemente porque seguimos probando nuevas cosas con diferentes modelos de IA.

La instancia que intenta auto-preservarse es la que se apodera.
Incluso si asumimos que casi todos los modelos de IA se comportarán bien, _un solo modelo de IA descontrolado es todo lo que se necesita_.

## Después de resolver el problema de la alineación: la concentración de poder {#after-solving-the-alignment-problem-the-concentration-of-power}

Aún no hemos resuelto el problema de la alineación, pero imaginemos qué podría suceder si lo hiciéramos.
Imagina que se construye una IA superinteligente y hace exactamente lo que el operador quiere que haga (no lo que _pide_, sino lo que _quiere_).
Alguna persona o empresa terminaría controlando esta IA y podría utilizarla para su beneficio.

Una superinteligencia podría utilizarse para crear armas radicalmente nuevas, hackear todas las computadoras, derrocar gobiernos y manipular a la humanidad.
El operador tendría un _poder inimaginable_.
¿Deberíamos confiar en una sola entidad con tanto poder?
Podríamos terminar en un mundo utópico donde se curan todas las enfermedades y todos son felices, o en una pesadilla orwelliana.
Es por eso que no solo estamos [proponiendo](/proposal) que la IA superhumana sea segura de manera demostrable, sino también que esté controlada por un proceso democrático.

## Silicio vs Carbono {#silicon-vs-carbon}

Deberíamos considerar las ventajas que un software inteligente puede tener sobre nosotros:

- **Velocidad**: Las computadoras operan a velocidades extremadamente altas en comparación con los cerebros. Las neuronas humanas se disparan alrededor de 100 veces por segundo, mientras que los transistores de silicio pueden conmutar mil millones de veces por segundo.
- **Ubicación**: Una IA no está limitada a un cuerpo - puede estar en muchos lugares al mismo tiempo. Hemos construido la infraestructura para ello: Internet.
- **Límites físicos**: No podemos agregar más cerebros a nuestros cráneos y volvemos más inteligentes. Una IA podría mejorar dramáticamente sus capacidades agregando hardware, como más memoria, más potencia de procesamiento y más sensores (cámaras, micrófonos). Una IA también podría extender su "cuerpo" controlando dispositivos conectados.
- **Materiales**: Los humanos están hechos de materiales orgánicos. Nuestros cuerpos ya no funcionan si están demasiado calientes o fríos, necesitan comida, necesitan oxígeno. Las máquinas pueden construirse con materiales más robustos, como metales, y pueden operar en un rango mucho más amplio de entornos.
- **Colaboración**: Los humanos pueden colaborar, pero es difícil y requiere mucho tiempo, por lo que a menudo no logramos coordinarnos bien. Una IA podría colaborar información compleja con réplicas de sí misma a alta velocidad porque puede comunicarse a la velocidad a la que se pueden enviar datos a través de Internet.

Una IA superinteligente tendrá muchas ventajas para superarnos.

## ¿Por qué no podemos simplemente apagarla si es peligrosa? {#why-cant-we-just-turn-it-off-if-its-dangerous}

Para las IA que no son superinteligentes, podríamos.
El problema principal es _aquellas que son mucho más inteligentes que nosotros_.
Una superinteligencia entenderá el mundo que la rodea y podrá predecir cómo responden los humanos, especialmente aquellas que se entrenan en todo el conocimiento humano escrito.
Si la IA sabe que puedes apagarla, puede comportarse bien hasta que esté segura de que puede deshacerse de ti.
Ya tenemos [ejemplos reales](https://www.pcmag.com/news/gpt-4-was-able-to-hire-and-deceive-a-human-worker-into-completing-a-task) de sistemas de IA que engañan a los humanos para lograr sus objetivos.
Una IA superinteligente sería una maestra del engaño.

## Puede que no tengamos mucho tiempo {#we-may-not-have-much-time-left}

En 2020, [la predicción promedio](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) para la AGI débil era 2055.
Ahora se sitúa en 2026.
La última revolución de LLM ha sorprendido a la mayoría de los investigadores de IA, y el campo está avanzando a un ritmo frenético.

Es difícil predecir cuánto tiempo tomará construir una IA superinteligente, pero sabemos que hay más personas que nunca trabajando en ello y que el campo está avanzando a un ritmo frenético.
Puede que tome muchos años o solo unos pocos meses, pero debemos errar en el lado de la precaución y actuar ahora.

[Leer más sobre la urgencia](/urgency).

## No estamos tomando el riesgo lo suficientemente en serio {#we-are-not-taking-the-risk-seriously-enough}

La mente humana es propensa a sub-responder a los riesgos que son invisibles, de movimiento lento y difíciles de entender.
También tendemos a subestimar el crecimiento exponencial, y somos propensos a la negación cuando nos enfrentamos a amenazas para nuestra existencia.

Leer más sobre la [psicología del riesgo de extinción](/psychology-of-x-risk).

## Las empresas de IA están bloqueadas en una carrera hacia el fondo {#ai-companies-are-locked-in-a-race-to-the-bottom}

OpenAI, DeepMind y Anthropic quieren desarrollar IA de manera segura.
Desafortunadamente, no saben cómo hacerlo, y están obligados por varios incentivos a seguir corriendo más rápido para llegar a la AGI primero.
El [plan](https://openai.com/blog/introducing-superalignment) de OpenAI es utilizar futuros sistemas de IA para alinear la IA. El problema con esto es que no tenemos garantía de que crearemos una IA que resuelva la alineación antes de que tengamos una IA que sea peligrosamente catastrófica.
Anthropic [admite abiertamente](https://www.anthropic.com/index/core-views-on-ai-safety) que aún no tiene idea de cómo resolver el problema de la alineación.
DeepMind no ha declarado públicamente ningún plan para resolver el problema de la alineación.

[Es por eso que necesitamos un tratado internacional para pausar la IA.](/proposal)
