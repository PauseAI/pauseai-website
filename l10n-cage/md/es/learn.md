---
title: Entiende por qué la seguridad de la IA es importante
description: Recursos educativos (videos, artículos, libros) sobre riesgos de IA y alineación de IA
---

<script>
import NewsletterSignup from '$lib/components/NewsletterSignup.svelte';
import IconBullets from '$lib/components/IconBullets.svelte'
import * as m from '$lib/paraglide/messages'
import { CirclePause, CircleQuestionMark, Cog, Earth, Footprints, MessageSquareQuote, Skull, TriangleAlert } from 'lucide-svelte';
</script>

<NewsletterSignup />

## En este sitio web {#on-this-website}

<IconBullets items={[
[TriangleAlert, m.learn_risks],
[Skull, m.learn_xrisk],
[Earth, m.learn_ai_takeover],
[MessageSquareQuote, m.learn_quotes],
[Cog, m.learn_feasibility],
[CirclePause, m.learn_building_the_pause_button],
[CircleQuestionMark, m.learn_faq],
[Footprints, m.learn_action],
]} />

## Otros sitios web {#other-websites}

- [El Compendio](https://www.thecompendium.ai/). Un conjunto exhaustivo de conocimientos sobre por qué la carrera actual de IA es tan peligrosa y qué podemos hacer al respecto.
- [Un camino estrecho](https://www.narrowpath.co/). Un plan detallado sobre los pasos que debemos dar para aumentar nuestras posibilidades de sobrevivir en las próximas décadas.
- [Mantén el futuro humano](https://keepthefuturehuman.ai/). Un artículo de Anthony Aguirre de FLI sobre por qué y cómo podemos mantener el futuro humano. (revisa [capítulo 8: Cómo no construir AGI](https://keepthefuturehuman.ai/chapter-8-how-to-not-build-agi/))
- [AISafety.com](https://www.aisafety.com) y [AISafety.info](https://aisafety.info). Las páginas de aterrizaje para la seguridad de la IA. Aprende sobre los riesgos, comunidades, eventos, trabajos, cursos, ideas para mitigar los riesgos y más.
- [Seguridad existencial](https://existentialsafety.org/). Una lista completa de acciones que podemos tomar para aumentar nuestra seguridad existencial frente a la IA.
- [AISafety.dance](https://aisafety.dance). Una introducción divertida y accesible a los riesgos catastróficos de la IA.
- [AISafety.world](https://aisafety.world/tiles/). Un panorama completo de la seguridad de la IA con todas las organizaciones, medios de comunicación, foros, blogs y otros actores y recursos.
- [Base de datos de incidentes de IA](https://incidentdatabase.ai/). Base de datos de incidentes en los que los sistemas de IA causaron daño.

- [LethalIntelligence.ai](https://lethalintelligence.ai/). Una colección de recursos sobre riesgos de IA y alineación de IA.

## Boletines {#newsletters}

- [PauseAI Substack](https://pauseai.substack.com/): Nuestro boletín.
- [TransformerNews](https://www.transformernews.ai/) Boletín semanal completo sobre seguridad y gobernanza de la IA.
- [No te preocupes por el jarrón](https://thezvi.substack.com/): Un boletín sobre seguridad de la IA, racionalidad y otros temas.

## Videos {#videos}

- [Lista de reproducción de PauseAI](https://www.youtube.com/playlist?list=PLI46NoubGtIJa0JVCBR-9CayxCOmU0EJt) es una lista de reproducción de YouTube que compilamos, con videos que van desde 1 minuto hasta 1 hora en diferentes formatos y de diversas fuentes, y no requiere conocimientos previos.
- [YouTube de Robert Miles](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40) es un excelente lugar para comenzar a entender los fundamentos de la alineación de la IA.
- [YouTube de LethalIntelligence](https://www.youtube.com/channel/UCLwop3J1O7wL-PNWGjQw8fg)

## Podcasts {#podcasts}

- [DoomDebates](https://www.youtube.com/@DoomDebates) de Liron Shapira, completamente enfocado en el destino de la IA.
- [For Humanity Podcast](https://www.youtube.com/@ForHumanityPodcast) del ex presentador de noticias John Sherman.
- [Future of Life Institute | Connor Leahy sobre seguridad de la IA y por qué el mundo es frágil](https://youtu.be/cSL3Zau1X8g?si=0X3EKoxZ80_HN9Rl&t=1803). Entrevista con Connor sobre las estrategias de seguridad de la IA.
- [Lex Fridman | Max Tegmark: El caso para detener el desarrollo de la IA](https://youtu.be/VcVfceTsD0A?t=1547). Entrevista que profundiza en los detalles de nuestra situación actual peligrosa.
- [Sam Harris | Eliezer Yudkowsky: IA, corriendo hacia el borde](https://samharris.org/episode/SE60B0CF4B8). Conversación sobre la naturaleza de la inteligencia, diferentes tipos de IA, el problema de la alineación, Es vs Debe, y más. Uno de los muchos episodios que Making Sense tiene sobre seguridad de la IA.
- [Connor Leahy, alarma de incendio de IA](https://youtu.be/pGjyiqJZPJo?t=2510). Charla sobre la explosión de inteligencia y por qué sería lo más importante que podría suceder.
- [Episodios recomendados del podcast de 80,000 horas sobre IA](https://80000hours.org/podcast/on-artificial-intelligence/). No 80k horas de duración, sino una recopilación de episodios del podcast de 80,000 horas sobre seguridad de la IA.
- [Episodios del podcast del Future of Life Institute sobre IA](https://futureoflife.org/podcast/?_category_browser=ai). Todos los episodios del podcast de FLI sobre el futuro de la inteligencia artificial.

Los podcasts que presentan a miembros de PauseAI se pueden encontrar en la [lista de cobertura de medios](/press).

## Artículos {#articles}

- [El pensamiento que podría condenarnos con la IA](https://time.com/6273743/thinking-that-could-doom-us-with-ai/) (de Max Tegmark)
- [Pausar el desarrollo de la IA no es suficiente. Debemos cerrarlo todo](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) (de Eliezer Yudkowsky)
- [El caso para ralentizar la IA](https://www.vox.com/the-highlight/23621198/artificial-intelligence-chatgpt-openai-existential-risk-china-ai-safety-technology) (de Sigal Samuel)
- [La revolución de la IA: El camino hacia la superinteligencia](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) (de WaitButWhy)
- [Cómo pueden surgir las IA deshonestas](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) (de Yoshua Bengio)

- [Razonar a través de argumentos en contra de tomar en serio la seguridad de la IA](https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/) (de Yoshua Bengio)
- [El Compendio](https://www.thecompendium.ai/)
- [Un camino estrecho](https://www.narrowpath.co/)
- [Mantén el futuro humano](https://keepthefuturehuman.ai/)
- Revisa la sección de Lectura en [LethalIntelligence.ai](https://lethalintelligence.ai/reading-time/)

Si deseas leer lo que los periodistas han escrito sobre PauseAI, revisa la lista de [cobertura de medios](/press).

## Libros {#books}

- [Si alguien lo construye, todos mueren](https://ifanyonebuildsit.com/) (Eliezer Yudkowsky y Nate Soares, 2025)
- [Incontrolable: La amenaza de la superinteligencia artificial y la carrera para salvar el mundo](https://www.goodreads.com/book/show/202416160-uncontrollable) (Darren McKee, 2023). ¡Obténlo [gratis](https://impactbooks.store/cart/47288196366640:1?discount=UNCON-P3SFRS)!

- [El problema de la alineación](https://www.goodreads.com/book/show/50489349-the-alignment-problem) (Brian Christian, 2020)
- [Compatible con humanos: Inteligencia artificial y el problema del control](https://www.goodreads.com/en/book/show/44767248) (Stuart Russell, 2019)
- [Vida 3.0: Ser humano en la era de la inteligencia artificial](https://www.goodreads.com/en/book/show/34272565) (Max Tegmark, 2017)
- [Superinteligencia: Caminos, peligros, estrategias](https://www.goodreads.com/en/book/show/20527133) (Nick Bostrom, 2014)

## Documentos {#papers}

- [Una recopilación](https://arkose.org/aisafety) de documentos sobre seguridad de la IA
- [Otra recopilación](https://futureoflife.org/resource/introductory-resources-on-ai-risks/#toc-44245428-2) de documentos sobre seguridad de la IA
- [Falsificación de alineación en grandes modelos de lenguaje](https://www.anthropic.com/news/alignment-faking) documento reciente de Anthropic
- [Gestión de riesgos extremos de IA en medio de un progreso rápido](https://www.science.org/doi/abs/10.1126/science.adn0117) de los padrinos del campo

## Cursos {#courses}

- [Introducción a la IA transformadora](https://aisafetyfundamentals.com/intro-to-tai/) (15 horas)
- [Fundamentos de seguridad de AGI](https://www.agisafetyfundamentals.com/) (30 horas)
- [Bibliografía de materiales recomendados de CHAI](https://humancompatible.ai/bibliography) (50 horas+)
- [AISafety.training](https://aisafety.training/): Visión general de programas de capacitación, conferencias y otros eventos

## Organizaciones {#organizations}

- [Future of Life Institute](https://futureoflife.org/cause-area/artificial-intelligence/) inició la [carta abierta](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), liderada por Max Tegmark.
- [Centro para la seguridad de la IA](https://www.safe.ai/) (CAIS) es un centro de investigación en la Universidad Técnica Checa en Praga, liderado por Dan Hendrycks.
- [Conjecture](https://www.conjecture.dev/). Start-up que trabaja en la alineación de la IA y la política de IA, liderada por Connor Leahy.
- [Observatorio de Riesgo Existencial](https://existentialriskobservatory.org/). Organización holandesa que informa al público sobre riesgos existenciales y estudia estrategias de comunicación.
- [Centro para la gobernanza de la IA](https://www.governance.ai/)
- [FutureSociety](https://thefuturesociety.org/about-us/)
- [Centro para la inteligencia artificial compatible con humanos](https://humancompatible.ai/about/) (CHAI), liderado por Stuart Russell.
- [Instituto de Investigación de Inteligencia de Máquina](https://intelligence.org/) (MIRI), haciendo investigación matemática sobre seguridad de la IA, liderado por Eliezer Yudkowsky.
- [Instituto de Política y Estrategia de IA](https://www.iaps.ai/) (IAPS)
- [El Instituto de Política de IA](https://theaipi.org/)
- [Centro de Comunicaciones de Seguridad de la IA](https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/)
- [El Proyecto Midas](https://www.themidasproject.com/) Campañas de presión corporativa para la seguridad de la IA.
- [El Proyecto de Supervivencia Humana](https://thehumansurvivalproject.org/)
- [Mundo de seguridad de la IA](https://aisafety.world/) Aquí hay una visión general del panorama de la seguridad de la IA.

## Si estás convencido y deseas tomar medidas {#if-you-are-convinced-and-want-to-take-action}

Hay muchas [cosas que puedes hacer](/action).
Escribir una carta, ir a una protesta, donar algo de dinero o unirse a una comunidad no es tan difícil.
Y estas acciones tienen un impacto real.
Incluso cuando enfrentamos el fin del mundo, todavía puede haber esperanza y un trabajo muy gratificante que hacer.

## O si todavía no te sientes seguro {#or-if-you-still-dont-feel-quite-sure-of-it}

Aprender sobre la [psicología del riesgo existencial](/psychology-of-x-risk) podría ayudarte.
