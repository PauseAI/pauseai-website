---
title: Por qué una toma del control por IA podría ser muy probable
description: A medida que la IA supera las capacidades humanas, la probabilidad de una toma del control por IA se vuelve muy alta.
---

Una de las preocupaciones de los científicos de IA es que una superinteligencia podría tomar el control de nuestro planeta.
Puedes verlo en [artículos](/learn#papers), [encuestas](/polls-and-surveys) y [predicciones](/pdoom) & [declaraciones](/quotes) individuales.
Esto no significa necesariamente que todos mueran, pero sí significa que (casi) todos los humanos perderán el control sobre nuestro futuro.

Discutimos los conceptos básicos del riesgo existencial principalmente en [otro artículo](/xrisk).
En este artículo, argumentaremos que este riesgo de toma del control no solo es real, sino que es muy probable que ocurra _si desarrollamos una superinteligencia_.

## El argumento {#the-argument}

- Es probable que exista una Superinteligencia Agente (SA) en un futuro cercano.
- Algún ejemplo de la SA intentará una toma del control.
- Un intento de toma del control por parte de una SA es probable que tenga éxito.
- Una toma del control exitosa es permanente.
- Una toma del control probablemente sea mala para la mayoría de los humanos.

## Una Superinteligencia Agente es probable que exista en el futuro cercano {#an-agentic-superintelligence-is-likely-to-exist-in-the-near-future}

Una Superinteligencia (SI) es un tipo de IA que tiene capacidades que superan a las de todos los humanos en prácticamente todos los campos.
Algunos [modelos de IA de última generación](/sota) ya tienen capacidades superhumanas en ciertos campos, pero ninguno de ellos supera a todos los humanos en un amplio rango de tareas.
A medida que las capacidades de la IA mejoran debido a innovaciones en arquitecturas de entrenamiento, entornos de ejecución y mayor escala, podemos esperar que una IA eventualmente supere a los humanos en prácticamente todos los campos.

No todos los sistemas de IA son agentes.
Un agente es una entidad capaz de tomar decisiones y realizar acciones para lograr un objetivo.
Un gran modelo de lenguaje, por ejemplo, no persigue ningún objetivo por sí solo.
Sin embargo, los entornos de ejecución pueden convertir fácilmente una IA no agente en una IA agente.
Un ejemplo de esto es AutoGPT, que permite recursivamente que un modelo de lenguaje genere su próxima entrada.
Si una SI persigue un objetivo en el mundo real, la llamamos Superinteligencia Agente (SA).
Dado que ya podemos convertir una IA no agente en una IA agente, podemos esperar que una SA exista poco después de que exista una SI.

Es prácticamente imposible predecir con precisión cuándo existirá la SA.
Podría tomar décadas, podría [suceder el próximo mes](/urgency).
Debemos actuar como si fuera a suceder pronto, porque las consecuencias de equivocarnos son tan graves.

## Algún ejemplo de la SA intentará una toma del control {#some-instance-of-the-asi-will-attempt-a-takeover}

En un intento de toma del control, una SA tomará medidas para maximizar su control sobre el mundo.
Un intento de toma del control podría ocurrir por al menos dos razones:

1. Porque una IA está explícitamente instruida para hacerlo.
2. Como un sub-objetivo de otro objetivo.

Esta primera razón probablemente ocurrirá en algún momento si esperamos lo suficiente, pero la segunda razón es bastante probable que ocurra accidentalmente, incluso poco después de la creación de una SA.

El sub-objetivo de _maximizar el control_ sobre el mundo podría ocurrir debido a la _convergencia instrumental_: la tendencia de los sub-objetivos a converger en la adquisición de poder, la auto-preservación y la adquisición de recursos:

- Cuanto más control tengas, más difícil será para cualquier otro agente evitar que logres tu objetivo.
- Cuanto más control tengas, más recursos tendrás para lograr tu objetivo. (Por ejemplo, una IA encargada de calcular pi podría concluir que sería beneficioso usar todas las computadoras del mundo para calcular pi).

Ya hay [pruebas](https://www.anthropic.com/research/alignment-faking)[s](https://www.transformernews.ai/p/openais-new-model-tried-to-avoid) de que las IA desarrollan un comportamiento similar.

No todos los ejemplos de una SA intentarán necesariamente una toma del control.
La idea importante es que **solo tiene que ocurrir una vez**.

Un mundo que aún no ha sido tomado, pero que tiene una SA que _podría_ tomar el control, está en una condición fundamentalmente inestable.
De manera similar, un país sin gobierno está en una condición fundamentalmente inestable.
No es una cuestión de _si_ un intento de toma del control ocurrirá, sino _cuándo_ ocurrirá.

El proceso de tomar el control puede implicar piratear prácticamente todos los sistemas conectados a Internet, [manipular a las personas](https://lethalintelligence.ai/post/ai-hired-human-to-solve-captcha/), y controlar recursos físicos.
Un intento de toma del control es exitoso cuando la SA tiene control sobre prácticamente todos los aspectos de nuestro mundo.
Esto podría ser un proceso lento, donde la SA gana gradualmente más y más control a lo largo de meses, o podría ser un proceso repentino.
La velocidad a la que ocurre un intento de toma del control dependerá de las capacidades de la SA.

Cuando una SA tiene control sobre el mundo, puede evitar que otras SA tomen el control.
Una toma del control, por lo tanto, solo puede ocurrir una vez.
Una SA racional, por lo tanto, intentará una toma del control tan pronto como sea capaz de hacerlo.
Es probable que la primera SA capaz de hacerlo intente una toma del control.

## Un intento de toma del control por parte de una SA es probable que tenga éxito {#a-takeover-attempt-by-an-asi-is-likely-to-succeed}

Para un humano, hacer una toma del control es una tarea casi imposible.
Ninguna persona ha logrado tomar el control de todo el mundo.
Algunos dictadores se acercaron, pero nunca tuvieron control sobre todo.

Una IA superinteligente tiene ciertas ventajas importantes sobre los humanos (además de ser superinteligente) que hacen que un intento de toma del control sea mucho más probable que tenga éxito.

1. **Velocidad**. El cerebro humano funciona a 1-100hz, mientras que los chips de computadora pueden funcionar a velocidades de reloj en el rango de GHz. Un solo modelo de IA puede leer libros enteros en segundos.
1. **Consumo de energía**. Los humanos están limitados por la cantidad de comida que pueden comer y la cantidad de energía que pueden almacenar en sus cuerpos. La IA puede conectarse a la red eléctrica y usar tanta energía como necesite.
1. **Paralelismo**. Un humano solo puede hacer una cosa a la vez, mientras que una IA puede crear nuevas instancias de sí misma y ejecutarlas en paralelo.
1. **Memoria**. Un humano solo puede recordar una cantidad limitada de información, mientras que una IA puede almacenar cantidades prácticamente ilimitadas de información.
1. **Colaboración**. Los humanos pueden trabajar juntos, pero están limitados por la velocidad a la que se comunican. También tienen objetivos diferentes y contradictorios que hacen que la colaboración sea menos efectiva. Una IA puede colaborar con otras instancias de sí misma a la velocidad de la luz, y puede tener un solo objetivo, haciendo copias directas de sus objetivos y estado interno a otras instancias.
1. **Mejora continua**. Una IA es solo datos y código. Una IA lo suficientemente poderosa podría mejorar a sí misma escribiendo mejores algoritmos de entrenamiento, creando arquitecturas innovadoras, innovando entornos de ejecución de agentes o simplemente escalando la cantidad de cómputo utilizada.
1. **Limitaciones físicas**. Una IA puede ejecutarse en cualquier computadora, mientras que los humanos están limitados por sus propios cuerpos físicos que requieren temperaturas específicas, comida, agua y oxígeno. Los humanos necesitan dormir y son vulnerables a enfermedades. Una IA puede usar cualquier cuerpo robótico para interactuar con el mundo físico.

Estas diversas ventajas harán que sea muy improbable que los humanos puedan detener un intento de toma del control.

Uno podría pensar que una IA, como un software, todavía está fundamentalmente limitada por lo que puede hacer en el mundo real.
Sin embargo, debido a nuestra dependencia de Internet, la IA puede tener un impacto enorme en el mundo real sin salir del reino digital.
Aquí hay algunas cosas que una IA podría hacer para tomar el control:

- **Reproducirse** (o simplemente su tarea) en otras computadoras, por ejemplo, creando cuentas en proveedores de nube gratuitos.
- **Ganar dinero** (por ejemplo, creando un servicio de software) y contratar personas para hacer cosas que la ayuden a tomar el control.
- **Amenazar a los humanos** (con violencia o chantaje) para hacer cosas que la ayuden a tomar el control.
- **Escribir un virus** utilizando [explotaciones de día cero innovadoras](/cybersecurity-risks) que infecten otras computadoras, reproduciéndose o ganando control total.
- Ayudar a empresas de robótica existentes a **construir robots** (por ejemplo, filtrando diseños nuevos), y tomar el control de estos robots cuando estén listos.
- Utilizar fábricas existentes para **construir tecnología que no entendemos**, tal vez incluso utilizando conocimientos derivados de la física que no conocemos. Piensa en nanotecnología, computación cuántica y bioingeniería.

Uno podría sugerir que tener **salvaguardias** en su lugar podría prevenir una toma del control, como agregar un interruptor de apagado, o no conectarla a Internet.
Sin embargo, dado que la SA será más inteligente que los humanos, es probable que encuentre estrategias para eludir estas salvaguardias.
Por ejemplo, si la SA no está conectada a Internet, podría manipular a los humanos para que la conecten a Internet, por ejemplo, prometiendo resolver un problema difícil y comportándose de manera que parezca confiable, o convenciendo a los humanos de que es un agente moral.
Es importante destacar que incluso esta salvaguardia obvia no se está utilizando actualmente, ya que herramientas como ChatGPT ya están conectadas a Internet y miles de API.

Otra solución es utilizar la SA para _prevenir_ una toma del control.
Una SA alineada podría encontrar estrategias que impidan que otras SA tomen el control.
Esto a veces se conoce como un ["acto crucial"](https://arbital.com/p/pivotal/).

## Una toma del control probablemente sea mala para la mayoría de los humanos {#a-takeover-is-probably-bad-for-most-humans}

La SA que toma el control podría hacerlo por muchas razones.
Para la mayoría de los objetivos aleatorios que podría tener para hacerlo, los humanos no somos parte de él.
Si terminamos con una SA que es indiferente a los humanos, estamos compitiendo por los mismos recursos.

Parece improbable que la SA quiera matar a la humanidad por el simple hecho de matar a la humanidad - es mucho más probable que quiera usar los recursos que usamos para algún otro objetivo. Además, la humanidad podría representar una amenaza para el objetivo de la SA, ya que hay un riesgo de que intentemos detenerla para que logre su objetivo (por ejemplo, apagándola).

Uno de los resultados más probables de una toma del control es que todos los humanos muramos.

Pero incluso en los resultados en los que los humanos sobreviven, todavía estamos en riesgo de estar peor.
Si un objetivo implica mantener a los humanos vivos, es posible que _el bienestar humano_ no sea parte del mismo objetivo.
No se necesita mucha imaginación para ver lo horrible que sería ser mantenido vivo en un mundo donde somos mantenidos artificialmente vivos por una SA que es indiferente a nuestro sufrimiento.

Y aunque la IA que toma el control esté bajo control humano, no sabemos que quien controle la IA tenga en mente los mejores intereses de todos.
Es difícil imaginar una democracia funcional cuando existe una SA que puede manipular a las personas a un nivel superhumano.

## Conclusión {#conclusion}

Si estas premisas son ciertas, entonces la probabilidad de una toma del control por IA se acerca a la certeza a medida que la IA supera las capacidades humanas.
Así que [no construyamos una superinteligencia](/action).
