---
title: Protesta de PauseAI en La Haya, Pa√≠ses Bajos - 11 de agosto
description: Estamos organizando una protesta para exigir una pausa en el desarrollo de inteligencia artificial peligrosa.
---

 <!-- end of frontmatter metadata, dashes above need to stay -->

<script>
    import WidgetConsent from '$lib/components/widget-consent/WidgetConsent.svelte'
</script>

<WidgetConsent>
<div>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Protestamos en La Haya, Pa√≠ses Bajos, para pedir a nuestro gobierno que priorice la mitigaci√≥n de los riesgos de la IA. Tuvimos algunos discursos, hablamos con la gente en la calle, repartimos folletos y lo pasamos bien!<br><br>Consulta el comunicado de prensa (EN + NL) para obtener m√°s informaci√≥n: <a href="https://t.co/Dd7CXHlajc">https://t.co/Dd7CXHlajc</a> <a href="https://t.co/T306vZD974">pic.twitter.com/T306vZD974</a></p>&mdash; PauseAI ‚è∏ü§ñ (@PauseAI) <a href="https://twitter.com/PauseAI/status/1690290512643719168?ref_src=twsrc%5Etfw">12 de agosto de 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
</WidgetConsent>

- Protesta de PauseAI
- D√≥nde: Wijnhaven, La Haya
- Cu√°ndo: 11 de agosto de 2023, 16:00 - 17:00

## Por qu√© protestamos {#why-we-protest}

La inteligencia artificial (IA) se est√° desarrollando a un ritmo vertiginoso, mucho m√°s r√°pido de lo que cualquier cient√≠fico hab√≠a predicho.
Se est√°n invirtiendo miles de millones en capacidades de IA, y los resultados son asombrosos.
Los nuevos modelos est√°n [superando a los humanos](/sota) en muchos dominios.
A medida que aumentan las capacidades, tambi√©n aumentan los [riesgos](/risks).
Los cient√≠ficos incluso [advierten](https://www.safe.ai/statement-on-ai-risk) que la IA podr√≠a [acabar con la humanidad](/xrisk).

Nuestros pol√≠ticos no est√°n tomando este tema con la seriedad que merece.
Necesitamos que nuestros l√≠deres escuchen estas advertencias.
Necesitamos que tomen medidas y [implementen una pausa](/proposal) para detener esta carrera suicida.

Queremos que el gobierno holand√©s:

- Invite a expertos en seguridad de la IA a informar al parlamento sobre estos riesgos
- Programe un debate sobre los riesgos existenciales de la IA
- Priorice las preparaciones para la cumbre de seguridad de la IA m√°s adelante este a√±o y asuma un papel de liderazgo en la elaboraci√≥n de pol√≠ticas efectivas
- Colabore internacionalmente para implementar medidas de seguridad suficientes a escala global

## Agenda {#agenda}

- 12:00 - 16:00 Preparar carteles en el taller (solo para los verdaderos entusiastas, ¬°cont√°ctanos si quieres estar all√≠!)
- 16:00 Discursos + protesta + reparto de folletos
- 17:00 Bebidas en un pub cercano

## Contacto {#contact}

- Joep Meindertsma ([twitter](https://twitter.com/joepmeindertsma), [correo electr√≥nico](mailto:joep@ontola.io))

## Comunicado de prensa (EN): PauseAI pide al gobierno holand√©s que prevenga desastres relacionados con la IA que amenazan a la humanidad {#press-release-en-pauseai-calls-on-dutch-government-to-prevent-human-threatening-ai-related-disasters}

El viernes 11 de agosto, a las 16:00, un grupo de personas preocupadas se reunir√° en el Ministerio del Interior bajo el nombre de [PauseAI](http://pauseai.info) para abordar los desarrollos en el campo de la IA (generativa). Est√°n instando al gobierno a tomar medidas para pausar el desarrollo de inteligencia artificial poderosa y potencialmente peligrosa.

Hasta ahora, el gobierno holand√©s no ha tomado ninguna medida para abordar la amenaza existencial planteada por la IA. No ha habido respuesta a las advertencias y declaraciones de entidades como la [ONU](https://www.linkedin.com/feed/update/urn:li:activity:7075767810336923648), el Primer Ministro del [Reino Unido](https://www.theguardian.com/technology/2023/may/25/no-10-acknowledges-existential-risk-ai-first-time-rishi-sunak?) (donde se planea una cumbre sobre este tema para el oto√±o), y [expertos en IA](https://nos.nl/op3/artikel/2012979-wetenschappers-waarschuwen-voor-kunstmatige-intelligentie), incluso despu√©s de que una [moci√≥n](https://www.parlementairemonitor.nl/9353000/1/j9vvij5epmj1ey0/vm1rshv2ulz5) en la C√°mara de Representantes instara a tal acci√≥n a principios de este a√±o.

"[Los cient√≠ficos](https://www.safe.ai/statement-on-ai-risk) est√°n sonando la alarma: la IA podr√≠a significar el fin de la humanidad. Los expertos incluso estiman una [probabilidad del 30%](https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results) de que esto suceda. Las empresas de IA est√°n avanzando a toda velocidad, arriesgando todas nuestras vidas, mientras que la regulaci√≥n se queda atr√°s." - Joep Meindertsma, director ejecutivo de la empresa de software Ontola y fundador de PauseAI.

Las preocupaciones sobre los riesgos asociados con la IA est√°n creciendo r√°pidamente en todo el mundo. Esta semana, la firma de investigaci√≥n Axios public√≥ los resultados de una encuesta de opini√≥n p√∫blica realizada entre residentes de los Estados Unidos, que revel√≥ que el 86% de los encuestados est√°n preocupados por los riesgos catastr√≥ficos de la IA.

"Los EE. UU. tienen audiencias en el Senado donde los expertos en IA discuten c√≥mo la IA podr√≠a provocar el fin de la humanidad. ¬øPor qu√© se ignora este tema en la pol√≠tica holandesa? Especialmente considerando que los Pa√≠ses Bajos juegan un papel clave en la cadena de suministro de chips, gracias a ASML. ¬°Esto es por qu√© tambi√©n puede desempe√±ar un papel clave en la gobernanza de la computaci√≥n de IA! Todas las vidas est√°n en juego." - Joep Meindertsma

PauseAI pide al gobierno holand√©s que:

- Invite a expertos en seguridad de la IA a informar al parlamento sobre estos riesgos
- Programe un debate parlamentario sobre los riesgos existenciales de la inteligencia artificial avanzada
- Priorice las preparaciones para la cumbre de IA propuesta en el Reino Unido m√°s adelante este a√±o y asuma un papel de liderazgo en la elaboraci√≥n de pol√≠ticas efectivas. Los activistas tienen [propuestas concretas](https://pauseai.info/summit) y [ideas de pol√≠ticas](https://pauseai.info/proposal) para la pr√≥xima cumbre de IA.
- Colabore internacionalmente para implementar un conjunto adecuado de medidas a escala global, incluida una pausa en la IA.

Para obtener m√°s informaci√≥n, visite [PauseAI.info](http://pauseai.info). Contacto: Joep Meindertsma ([twitter](https://twitter.com/joepmeindertsma), [correo electr√≥nico](mailto:joep@ontola.io)) y Ruben Dieleman ([correo electr√≥nico](mailto:ruben@existentialriskobservatory.org))

## Comunicado de prensa (NL): PauseAI roept overheid op tot het voorkomen van mensbedreigende, AI-gerelateerde rampen {#press-release-nl-pauseai-roept-overheid-op-tot-het-voorkomen-van-mensbedreigende-ai-gerelateerde-rampen}

Op vrijdag 11 augustus om 16.00 komt een groep mensen samen die zich zorgen maken over de ontwikkelingen op het gebied van (generatieve) AI bij het Ministerie van Binnenlandse Zaken onder de naam [PauseAI](http://pauseai.info). Zij roepen de regering op zich in te spannen voor een pauze van de ontwikkeling van krachtige en mogelijk gevaarlijke kunstmatige intelligentie.

Tot nu toe heeft de Nederlandse regering echter geen actie ondernomen tegen de existenti√´le bedreiging van AI . Er is nog niet [gereageerd](https://www.linkedin.com/feed/update/urn:li:activity:7075767810336923648) op waarschuwingen en uitspraken van onder meer de [VN](https://www.linkedin.com/feed/update/urn:li:activity:7075088560508284928), de premier van het [Verenigd Koninkrijk](https://www.theguardian.com/technology/2023/may/25/no-10-acknowledges-existential-risk-ai-first-time-rishi-sunak?) (waar in het najaar een top wordt georganiseerd over dit onderwerp) en [experts op het gebied van AI](https://nos.nl/op3/artikel/2012979-wetenschappers-waarschuwen-voor-kunstmatige-intelligentie). Ook niet nadat eerder dit jaar een [motie](https://www.parlementairemonitor.nl/9353000/1/j9vvij5epmj1ey0/vm1rshv2ulz5) in de Tweede Kamer daartoe aanspoorde.

"[Wetenschappers](https://www.safe.ai/statement-on-ai-risk) trekken aan de bel: AI kan het einde betekenen van de mensheid. Experts geven dit gemiddeld zelfs [30% kans](https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results). AI bedrijven racen vooruit en gokken met al onze levens, terwijl regulering hopeloos achter blijft." - Joep Meindertsma, directeur van softwarebedrijf Ontola en oprichter van PauseAI.

De zorgen over de risico's die kleven aan AI zijn mondiaal snel aan het groeien. Deze week nog publiceerde onderzoeksbureau Axios de resultaten van een opiniepeiling onder inwoners van de Verenigde Staten, waaruit [bleek](https://www.axios.com/2023/08/09/ai-voters-trust-government-regulation) dat 86% zich zorgen maakt over catastrofale risico's van AI.

"De VS heeft senaatshoorzittingen waarbij AI experts vertellen over hoe AI het einde kan vormen van de mensheid. Waarom wordt dit onderwerp genegeerd in de Nederlandse politiek? En dat terwijl Nederland een sleutelrol speelt in de chip supply chain, dankzij ASML. Hierom kan het √≥√≥k een sleutelrol spelen in AI compute governance. Alle levens staan op het spel!" - Joep Meindertsma

PauseAI wil dat de Nederlandse regering:

- AI safety-experts uitnodigt om het parlement te informeren over deze risico's
- Een parlementair debat inroostert over de existenti√´le risico's van geavanceerde kunstmatige intelligentie
- Voorbereidingen op de voorgestelde AI-top in het Verenigd Koninkrijk van later dit jaar voorrang geeft en een leidende rol neemt inzake effectief beleid. De actievoerders hebben concrete [voorstellen](https://pauseai.info/summit) en [beleidsidee√´n](https://pauseai.info/proposal) voor de te houden AI-top.
- Internationaal samenwerkt om een toereikende set maatregelen op mondiale schaal toegepast te krijgen, waaronder een zogenoemde AI-pauze.

Voor meer info, bezoek [PauseAI.info](http://pauseai.info). Contact: Joep Meindertsma ([twitter](https://twitter.com/joepmeindertsma), [email](mailto:joep@ontola.io)) & Ruben Dieleman ([email](mailto:ruben@existentialriskobservatory.org))
