---
title: La difícil psicología del riesgo existencial
description: Pensar en el fin del mundo es difícil.
---

La mayoría de las personas responden inicialmente al tema del riesgo existencial de la inteligencia artificial con una mezcla de burla, negación e incredulidad.
El miedo a menudo solo aparece después de un largo tiempo de reflexión sobre ello.

La psicología del riesgo existencial es un tema que no se discute a menudo, en comparación con los aspectos técnicos de la seguridad de la IA.
Sin embargo, se podría argumentar que es quizás igual de importante.
Después de todo, si no podemos lograr que las personas tomen el tema en serio y actúen al respecto, no podremos hacer nada al respecto.

Es difícil **sacar a colación**, difícil **creer**, difícil **entender** y difícil **actuar**.
Tener una mejor comprensión de por qué estas cosas son tan difíciles puede ayudarnos a ser más convincentes, efectivos y empáticos.

## Difícil de sacar a colación {#difficult-to-bring-up}

El riesgo existencial es un tema difícil de sacar a colación en una conversación, especialmente si eres un político.
Las personas pueden pensar que estás loco, y es posible que no te sientas cómodo hablando sobre este tema técnicamente complejo.

### Miedo a ser ridiculizado {#fear-of-being-ridiculed}

La primera reacción al riesgo existencial es a menudo simplemente reírse de ello.
También hemos visto esto suceder en la Casa Blanca, la primera vez que se mencionó el riesgo existencial.
Esto, a su vez, hace que sea más difícil sacar a colación el tema de nuevo, ya que otros temerán ser ridiculizados por sacarlo a colación.

Los profesionales pueden temer que su reputación se vea dañada si comparten sus preocupaciones.

> "Era casi peligroso desde una perspectiva profesional admitir que estabas preocupado", - [dijo Jeff Clune](https://www.theglobeandmail.com/business/article-i-hope-im-wrong-why-some-experts-see-doom-in-ai/)

Presionar para que se tomen medidas políticas sensatas (como pausar) puede considerarse "extremista" o "alarmista", lo que puede disminuir tu credibilidad o reputación.

### Miedo a ser llamado racista/cultista/teórico de la conspiración {#fear-of-being-called-racistcultistconspiracy-theorist}

En los últimos meses, han surgido varias teorías de la conspiración.
Algunos individuos han afirmado que [todas las personas que se preocupan por la seguridad de la IA son racistas](https://medium.com/%2540emilymenonbender/talking-about-a-schism-is-ahistorical-3c454a77220f) y que [la seguridad de la IA es una secta](https://www.cnbc.com/2023/06/06/ai-doomers-are-a-cult-heres-the-real-threat-says-marc-andreessen.html).
Algunos han afirmado que los "fatalistas de la IA" son parte de una [conspiración de las grandes empresas de tecnología para "exagerar" la IA](https://www.latimes.com/business/technology/story/2023-03-31/column-afraid-of-ai-the-startups-selling-it-want-you-to-be).
Estas acusaciones ridículas pueden hacer que las personas preocupadas duden en compartir sus preocupaciones.

Sin embargo, antes de enfadarse con las personas que hacen estas acusaciones, tenga en cuenta que pueden ser el resultado del miedo y la negación (ver más abajo).
Reconocer los peligros de la IA es aterrador, y puede ser más fácil desestimar al mensajero que internalizar el mensaje.

### Un tema complejo para discutir {#a-complex-topic-to-argue-about}

A las personas les gusta hablar sobre cosas que conocen.
La dificultad técnica de la seguridad de la IA hace que sea un tema abrumador para la mayoría de las personas.
Se necesita tiempo y esfuerzo para entender los argumentos.
Como político, no quieres ser atrapado diciendo algo que esté mal, así que podrías simplemente evitar el tema por completo.

## Difícil de creer {#difficult-to-believe}

Incluso si hay una discusión sobre el riesgo existencial, es difícil convencer a las personas de que es un problema real.
Hay varias razones por las que la mayoría de las personas descartarán instantáneamente la idea.

### Sesgo de normalidad {#normalcy-bias}

Todos conocemos las imágenes de desastres en las películas, ¿verdad? Personas gritando y corriendo en pánico.
Resulta que lo contrario es a menudo cierto: alrededor del 80% de las personas muestran síntomas de [_sesgo de normalidad_](https://es.wikipedia.org/wiki/Sesgo_de_normalidad) durante los desastres: no buscar refugio durante un tornado, ignorar las advertencias del gobierno, seguir estrechando manos en los primeros días de la COVID.
El sesgo de normalidad describe nuestra tendencia a subestimar la posibilidad de un desastre y creer que la vida continuará como siempre, incluso frente a amenazas o crisis significativas.
Esto está [sucediendo actualmente con la IA](https://lethalintelligence.ai/post/category/warning-signs/).

> Las personas se quedan quietas, pidiendo opiniones, porque _quieren_ que les digan que todo está bien. Seguirán preguntando y retrasando hasta que obtengan la respuesta que quieren.

> Durante el 11 de septiembre, por ejemplo, el tiempo de espera promedio entre los supervivientes para evacuar las torres fue de 6 minutos, con algunos esperando hasta media hora para salir. Alrededor de 1000 personas incluso se tomaron el tiempo para apagar sus computadoras y completar otras actividades de oficina, una estrategia para seguir participando en actividades normales durante una situación desconocida.

_De ["La calma congelada del sesgo de normalidad"](https://gizmodo.com/the-frozen-calm-of-normality-bias-486764924)_

Otro ejemplo de esto es el desastre del transbordador espacial Challenger en 1986.
Roger Boisjoly era un ingeniero que predijo que explotaría, pero ninguno de sus gerentes quería creer que era posible:

> Todos sabíamos que si los sellos fallaban, el transbordador explotaría.
> Luché como un demonio para detener ese lanzamiento. Estoy tan destrozado por dentro que apenas puedo hablar de ello, incluso ahora.
> Estábamos hablando con las personas adecuadas, estábamos hablando con las personas que tenían el poder de detener ese lanzamiento.

_De ["Recordando a Roger Boisjoly"](https://www.npr.org/sections/thetwo-way/2012/02/06/146490064/remembering-roger-boisjoly-he-tried-to-stop-shuttle-challenger-launch)_

Una explicación de por qué nuestro cerebro se niega a creer que el peligro podría estar sobre nosotros es la disonancia cognitiva.

### Disonancia cognitiva {#cognitive-dissonance}

Cuando se enfrenta a nueva información, el cerebro intentará hacer que se ajuste a lo que ya sabe.
Las ideas que ya coinciden con las creencias existentes se agregan fácilmente a nuestro modelo del mundo.
Las ideas que son demasiado diferentes de lo que ya creemos causarán _disonancia cognitiva_ - nos sentiremos incómodos y trataremos de rechazar las ideas o encontrar explicaciones alternativas para lo que escuchamos.

Muchas creencias que la mayoría de las personas tienen serán desafiadas por la idea del riesgo existencial:

- La tecnología está allí para servirnos y puede ser controlada fácilmente
- Hay personas inteligentes a cargo que se asegurarán de que todo esté bien
- Probablemente envejeceré, y también mis hijos

Muchos de estos pensamientos serán desafiados por la idea de que la IA plantea un riesgo existencial.
Nuestros cerebros buscarán explicaciones alternativas para por qué escuchamos que los científicos están [advertiendo sobre esto](/quotes):

- Están pagados por las grandes empresas de tecnología
- Son parte de alguna conspiración o secta
- Solo quieren atención o poder

Internalizar que _los científicos están advirtiendo porque creen que estamos en peligro_ entra en conflicto con nuestras creencias existentes, causa demasiada disonancia cognitiva.

### El fin del mundo nunca ha sucedido {#the-end-of-the-world-has-never-happened}

Ver es creer (ver: [_Heurística de disponibilidad_](https://es.wikipedia.org/wiki/Heur%C3%ADstica_de_disponibilidad)).
Ese es un problema para el riesgo de extinción porque nunca podremos verlo antes de que sea demasiado tarde.

Por otro lado, tenemos toneladas de evidencia en contrario.
El fin de los tiempos ha sido predicho por muchas personas, y cada una de ellas ha estado equivocada hasta ahora.

Así que cuando las personas escuchan sobre el riesgo existencial, pensarán que es solo otra de esas predicciones de cultos apocalípticos.
Trata de entender este punto de vista, y no seas demasiado duro con las personas que piensan de esta manera.
Probablemente no han visto la misma información que tú.

### Nos gusta pensar que somos especiales {#we-like-to-think-that-we-are-special}

Tanto a nivel colectivo como individual, queremos creer que somos especiales.

A nivel colectivo, nos gusta pensar en los humanos como algo muy diferente de los animales - la idea de Darwin de que evolucionamos a partir de los monos era casi impensable para la mayoría.
La mayoría de las religiones tienen historias sobre el cielo o la reencarnación, donde los humanos (o al menos los creyentes) vivirán para siempre de alguna manera.
La idea de que un día la humanidad puede dejar de existir es muy desconcertante y difícil de internalizar.
Nos gusta creer que tenemos _armadura de trama_ - que somos los personajes principales en una historia, y que la historia tendrá un final feliz.
Las personas pueden considerarlo racionalmente, pero no lo _sentirán_.
Un video de Robert Miles titulado ["No hay regla que diga que lo lograremos"](https://www.youtube.com/watch?v=JD_iA7imAPs) explica esto muy bien.

A nivel individual, nos enorgullecemos de las habilidades intelectuales únicas que tenemos.
Muchos nunca quisieron creer que un día una IA podría crear arte, escribir libros o incluso vencer a los humanos en ajedrez.
El pensamiento de que nuestra propia inteligencia es solo un producto de la evolución y que puede ser replicada por una máquina es algo que muchas personas encuentran difícil de aceptar.
Esto hace que sea difícil aceptar que una IA podría ser más inteligente que nosotros.

### La ficción nos ha condicionado a esperar un final feliz {#fiction-has-conditioned-us-to-expect-a-happy-ending}

La mayoría de lo que sabemos sobre el riesgo existencial proviene de la ficción.
Esto probablemente no ayuda, porque las historias ficticias no están escritas para ser realistas: están escritas para ser entretenidas.

En la ficción, a menudo hay un héroe, conflicto, esperanza y finalmente un final feliz.
Estamos condicionados a esperar una lucha en la que podemos luchar y ganar.
En la ciencia ficción, las IA a menudo se retratan de manera muy antropomórfica - como malvadas, como queriendo ser humanas, como cambiando sus objetivos.
Nada de esto coincide con lo que los expertos en seguridad de la IA están preocupados.

Y en la mayoría de las historias, el héroe gana.
La IA comete algún error tonto y el héroe encuentra una forma de superar a la cosa que se supone que es mucho más inteligente.
El héroe está protegido por armadura de trama.
En escenarios de fatalidad de la IA más realistas, no hay héroe, no hay armadura de trama, no hay lucha, no hay humanos que superen a una superinteligencia, y no hay final feliz.

### El progreso siempre ha sido (en su mayoría) bueno {#progress-has-always-been-mostly-good}

Muchas de las tecnologías introducidas en nuestra sociedad han sido en su mayoría beneficiosas para la humanidad.
Hemos curado enfermedades, aumentado nuestra esperanza de vida y hemos hecho que nuestras vidas sean más cómodas.
Y cada vez que lo hemos hecho, ha habido personas que se han opuesto a estas innovaciones y han advertido sobre los peligros.
Los luditas destruyeron las máquinas que les quitaban el trabajo, y las personas temían a los primeros trenes y automóviles.
Estas personas siempre han estado equivocadas.

### No nos gusta pensar en nuestra muerte {#we-dont-like-to-think-about-our-death}

La mente humana no le gusta recibir malas noticias, y tiene varios mecanismos de afrontamiento para lidiar con ellas.
Los más importantes cuando se habla de riesgo existencial son la [negación](https://es.wikipedia.org/wiki/Disonancia_cognitiva) y la [compartmentalización](<https://es.wikipedia.org/wiki/Compartmentalizaci%C3%B3n_(psicolog%C3%ADa)>).
Cuando se trata de nuestra propia muerte, específicamente, somos muy propensos a la negación.
Se han escrito libros sobre la [Negación de la muerte](https://es.wikipedia.org/wiki/La_negaci%C3%B3n_de_la_muerte).

Estos mecanismos de afrontamiento nos protegen del dolor de tener que aceptar que el mundo no es como pensábamos que era.
Sin embargo, también pueden impedir que respondamos adecuadamente a una amenaza.

Cuando notes que alguien está utilizando estos mecanismos de afrontamiento, trata de ser empático.
No lo están haciendo a propósito, y no son estúpidos.
Es una reacción natural a las malas noticias, y todos lo hacemos en cierta medida.

### Admitir que tu trabajo es peligroso es difícil {#admitting-your-work-is-dangerous-is-hard}

Para aquellos que han estado trabajando en capacidades de IA, aceptar sus peligros es aún más difícil.

Toma a Yoshua Bengio, por ejemplo.
Yoshua Bengio tiene una mente brillante y es uno de los pioneros en IA.
Los expertos en seguridad de la IA han estado advirtiendo sobre los peligros potenciales de la IA durante años, pero todavía le tomó mucho tiempo tomar sus advertencias en serio.
En una [entrevista](https://youtu.be/0RknkWgd6Ck?t%25253D949), dio la siguiente explicación:

> "¿Por qué no pensé en ello antes? ¿Por qué Geoffrey Hinton no pensó en ello antes? [...] Creo que hay un efecto psicológico que todavía puede estar en juego para muchas personas. [...] Es muy difícil, en términos de tu ego y sentirte bien con lo que haces, aceptar la idea de que la cosa en la que has estado trabajando durante décadas podría ser en realidad muy peligrosa para la humanidad. [...] Creo que no quería pensar demasiado en ello, y probablemente sea el caso para otros."

No debería sorprender a nadie que algunos de los más feroces negadores del riesgo de la IA sean investigadores de la IA ellos mismos.

### Fácil de descartar como conspiración o secta {#easy-to-dismiss-as-conspiracy-or-cult}

En el último año, la mayoría de la población se enteró del concepto de riesgo existencial de la IA.
Cuando escuchan sobre esto, las personas buscarán una explicación.
La explicación correcta es que la IA es en realidad peligrosa, pero creer esto es difícil y aterrador: llevará a mucha fricción cognitiva.
Así que las personas casi directamente buscarán una forma diferente de explicar sus observaciones.
Hay dos explicaciones alternativas que son mucho más fáciles de creer:

1. **Es una gran conspiración**. Las empresas de IA están exagerando la IA para obtener más financiamiento, y las personas que se preocupan por la seguridad de la IA son solo parte de esta máquina de exageración. Esta narrativa coincide con varias observaciones: las empresas a menudo mienten, muchos expertos en seguridad de la IA están empleados por empresas de IA, y hay un grupo de multimillonarios que están financiando la investigación sobre seguridad de la IA. Sin embargo, también podemos señalar por qué esta historia de conspiración simplemente no es cierta. Muchos de los "alarmistas" son científicos que no tienen nada que ganar. Las empresas pueden beneficiarse de alguna manera, pero hasta hace muy poco (mayo de 2023), han estado casi completamente en silencio sobre los riesgos de la IA. Esto tiene sentido, ya que las empresas en su mayoría no se benefician de que las personas teman a su producto o servicio. Hemos estado protestando fuera de Microsoft y OpenAI en parte _porque_ queríamos que reconocieran los riesgos.
2. **Es una secta**. El grupo que cree en la seguridad de la IA es solo un grupo de extremistas religiosos locos que creen en el fin del mundo. Esto parece encajar, también, ya que las personas en la comunidad de seguridad de la IA a menudo son muy apasionadas sobre el tema y utilizan todo tipo de jerga de grupo. Sin embargo, se desmorona cuando se señala que las personas que advierten sobre los riesgos de la IA no son una sola organización. Es un grupo grande y diverso de personas, no hay un solo líder, no hay rituales y no hay dogma.

Lo que hace que estas explicaciones sean tan convincentes no es solo que sean fáciles de comprender, o que expliquen todas las observaciones a la perfección, sino que son reconfortantes.
Creer que las personas advierten sobre la IA _porque hay una amenaza real_ es aterrador y difícil de aceptar.

## Difícil de entender {#difficult-to-understand}

Los argumentos a favor del riesgo existencial de la IA son a menudo muy técnicos, y somos muy propensos a antropomorfizar los sistemas de IA.

### La alineación de la IA es sorprendentemente difícil {#ai-alignment-is-surprisingly-hard}

Las personas pueden sentir intuitivamente que podrían resolver el problema de la alineación de la IA.
¿Por qué no agregar un [botón de parada](https://www.youtube.com/watch?v=3TYT1QfdfsM&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40&index=10)? ¿Por qué no [criar a la IA como a un niño](https://www.youtube.com/watch?v=eaYIU6YXr3w)? ¿Por qué no [las tres leyes de Asimov](https://www.youtube.com/watch?v=7PKx3kS7f4A)?
Contrariamente a la mayoría de los tipos de problemas técnicos, las personas tendrán una opinión sobre cómo resolver la alineación de la IA y subestimarán la dificultad del problema.
Comprender la dificultad real de ello lleva mucho tiempo y esfuerzo.

### Antropomorfizamos {#we-anthropomorphize}

Vemos caras en las nubes y vemos cualidades humanas en los sistemas de IA.
Millones de años de evolución nos han convertido en criaturas altamente sociales, pero estos instintos no siempre son útiles.
Tendemos a pensar en las IA como si tuvieran metas y motivaciones similares a las humanas, pudieran sentir emociones y tuvieran un sentido de la moral.
Tendemos a esperar que una IA muy inteligente también sea muy sabia y amable.
Esta es una de las razones por las que las personas intuitivamente piensan que la alineación de la IA es fácil, y por qué la [tesis de ortogonalidad](https://www.youtube.com/watch?v=hEUO6pjwFOo) puede ser tan contraintuitiva.

### La seguridad de la IA utiliza un lenguaje complejo {#ai-safety-uses-complex-language}

El campo de la seguridad de la IA consiste principalmente en un pequeño grupo de personas (inteligentes) que han desarrollado su propio jerga.
Leer publicaciones en LessWrong puede sentirse como leer un idioma extranjero.
Muchas publicaciones asumen que el lector ya está familiarizado con conceptos matemáticos, varios conceptos técnicos y el jerga del campo.

## Difícil de actuar {#difficult-to-act-on}

Incluso si las personas entienden los argumentos, sigue siendo difícil actuar en base a ellos.
El impacto es demasiado grande, tenemos mecanismos de afrontamiento que minimizan los riesgos, y si sentimos la gravedad de la situación, podemos sentirnos impotentes.

### Falta de respuesta de miedo innata {#lack-of-innate-fear-response}

Nuestros cerebros han evolucionado para temer cosas que son peligrosas.
Instintivamente tememos las alturas, los animales grandes con dientes afilados, los ruidos fuertes y repentinos, y las cosas que se mueven en forma de S.
Una IA superinteligente no desencadena ninguno de nuestros miedos primarios.
Además, tenemos un fuerte miedo al rechazo social o a perder estatus social, lo que significa que las personas tienden a tener miedo de hablar sobre los riesgos de la IA.

### Insensibilidad al alcance {#scope-insensitivity}

> "Una sola muerte es una tragedia; un millón de muertes es una estadística". - Joseph Stalin

La insensibilidad al alcance es la tendencia humana a subestimar el impacto de los grandes números.
No nos importa 10 veces más que mueran 1000 personas que 100 personas.
El riesgo existencial significa la muerte de los 8000 millones de personas en la Tierra (sin contar a sus descendientes).

Incluso si hay un 1% de probabilidad de que esto suceda, sigue siendo un gran problema.
Racionalmente, deberíamos considerar este 1% de probabilidad de 8000 millones de muertes tan importante como la muerte segura de 80 millones de personas.

Si alguien siente que el fin del mundo no es gran cosa (te sorprenderías de cómo sucede esto a menudo), puedes intentar hacer las cosas más personales.
La humanidad no es solo un concepto abstracto, son tus amigos, tu familia y tú mismo.
Todas las personas que te importan morirán.

### Nuestro comportamiento está moldeado por nuestro entorno y mentes primitivas {#our-behavior-is-shaped-by-our-environment-and-primitive-minds}

Nuestras acciones están condicionadas por lo que se considera normal, bueno y razonable.
No importa cuánto queramos actuar en una situación que lo requiera, si esas acciones son poco comunes, gran parte del tiempo tememos consciente o inconscientemente ser excluidos por la sociedad por hacerlas. Y lo que es normal se nos impone en nuestras mentes al estar rodeados de ello en nuestros círculos sociales cercanos y en nuestras redes en línea.
Las personas que hacen y hablan de cosas no relacionadas con lo que realmente nos importa anularán lo que está en nuestras mentes y nos motivarán a hacer otras cosas a diario.

Los riesgos de extinción merecen **mucho** más de nuestro tiempo, energía y atención. Nuestras reacciones ante ellos deberían ser más parecidas a situaciones de vida o muerte que nos llenan de adrenalina. Pero, debido a la naturaleza abstracta de los problemas y nuestras mentes desadaptadas, la mayoría de las personas que se enteran de ellos simplemente terminan continuando con sus días como si no hubieran aprendido nada.

### Mecanismos de afrontamiento (impidiendo la acción) {#coping-mechanisms-preventing-action}

Los mismos mecanismos de afrontamiento que impiden que las personas _crean_ en el riesgo existencial también les impiden _actuar_ al respecto.
Si estás en negación o compartimentando, no sentirás la necesidad de hacer nada al respecto.

### Estrés y ansiedad {#stress-and-anxiety}

Mientras escribo esto, me siento estresado y ansioso.
No es solo porque temo el fin del mundo, sino también porque siento que tengo que hacer algo al respecto.
Hay mucha presión para actuar, y puede ser abrumadora.
Este estrés puede ser un buen motivador, pero también puede ser paralizante.

### Desesperanza e impotencia {#hopelessness-and-powerlessness}

Cuando las personas toman el tema en serio y la gravedad completa de la situación se hunde, pueden sentir que toda esperanza está perdida.
Puede sentirse como un diagnóstico de cáncer: vas a morir antes de lo que querías y no hay nada que puedas hacer al respecto.
El problema es demasiado grande para abordarlo y tú eres demasiado pequeño.
La mayoría de las personas no son expertos en seguridad de la IA o cabilderos experimentados, así que ¿cómo pueden hacer algo al respecto?

## ¡Pero puedes ayudar! {#but-you-can-help}

Hay muchas [cosas que puedes hacer](/action).
Escribir una carta, ir a una protesta, donar algo de dinero o unirse a una comunidad no es tan difícil.
Y estas acciones tienen un impacto real.
Incluso cuando enfrentamos el fin del mundo, aún puede haber esperanza y un trabajo muy gratificante por hacer.
[Únete a PauseAI](/join) y forma parte de nuestro movimiento.
