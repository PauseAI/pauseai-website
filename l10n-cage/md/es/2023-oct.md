---
title: Protesta internacional de PauseAI el 21 de octubre de 2023
description: Estamos organizando una protesta internacional para exigir una pausa en el desarrollo peligroso de la inteligencia artificial.
---

<script>
    import WidgetConsent from '$lib/components/widget-consent/WidgetConsent.svelte'
</script>

## 21 de octubre (s√°bado), en varios pa√≠ses {#october-21st-saturday-in-multiple-countries}

- EE. UU., California, San Francisco ([Facebook](https://fb.me/1RbYq9H2hOFQ4yi))
- EE. UU., Massachusetts, Boston ([Facebook](https://facebook.com/events/s/pauseai-protest-boston-make-th/6647554948613714/?mibextid=RQdjqZ))
- Reino Unido, Parliament Square, Londres ([Inscr√≠bete](https://www.mixily.com/event/4774799330762010477), [Facebook](https://www.facebook.com/events/644748401084077))
- Pa√≠ses Bajos, La Haya ([Inscr√≠bete](https://www.mixily.com/event/8536294863402363208))
- Australia, Melbourne ([Inscr√≠bete](https://www.mixily.com/event/8471341506387452508))
- Canad√°, Ottawa (Organizado por Align the World, inscr√≠bete en [Facebook](https://www.facebook.com/events/243643008241929/) o [Eventbrite](https://www.eventbrite.com/e/ai-safety-and-ethics-rally-tickets-725729686027))
- Dinamarca, Copenhague ([Facebook](https://www.facebook.com/events/869443424535827))
- ¬øTu pa√≠s no est√° en la lista? [Discute en Discord](https://discord.gg/anXWYCCdH5)

## Por qu√© protestamos {#why-we-protest}

La inteligencia artificial est√° avanzando a un ritmo vertiginoso, mucho m√°s r√°pido de lo que cualquier cient√≠fico de la IA hab√≠a predicho.
Se est√°n invirtiendo miles de millones en capacidades de IA, y los resultados son asombrosos.
Los nuevos modelos est√°n [superando a los humanos](/sota) en muchos √°mbitos.
A medida que aumentan las capacidades, tambi√©n aumentan los [riesgos](/risgos).
Los cient√≠ficos incluso [advierten](https://www.safe.ai/statement-on-ai-risk) que la IA podr√≠a [acabar destruyendo la humanidad](/xrisk).
Este resultado desastroso no solo parece posible, sino tambi√©n probable, ya que las estimaciones de probabilidad promedio para estos resultados [var√≠an del 14% al 40%](/polls-and-surveys).

Necesitamos que nuestros l√≠deres escuchen estas advertencias, pero no est√°n tomando este tema con la seriedad que merece.
Se est√° redactando legislaci√≥n sobre seguridad de la IA, pero [ni una sola medida evitar√≠a o retrasar√≠a la creaci√≥n de una IA superinteligente](https://twitter.com/PauseAI/status/1704998018322141496).
[M√°s del 70%](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation) de las personas quieren ralentizar el desarrollo de la IA, y [m√°s del 60%](https://www.vox.com/future-perfect/2023/9/19/23879648/americans-artificial-general-intelligence-ai-policy-poll) quieren que el gobierno intervenga para evitar la creaci√≥n de una IA superinteligente.
¬øPor qu√© no hay un borrador de legislaci√≥n que realmente haga esto?
La respuesta es el lobby: nuestros pol√≠ticos est√°n [reuni√©ndose principalmente con los directores ejecutivos de empresas de IA](https://fedscoop.com/sen-schumer-to-host-musk-zuckerberg-and-other-tech-ceos-for-closed-door-ai-forum/), y ellos impulsar√°n medidas pol√≠ticas que est√©n en su inter√©s.

El 1 y 2 de noviembre, se celebrar√° la primera Cumbre de Seguridad de la IA en el Reino Unido.
La oportunidad perfecta para dar los primeros pasos hacia una regulaci√≥n internacional sensata de la seguridad de la IA.

## Lo que pedimos {#what-we-ask}

- **Responsables pol√≠ticos**: No permitan que las empresas creen una superinteligencia. Las regulaciones y restricciones de hardware deben aplicarse antes de que comience la formaci√≥n, ya que es muy dif√≠cil controlar la difusi√≥n una vez que se ha logrado una nueva capacidad. No podemos permitir que las empresas entren modelos de IA potencialmente peligrosos. Redactar legislaci√≥n es dif√≠cil y lleva tiempo, pero es posible que no tengamos tanto tiempo, as√≠ que trabajen como si su vida dependiera de ello. Porque depende de ello.
- **Empresas**: Muchas de ustedes temen lo que la IA puede hacer, pero est√°n atrapadas en una carrera. As√≠ que sean vocales sobre su apoyo a una pausa en principio. Si firman declaraciones de que esta tecnolog√≠a podr√≠a matarnos a todos, muestren al mundo que preferir√≠an no construirla si fuera una opci√≥n viable.
- **Invitados a la cumbre**: Prioricen la seguridad sobre el crecimiento econ√≥mico. Sabemos que la IA puede hacer que nuestros pa√≠ses sean m√°s ricos, pero ese no es el motivo por el que est√°n aqu√≠. Sean los adultos en la sala.

Para nuestra propuesta completa, v√©ase [aqu√≠](/proposal).

## Comunicado de prensa {#press-release}

_PARA PUBLICACI√ìN INMEDIATA_

### La protesta internacional pide un alto al desarrollo peligroso de la IA {#international-protest-calls-for-a-halt-to-dangerous-ai-development}

**21 de octubre:** [**PauseAI**](https://pauseai.info/) **est√° organizando una protesta internacional** [**protesta**](https://pauseai.info/2023-oct) **instando a los responsables pol√≠ticos y a los asistentes a la Cumbre de Seguridad de la IA a trabajar hacia una prohibici√≥n de la creaci√≥n de una IA superinteligente. La protesta tendr√° lugar en** [**8 pa√≠ses**](https://pauseai.info/2023-oct) **simult√°neamente y se espera que sea la protesta m√°s grande por una moratoria de la IA por un amplio margen.**

Ubicaciones:

- EE. UU., California, San Francisco
- Reino Unido, Parliament Square, Londres
- Pa√≠ses Bajos, La Haya
- Israel, Jerusal√©n
- Australia, Melbourne
- Canad√°, Ottawa
- Alemania, Berl√≠n
- Dinamarca, Copenhague

En marzo de este a√±o, muchos expertos notables firmaron [una carta](https://futureoflife.org/open-letter/pause-giant-ai-experiments/#:~:text=We%20call%20on%20all%20AI,more%20powerful%20than%20GPT%2D4.&text=AI%20systems%20with%20human%2Dcompetitive,acknowledged%20by%20top%20AI%20labs.) pidiendo una pausa de seis meses en el desarrollo de sus modelos de IA de vanguardia. En mayo, cientos de cient√≠ficos de la IA firmaron [una declaraci√≥n](https://www.safe.ai/statement-on-ai-risk) diciendo que ‚ÄúMitigar el riesgo de extinci√≥n de la IA deber√≠a ser una prioridad global junto con otros riesgos a escala societal como las pandemias y la guerra nuclear‚Äù.

Recientes [encuestas](https://pauseai.info/polls-and-surveys) han mostrado que [m√°s del 70%](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation) de las personas quieren que el progreso de la IA se ralentice, y [m√°s del 60%](https://www.vox.com/future-perfect/2023/9/19/23879648/americans-artificial-general-intelligence-ai-policy-poll) quieren que el gobierno intervenga para evitar la creaci√≥n de una IA superinteligente. Hasta ahora, [no se han propuesto borradores](https://twitter.com/PauseAI/status/1706605169608159458) que hagan esto.

En dos semanas, el 1 y 2 de noviembre, se celebrar√° la primera Cumbre de Seguridad de la IA en Bletchley Park, Reino Unido. La cumbre contar√° con la asistencia de destacados cient√≠ficos de la IA, responsables pol√≠ticos y l√≠deres de la industria. Esto marca una oportunidad √∫nica para dar los primeros pasos hacia una regulaci√≥n internacional de la seguridad de la IA. Sin embargo, el Reino Unido no planea utilizar esta oportunidad para implementar una regulaci√≥n fuerte de la IA. El organizador y representante del primer ministro para la Cumbre de Seguridad de la IA, Matt Clifford, ha [declarado](https://twitter.com/PauseAI/status/1709845853668553065) que ‚ÄúPausar el desarrollo de la IA ahora ser√≠a prematuro‚Äù, y que [no espera](https://twitter.com/matthewclifford/status/1708819574739587356) ‚Äúcontroles duros‚Äù de la cumbre.

‚ÄúEstamos contentos de que el Reino Unido est√© liderando la seguridad de la IA y mostrando liderazgo internacional‚Äù, dice Joep Meindertsma, director de PauseAI. ‚ÄúPero no estamos viendo el nivel de urgencia que merece. En 2020, los pronosticadores [predijeron](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) la llegada de la IA a nivel humano en 2055. Hoy en d√≠a, la [predicci√≥n promedio](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) es 2026. No podemos arriesgarnos a un desastre subestimando la tasa de progreso. Necesitamos que nuestros pol√≠ticos se inclinen hacia la precauci√≥n. Cada vida est√° en peligro. Ninguna empresa deber√≠a poder crear una IA superinteligente‚Äù.

### Twitter {#twitter}

<WidgetConsent>
<div><blockquote class="twitter-tweet"><p lang="en" dir="ltr">M√°s de Londres despu√©s de la primera protesta internacional coordinada de <a href="https://twitter.com/PauseAI?ref_src=twsrc%5Etfw">@PauseAI</a>! El s√°bado, los manifestantes se reunieron en siete ciudades exigiendo una prohibici√≥n de la creaci√≥n de inteligencia artificial superinteligente, una semana antes de la Cumbre de Seguridad de la IA de <a href="https://twitter.com/RishiSunak?ref_src=twsrc%5Etfw">@RishiSunak</a>. Leer m√°s ‚¨áÔ∏è <a href="https://t.co/W2vYv4nVIl">pic.twitter.com/W2vYv4nVIl</a></p>&mdash; Alistair Stewart ‚ìã ‚è∏Ô∏è (@alistair___s) <a href="https://twitter.com/alistair___s/status/1716566914242121768?ref_src=twsrc%5Etfw">23 de octubre de 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div>
</WidgetConsent>

<WidgetConsent>
<div><blockquote class="twitter-tweet"><p lang="en" dir="ltr">La protesta de PauseAI en San Francisco fue un gran √©xito. Esta fue la protesta de seguridad de la IA m√°s grande en la historia de Estados Unidos, y fue parte de la primera y m√°s grande protesta global de seguridad de la IA en la historia! <br><br>¬°Muchas gracias a todos los que lo hicieron posible! ü©∑ <a href="https://t.co/Yttdpgnrfa">pic.twitter.com/Yttdpgnrfa</a></p>&mdash; Holly ‚è∏Ô∏è Elmore (@ilex_ulmus) <a href="https://twitter.com/ilex_ulmus/status/1715954127954751932?ref_src=twsrc%5Etfw">22 de octubre de 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div>
</WidgetConsent>
