---
title: La viabilidad de una pausa
description: ¿Es posible pausar la IA?
---

Pausar la IA no es imposible.
Establecer límites que determinen qué tipo de tecnologías y acciones pueden desarrollarse y realizarse es algo que hacemos constantemente.

## Viabilidad política de una pausa {#political-feasibility-of-a-pause}

Algunos han calificado la pausa de "radical" o "extrema", pero no es así como el público lo percibe.
Varias [encuestas y sondeos](/polls-and-surveys) han demostrado que:

- La gente está muy preocupada por la IA (principalmente por motivos de seguridad)
- La gran mayoría (casi el 70%) [apoya una pausa en el desarrollo de la IA](https://www.sentienceinstitute.org/aims-survey-supplement-2023)
- La gran mayoría (>60%) [apoya un tratado internacional para prohibir la AGI](https://www.sentienceinstitute.org/aims-survey-supplement-2023)

## Aplicabilidad técnica de una pausa {#technical-enforceability-of-a-pause}

La forma más sencilla de regular los modelos de vanguardia de manera efectiva es [gobernando la potencia de cálculo](https://www.governance.ai/post/computing-power-and-the-governance-of-ai).
Podemos [rastrear las GPU](https://arxiv.org/abs/2303.11341) como rastreamos los elementos utilizados en el desarrollo de armas nucleares.
Afortunadamente, la cadena de suministro de cómputo tiene varios puntos de estrangulamiento.
El hardware necesario para entrenar los modelos más grandes (GPU especializadas) está siendo producido por [solo 1 o 3 empresas](https://assets-global.website-files.com/614b70a71b9f71c9c240c7a7/65cb86a0341180453f268f38_SpwF1cBT0AS-m_n20TBXzCF6YprIVM4YRb9PMYWURseU1KtVkSAZJ735esGxNenwVO4Q4wlSUP-_MV3E-SEKp4SIgo1-oNe14CeDHtrb3PLXpJMym5qpWEDbXcf3maEi4yQYfQ-3NP7XgUmkO_4Zekw.jpeg).
Hay múltiples monopolios en la cadena de suministro de hardware de entrenamiento de IA:

- ASML es la única empresa que produce máquinas de litografía EUV
- TSMC es la única empresa que puede fabricar los chips más avanzados
- NVidia es la única empresa que diseña las GPU más avanzadas

## Poder sobre las empresas {#power-over-companies}

Si lo que temes son principalmente las empresas u organizaciones, podemos controlarlas a través de 1) leyes, regulaciones y tratados, o 2) opinión pública que las obligue a autorregularse.

Por supuesto, el primer método es el mejor, pero la reputación que afecta a los clientes, inversores, moral de los empleados y reclutamiento es una razón por la que organizamos protestas frente a algunos laboratorios de IA.
Además, es importante recordar que las regulaciones pueden beneficiar a las empresas a largo plazo, debido a la captura regulatoria, no perder consumidores si los peligros se materializan, y desventaja a los competidores.
Así que debemos tener cuidado de no solo obtener una pausa, sino de que no se levante hasta que sea seguro seguir desarrollando la IA de vanguardia.

## Poder sobre los gobiernos {#power-over-governments}

Si temes que los gobiernos no tomen en serio tu seguridad, eso es un problema más complicado.
Pero en general, los políticos se preocupan por no perder apoyo político hasta cierto punto.
Y, lo que es más importante, también pueden estar preocupados por los riesgos sin el enorme sesgo y las obligaciones legales que algunos individuos de las empresas tienen para maximizar las ganancias.

Si crees que podríamos obtener la regulación de un solo gobierno, pero no un tratado multilateral, debes darte cuenta de que si los gobiernos pueden reconocer que algunas tecnologías incontrolables son un peligro para su población y pueden originarse en otras naciones, las nuevas tecnologías se convierten en un problema de seguridad nacional, y los gobiernos se vuelven interesados en detener a otros países para que no las desarrollen también.
Además, es importante darse cuenta de que no necesitamos que muchos países estén de acuerdo con una pausa en primer lugar.
En realidad, puedes obtener una pausa en el desarrollo de modelos de vanguardia prohibiéndolos solo en EE. UU. (y incluso solo en California).
China y el resto del mundo parecen estar bastante atrasados, y no deberíamos preocuparnos si su adhesión a un tratado sucede algo más tarde.

## Casos históricos similares {#similar-historical-cases}

Aunque cada prueba de incompetencia o mala intención de nuestros gobiernos, empresas y sistemas puede llevarnos a un pensamiento derrotista, donde la coordinación es demasiado difícil, los intereses de la gente no están bien representados,
y/o están representados pero son estúpidos, a veces no reconocemos las victorias que hemos tenido como civilización a lo largo de la historia.

Para obtener evidencia empírica de por qué un tratado como este es posible, debemos mirar los acuerdos globales del pasado.
Ya sean informales o formales, han sido bastante comunes a lo largo de la historia, principalmente para resolver disputas y promover los derechos humanos.
Muchas victorias pasadas, como la abolición de la esclavitud, también tenían fuertes incentivos económicos a corto plazo en su contra.
Pero eso no las detuvo.

Si buscamos ejemplos modernos de acuerdos globales contra nuevas tecnologías, podemos encontrar muchos. Algunos de los más importantes fueron:

- El [Protocolo de Montreal](https://es.wikipedia.org/wiki/Protocolo_de_Montreal), que prohibió la producción de CFC en todos los 197 países y, como resultado, las emisiones globales de sustancias que agotan la capa de ozono han disminuido más del 99% desde 1986. Gracias al protocolo, el agujero en la capa de ozono se está curando ahora, y eso es por lo que ya no se habla de ello.
- La [Convención sobre Armas Biológicas](https://es.wikipedia.org/wiki/Convención_sobre_Armas_Biológicas), que prohibió las armas biológicas y tóxicas y fue firmada por 185 estados.
- La [Convención sobre Armas Químicas](https://es.wikipedia.org/wiki/Convención_sobre_Armas_Químicas), que prohibió las armas químicas y fue firmada por 193 estados.
- La [Convención sobre la Modificación del Medio Ambiente](https://es.wikipedia.org/wiki/Convención_sobre_la_Modificación_del_Medio_Ambiente), que prohibió la guerra climática y fue firmada por 78 estados.
- El [Tratado del Espacio Exterior](https://es.wikipedia.org/wiki/Tratado_del_Espacio_Exterior), que prohibió la estación de armas de destrucción masiva en el espacio exterior, prohibió las actividades militares en cuerpos celestes, vinculó legalmente la exploración y el uso pacífico del espacio, y fue firmado por 114 países.
- El [Tratado de No Proliferación Nuclear](https://es.wikipedia.org/wiki/Tratado_de_No_Proliferación_Nuclear) y un montón de otros acuerdos internacionales, que han sido clave para prevenir la propagación de armas nucleares y promover el objetivo de lograr el desarme nuclear. Gracias a ellos, hemos disuadido a muchos países de perseguir programas de armas nucleares, reducido la cantidad de arsenales nucleares desde los años 90, y evitado la guerra nuclear durante muchas décadas. Todos logros increíbles.
- La [Agencia Internacional de Energía Atómica](https://es.wikipedia.org/wiki/Agencia_Internacional_de_Energía_Atómica), que es una organización intergubernamental compuesta por 178 estados miembros que busca promover el uso pacífico de la energía nuclear y evitar su uso para fines militares. Independientemente de si crees que la energía nuclear está sobre-regulada o no, la AIEA se considera un buen ejemplo de una herramienta internacional que podríamos tener para evaluar la seguridad de los modelos de IA más grandes.
- Y la [Declaración de las Naciones Unidas sobre la Clonación Humana](https://es.wikipedia.org/wiki/Declaración_de_las_Naciones_Unidas_sobre_la_Clonación_Humana), que pidió a los estados miembros que prohibieran la clonación humana en 2005 y llevó a muchos de ellos a hacerlo. Es un caso interesante porque ahora, casi 20 años después y sin un acuerdo formal, 60 países han prohibido la clonación humana total o parcialmente y no ha habido un solo caso (verificado) de un ser humano clonado. Así que, en cierto sentido, sugiere la posibilidad de que muchas regulaciones unilaterales sean suficientes para prevenir el desarrollo de otras tecnologías peligrosas.

Si crees que la IA es similar a otros casos en los que no logramos hacer buenos tratados internacionales: todo lo que ha sucedido tuvo una primera vez.
Había particularidades que los hicieron la primera vez y esa es una razón para abordar [las particularidades de la IA](#ai-particular-case).

### Impacto de las protestas {#impact-of-protests}

Es bastante común que la gente cuestione la efectividad de las protestas y los movimientos sociales en general.
Por supuesto, hay muchos casos en los que las manifestaciones no producen resultados, pero también hay situaciones en las que las demandas de los manifestantes se cumplen y parece probable que esos resultados [fueran causados por las protestas](https://www.socialchangelab.org/_files/ugd/503ba4_052959e2ee8d4924934b7efe3916981e.pdf).
Y [hay razones para creer que el activismo de la IA podría lograr resultados similares](https://forum.effectivealtruism.org/posts/WfodoyjePTTuaTjLe/efficacy-of-ai-activism-have-we-ever-said-no).

Si no te gusta la idea de protestar de todos modos, también [tomamos otras acciones](/action), como contactar [directamente a los gobiernos](/lobby-tips).

## Caso particular de la IA {#ai-particular-case}

Si crees que la IA es lo suficientemente diferente a estos casos (o incluso si no lo crees), es útil analizar su situación particular.
Las cosas que hacen que la IA sea diferente pueden no necesariamente hacer que sea menos regulable.
Por ejemplo, no estamos [tratando de regular productos y servicios existentes que la gente ya disfruta y utiliza regularmente](/proposal), y no estamos en contra de muchas empresas que pueden hacer lobby o trabajadores que perderían sus empleos si tenemos éxito. Más bien al contrario.

Otro punto a favor es que el público no está partidista ni políticamente dividido, sino [unido en apoyo a la regulación](https://drive.google.com/file/d/1n0pXDBuIcb01tW4TQdP1Mb5aAiFDvWk0/view).
Sin embargo, debemos tener cuidado de no desanimarlos, escuchar sus perspectivas y ver de qué manera particular pueden ser ayudados por una pausa basada en lo que les preocupa. Dado que mucha gente aún no ha decidido qué tipo de regulación apoya.

Cuando se trata de los riesgos de la IA, el público y los expertos parecen [preocupados por los riesgos y interesados en la regulación](/polls-and-surveys).
Los políticos, basándose en las [políticas que se están aprobando y trabajando](https://www.bloomberg.com/news/articles/2024-03-13/regulate-ai-how-us-eu-and-china-are-going-about-it), las [cumbres que están organizando](/summit), y las [declaraciones que están haciendo](/quotes), parecen bastante preocupados también.
Incluso un reciente [informe encargado por el gobierno de EE. UU.](https://time.com/6898967/ai-extinction-national-security-risks-report/) recomienda, entre varias propuestas, diferentes tipos de pausas en el desarrollo de la IA para evitar riesgos para la seguridad nacional y la humanidad en su conjunto.

Todo esto está sucediendo mientras PauseAI todavía es bastante joven y la mayoría de la gente no ha oído hablar de la mayoría de los riesgos.
Si lográramos aumentar la conciencia y el consenso sobre los riesgos existenciales, por ejemplo, tendríamos el potencial de volverse más mainstream dado que prácticamente nadie quiere morir o que el mundo termine.
Eso no es algo que esté siquiera en el mejor interés de las empresas, gobiernos y personas más egoístas.

Incluso si lleva tiempo, la manifestación de los problemas que la IA traerá en los próximos años potenciará la conciencia de ellos y eventualmente desencadenará más y más regulación.
En el caso de que no obtengamos una pausa tan pronto como nos gustaría, el desempleo masivo y todo tipo de incidentes podrían poner a la mayoría de la gente en la misma página, ya sea progresivamente o de repente, y hacer que las personas que no habrían considerado seriamente una pausa, en realidad lo hagan.
Eso es por lo que es importante no basar nuestro potencial para tener éxito en resultados a corto plazo, sino estar siempre preparados para nuevos adherentes y aliados, y estar listos para guiar a los políticos para implementar nuestras propuestas en caso de que suceda un disparo de advertencia.

## Beneficios colaterales {#collateral-benefits}

Abogar por una pausa tiene otros impactos positivos fuera de lograrla.
Informar al público, a las personas técnicas y a los políticos sobre los riesgos ayuda a otras intervenciones que apuntan a hacer que las IA sean seguras y las IA sean seguras.
Hace que la gente dé más importancia al trabajo técnico, político y comunicacional que se realiza en la seguridad y la ética de la IA, lo que en última instancia significa más financiamiento y empleos en ellos, esperando mejores resultados.

No solo traería nuevas personas y recursos a nuevas intervenciones, sino que también ayudaría a que las iniciativas técnicas y políticas moderadas parezcan más "razonables" y aumenten sus posibilidades de ser apoyadas.

Además, podría preparar a la gente para los peligros, enseñarles a utilizar la IA de manera más ética, e incluso convencerlos de no invertir o trabajar en proyectos de vanguardia y peligrosos.

## No te rindas al pesimismo {#dont-give-in-to-pessimism}

Entendemos de dónde vienen las creencias pesimistas sobre la regulación fuerte y que no será fácil.
Pero no es fácil predecir el futuro tampoco, y este artículo busca argumentar en contra de la confianza excesiva en nuestra impotencia, ya que lo único que puede hacer es actuar como una profecía autocumplida.

Lo único fácil de hacer es rendirse, [es la salida fácil](/psychology-of-x-risk#difficult-to-act-on).
Porque si no hay nada que podamos hacer, no hay nada que debamos hacer.
Pero no deberíamos rendirnos sin siquiera intentarlo.
Esta es en realidad nuestra mejor oportunidad de tener un impacto en el mundo y el futuro de nuestra civilización.

## La teoría de la decisión dice: inténtalo de todos modos {#decision-theory-says-try-it-anyways}

Incluso si crees que una pausa es bastante improbable y no te importan los otros beneficios, a menos que no creas en los mayores riesgos o tengas mejores estrategias en mente, te recomendamos que [te unas](/join). No entierres tu cabeza en la arena y espera a morir o ser salvado, puedes ayudarnos a lograr esto.
