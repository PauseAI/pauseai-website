---
title: Preguntas frecuentes
description: Preguntas frecuentes sobre PauseAI y los riesgos de la inteligencia artificial superinteligente.
---

<script>
    import SimpleToc from '$lib/components/simple-toc/SimpleToc.svelte'
</script>
<style>
    h2 {
        font-size: 1.2rem;
    }
</style>

<SimpleToc />

## ¿Quiénes somos? {#who-are-you}

Somos una comunidad de voluntarios y [comunidades locales](/communities) coordinadas por una [organización sin fines de lucro](/organization) que busca mitigar los [riesgos de la IA](/risks) (incluyendo el [riesgo de extinción humana](/xrisk)).
Nuestro objetivo es convencer a nuestros gobiernos para que intervengan y [pausen el desarrollo de la IA superhumana](/proposal).
Lo hacemos informando al público, hablando con los tomadores de decisiones y organizando [eventos](/events).

## ¿Tienen redes sociales? {#do-you-have-social-media}

Puedes encontrarnos en [Discord](https://discord.gg/NuqkHutXW3) (¡este es donde sucede la mayor parte de la coordinación!), [Twitter](https://twitter.com/PauseAI), [Substack](https://substack.com/@pauseai), [Facebook](https://www.facebook.com/PauseAI), [TikTok](https://www.tiktok.com/@pauseai), [LinkedIn](https://www.linkedin.com/uas/login?session_redirect=/company/97035448/), [YouTube](https://www.youtube.com/@PauseAI), [Instagram](https://www.instagram.com/pause_ai), [Telegram](https://t.me/+UeTsIsNkmt82ZmQ8), [Whatsapp](https://chat.whatsapp.com/JgcAbjqRr8X3tvrXdeQvfj) y [Reddit](https://www.reddit.com/r/PauseAI/).
Puedes enviarnos un correo electrónico a [joep@pauseai.info](mailto:joep@pauseai.info).

## ¿No son solo miedo a los cambios y la nueva tecnología? {#arent-you-just-scared-of-changes-and-new-technology}

Puede que te sorprenda que la mayoría de las personas en PauseAI se consideran tecno-optimistas.
Muchos de ellos están involucrados en el desarrollo de la IA, son entusiastas de la tecnología y han estado emocionados con el futuro.
En particular, muchos de ellos han estado emocionados con el potencial de la IA para ayudar a la humanidad.
Por eso, para muchos de ellos, la triste realización de que la IA podría ser un riesgo existencial fue un golpe duro.

## ¿Quieren prohibir toda la IA? {#do-you-want-to-ban-all-ai}

No, solo el desarrollo de los sistemas de IA de propósito general más grandes y complejos, a menudo llamados "modelos de frontera".
Casi toda la IA existente sería legal bajo [nuestra propuesta](/proposal), y la mayoría de los modelos de IA futuros seguirán siendo legales también.
Estamos pidiendo una prohibición de los sistemas de IA más poderosos, hasta que sepamos cómo construir una IA segura y la tengamos bajo control democrático.

## ¿Creen que GPT-4 va a matarnos? {#do-you-believe-gpt-4-is-going-to-kill-us}

No, no pensamos que los [modelos de IA actuales](/sota) sean una amenaza existencial.
Parece probable que la mayoría de los próximos modelos de IA no lo sean tampoco.
Pero si seguimos construyendo sistemas de IA más y más poderosos, eventualmente llegaremos a un punto en el que uno se convertirá en una [amenaza existencial](/xrisk).

## ¿Puede una pausa tener un efecto contrario y empeorar las cosas? {#can-a-pause-backfire-and-make-things-worse}

Hemos abordado estas preocupaciones en [este artículo](/mitigating-pause-failures).

## ¿Es posible una pausa? {#is-a-pause-even-possible}

La AGI no es inevitable.
Requiere grandes recursos y esfuerzos de investigación y desarrollo.
Requiere una cadena de suministro completamente funcional y sin restricciones del hardware más complejo.
Requiere que todos nosotros permitamos que estas empresas jueguen con nuestro futuro.

[Lea más sobre la viabilidad de una pausa](/feasibility).

## ¿Quién los financia? {#who-is-paying-you}

Ver nuestra [página de financiamiento](/funding)

## ¿Cuáles son sus planes? {#what-are-your-plans}

Centrarnos en [hacer crecer el movimiento](/growth-strategy), organizar protestas, presionar a los políticos y informar al público.

Consulte nuestro [plan de acción](/roadmap) para una visión general detallada de nuestros planes y lo que podríamos hacer con más financiamiento.

## ¿Cómo creen que pueden convencer a los gobiernos para que pausen la IA? {#how-do-you-think-you-can-convince-governments-to-pause-ai}

Consulte nuestra [teoría del cambio](/theory-of-change) para una visión general detallada de nuestra estrategia.

## ¿Por qué protestan? {#why-do-you-protest}

- Protestar muestra al mundo que nos importa este tema. Al protestar, mostramos que estamos dispuestos a gastar nuestro tiempo y energía para que la gente escuche.
- Las protestas pueden y a menudo influyen positivamente en la opinión pública, el comportamiento de voto, el comportamiento corporativo y la política.
- La mayoría de las personas [apoyan las protestas pacíficas/no violentas](https://today.yougov.com/politics/articles/31718-do-protesters-want-help-or-hurt-america)
- No hay evidencia de un efecto "contraproducente" [a menos que la protesta sea violenta](https://news.stanford.edu/2018/10/12/how-violent-protest-can-backfire/). Nuestras protestas son pacíficas y no violentas.
- Es una experiencia de unión social. Conoces a otras personas que comparten tus preocupaciones y disposición a tomar medidas.
- Consulte [este artículo](https://forum.effectivealtruism.org/posts/4ez3nvEmozwPwARr9/a-case-for-the-effectiveness-of-protest) para obtener más información sobre por qué funcionan las protestas

Si deseas [organizar una protesta](/organizing-a-protest), podemos ayudarte con consejos y recursos.

## ¿Qué tan probable es que la IA superinteligente cause resultados muy malos, como la extinción humana? {#how-likely-is-it-that-superintelligent-ai-will-cause-very-bad-outcomes-like-human-extinction}

Hemos recopilado [una lista de estimaciones de probabilidad de resultados malos](/pdoom) de varios expertos notables en el campo.

Los investigadores de seguridad de la IA (que son los expertos en este tema) están divididos en esta pregunta, y las estimaciones [varían del 2% al 97% con un promedio del 30%](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results).
Tenga en cuenta que ningún investigador de seguridad de la IA encuestado cree que haya una probabilidad del 0%.
Sin embargo, puede haber un sesgo de selección aquí: las personas que trabajan en el campo de la seguridad de la IA probablemente lo hacen porque creen que prevenir resultados malos de la IA es importante.

Si preguntas a los investigadores de la IA en general (no especialistas en seguridad), este número baja a un [promedio de alrededor del 14%](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/), con una mediana del 5%.
La gran mayoría, el 86% de ellos, cree que el problema de la alineación es un problema real y importante.
Tenga en cuenta que puede haber un sesgo de selección en la dirección opuesta: las personas que trabajan en la IA probablemente lo hacen porque creen que la IA será beneficiosa.

_Imagina que estás invitado a un vuelo de prueba en un avión nuevo_.
Los ingenieros del avión piensan que hay un 14% de probabilidad de que se estrelle.
¿Subirías a ese avión? Porque ahora mismo, todos estamos abordando el avión de la IA.

## ¿Cuánto tiempo tenemos hasta que la IA sea superinteligente? {#how-long-do-we-have-until-superintelligent-ai}

Puede tomar meses, puede tomar décadas, nadie lo sabe con certeza.
Sin embargo, sabemos que el ritmo del progreso de la IA a menudo se subestima.
Hace solo tres años, pensábamos que tendríamos sistemas de IA que superarían el SAT en 2055.
Llegamos allí en abril de 2023.
Debemos actuar como si tuviéramos muy poco tiempo porque no queremos ser sorprendidos.

[Lea más sobre la urgencia](/urgency).

## Si hacemos una pausa, ¿qué pasa con China? {#if-we-pause-what-about-china}

Para empezar, en este momento, China tiene regulaciones de IA más estrictas que prácticamente cualquier otro país.
No [permitieron chatbots](https://www.reuters.com/technology/chinas-slow-ai-roll-out-points-its-tech-sectors-new-regulatory-reality-2023-07-12/) y [prohibieron el entrenamiento con datos de Internet](https://cointelegraph.com/news/china-sets-stricter-rules-training-generative-ai-models) hasta [septiembre de 2023](https://asia.nikkei.com/Business/Technology/China-approves-AI-chatbot-releases-but-will-it-unleash-innovation).
China tiene un gobierno más controlador y, por lo tanto, tiene aún más razones para temer los impactos incontrolables y impredecibles de la IA.
Durante la reunión del Consejo de Seguridad de la ONU sobre la seguridad de la IA, China fue el único país que mencionó la posibilidad de implementar una pausa.

También tenga en cuenta que estamos pidiendo una pausa _internacional_, impuesta por un tratado.
Tal tratado también debe ser firmado por China.
Si el tratado garantiza que otras naciones también se detendrán, y hay mecanismos de aplicación suficientes en su lugar,
esto debería ser algo que China también querrá ver.

## OpenAI y Google dicen que quieren ser regulados. ¿Por qué están protestando contra ellos? {#openai-and-google-are-saying-they-want-to-be-regulated-why-are-you-protesting-them}

Aplaudimos a [OpenAI](https://openai.com/blog/governance-of-superintelligence) y [Google](https://www.ft.com/content/8be1a975-e5e0-417d-af51-78af17ef4b79) por sus llamados a la regulación internacional de la IA.
Sin embargo, creemos que las propuestas actuales no son suficientes para prevenir una catástrofe de la IA.
Google y Microsoft aún no han declarado públicamente nada sobre el riesgo existencial de la IA.
Solo OpenAI [menciona explícitamente el riesgo de extinción](https://openai.com/blog/governance-of-superintelligence), y nuevamente los aplaudimos por tomar este riesgo en serio.
Sin embargo, su estrategia es bastante explícita: una pausa es imposible, debemos llegar a la superinteligencia primero.
El problema con esto, sin embargo, es que [no creen que hayan resuelto el problema de la alineación](https://youtu.be/L_Guz73e6fw?t=1478).
Las empresas de IA están atrapadas en una carrera hacia el fondo, donde la seguridad de la IA se sacrifica por una ventaja competitiva.
Esto es simplemente el resultado de la dinámica del mercado.
Necesitamos que los gobiernos intervengan e implementen políticas (a nivel internacional) que [prevengan los peores resultados](/proposal).

## ¿Las empresas de IA están empujando la narrativa del riesgo existencial para manipularnos? {#are-ai-companies-pushing-the-existential-risk-narrative-to-manipulate-us}

No podemos saber con certeza qué motivaciones tienen estas empresas, pero sabemos que **el riesgo existencial no fue inicialmente impulsado por las empresas de IA: fueron científicos, activistas y ONG**.
Veamos la cronología.

Ha habido muchas personas que han advertido sobre el riesgo existencial desde principios de la década de 2000.
Eliezer Yudkowsky, Nick Bostrom, Stuart Russell, Max Tegmark y muchos otros.
No tenían tecnología de IA para impulsar: simplemente estaban preocupados por el futuro de la humanidad.

Las empresas de IA nunca mencionaron el riesgo existencial hasta hace muy poco.

Sam Altman es una excepción interesante.
Escribió sobre el riesgo existencial de la IA [en 2015, en su blog personal](https://blog.samaltman.com/machine-intelligence-part-1), antes de fundar OpenAI.
En los años siguientes, no hizo ninguna mención explícita del riesgo existencial nuevamente.
Durante la audiencia del Senado el 16 de mayo de 2023, cuando se le preguntó sobre su publicación de blog sobre el riesgo existencial, solo respondió hablando de empleos y la economía.
No estaba impulsando la narrativa del riesgo existencial aquí, estaba evitándola activamente.

En mayo de 2023, todo cambió:

- El 1 de mayo, el 'padre de la IA' Geoffrey Hinton [renuncia a su trabajo en Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) para advertir sobre el riesgo existencial.
- El 5 de mayo, se anuncia la [primera protesta de PauseAI](https://twitter.com/Radlib4/status/1654262421794717696), justo en la puerta de OpenAI.
- El 22 de mayo, OpenAI publicó [una publicación de blog sobre la gobernanza de la superinteligencia](https://openai.com/blog/governance-of-superintelligence), y mencionó el riesgo existencial por primera vez.
- El 24 de mayo, el ex CEO de Google, Eric Schmidt, reconoce el riesgo existencial.
- El 30 de mayo, se publicó la [declaración de Safe.ai (reconociendo el riesgo existencial)](https://www.safe.ai/statement-on-ai-risk). Esta vez, incluyendo personas de OpenAI, Google y Microsoft.

Estas empresas han sido muy lentas en reconocer el riesgo existencial, considerando que muchos de sus empleados han sido conscientes de él durante años.
Entonces, en nuestra opinión, las empresas de IA no están impulsando la narrativa del riesgo existencial, han sido reactivas a otros que lo impulsan, y han esperado con su respuesta hasta que fue absolutamente necesario.

Los incentivos comerciales apuntan en la dirección opuesta: las empresas preferirían que la gente no se preocupe por los riesgos de sus productos.
Prácticamente todas las empresas restan importancia a los riesgos para atraer a los clientes y las inversiones, en lugar de exagerarlos.
¿Cuánta regulación estricta y atención negativa están invitando las empresas debido a admitir estos peligros?
¿Y dedicaría una empresa como OpenAI [el 20% de sus recursos informáticos](https://openai.com/blog/introducing-superalignment) a la seguridad de la IA si no creyera en los riesgos?

Aquí está nuestra interpretación: las empresas de IA firmaron la declaración porque _saben que el riesgo existencial es un problema que debe tomarse muy en serio_.

¿Una gran razón por la que muchas otras personas aún no quieren creer que el riesgo existencial es una preocupación real?
Porque reconocer que _estamos en peligro_ es algo muy, muy aterrador.

[Lea más sobre la psicología del riesgo existencial](/psychology-of-x-risk).

## ¡Está bien, quiero ayudar! ¿Qué puedo hacer? {#ok-i-want-to-help-what-can-i-do}

Hay muchas [cosas que puedes hacer](/action).
Por tu cuenta, puedes escribir una [carta](/writing-a-letter), publicar [volantes](/flyering), [aprender](/learn) e informar a otros, unirte a una [protesta](/protests), o [donar](/donate) algo de dinero.
Pero aún más importante: puedes [unirte a PauseAI](/join) y coordinar con otros que están tomando medidas.
Consulta si hay [comunidades locales](/communities) en tu área.
Si deseas contribuir más, puedes convertirte en voluntario y unirte a uno de nuestros [equipos](/teams), o [establecer una comunidad local](/local-organizing).

Incluso cuando enfrentamos el fin del mundo, todavía puede haber esperanza y un trabajo muy gratificante que hacer.
