---
title: PauseAI en Australia
slug: australia
description: el capítulo australiano de PauseAI
---

**Un mensaje de los voluntarios de PauseAI en Australia:**

Para 2030, la inteligencia artificial podría estar completamente automatizada, auto-mejorada y **más inteligente que los humanos en casi todos los aspectos**. Esto no es ciencia ficción, es la evaluación de las principales empresas y expertos en IA. Cuando esto suceda, todos los aspectos de la vida cambiarán para siempre.

**[Firma la petición a la Cámara de Representantes de Australia](https://www.aph.gov.au/e-petitions/petition/EN7777)**

**[Únete a nuestra comunidad](/join)** | [Envíanos un correo electrónico](mailto:australia@pauseai.info) | [Conéctate con nosotros en Facebook](https://www.facebook.com/groups/571590459293618) | [Canal de YouTube](https://www.youtube.com/channel/UCjjMieiOlSFf7jud0yhHQSg) | [LinkedIn](https://www.linkedin.com/company/pauseai-australia) | [Eventos](https://lu.ma/PauseAIAustralia)

### ¿Qué riesgos enfrentamos? {#what-risks-are-we-facing}

La inteligencia artificial está avanzando [a un ritmo vertiginoso](/urgency). Algunos expertos como [Sam Altman](https://time.com/7205596/sam-altman-superintelligence-agi/), [Dario Amodei](https://arstechnica.com/ai/2025/01/anthropic-chief-says-ai-could-surpass-almost-all-humans-at-almost-everything-shortly-after-2027/) y [Geoffrey Hinton](https://en.wikipedia.org/wiki/Artificial_general_intelligence) advierten que **la IA podría superar la inteligencia humana en los próximos cinco años**. Sin cooperación internacional, esto podría resultar en caos económico, guerra y hasta [extinción humana](/xrisk).

> "A medida que la IA de propósito general se vuelve más capaz, la evidencia de riesgos adicionales está emergiendo gradualmente. Estos incluyen riesgos como impactos a gran escala en el mercado laboral, ataques cibernéticos o biológicos habilitados por IA, y la sociedad perdiendo el control sobre la IA de propósito general".
>
> – [Informe de Seguridad de IA Internacional (2025)](https://assets.publishing.service.gov.uk/media/679a0c48a77d250007d313ee/International_AI_Safety_Report_2025_accessible_f.pdf), coautorizado por 96 expertos de 30 países, incluyendo Australia.

### ¿No queremos los beneficios de la IA? {#dont-we-want-ais-benefits}

Claro que sí. La inteligencia artificial ya tiene el potencial de ser una herramienta poderosa. Si la IA permanece bajo control, podría ser utilizada para curar enfermedades, impulsar avances científicos y difundir oportunidades y bienestar. Pero sería trágico lograr estos avances solo para luego [perder el control](/ai-takeover) y sufrir pérdidas catastróficas.

> "Parecemos asumir que la IA se ajustará a un patrón benigno. Esa suposición solo se sostiene en la medida en que la IA sea análoga a la mayoría de lo que ha venido antes. Y en las circunstancias, seríamos sabios al examinarla con mucha más rigidez antes de decidirnos por ella, porque hay buenas razones para suponer que es una especie diferente por completo, para la cual la historia es una guía pobre".
>
> – Waleed Aly
>
> [The Age](https://www.theage.com.au/politics/federal/the-treasurer-is-telling-us-to-stay-calm-but-this-could-be-the-time-to-panic-20250807-p5ml5k.html)

Las nuevas tecnologías siempre han traído cambios, pero los humanos necesitan tiempo para adaptarse, salvaguardar y planificar para el futuro. Para cualquier otra tecnología, ya sean aviones, rascacielos o nuevos medicamentos, insistimos en medidas de seguridad diseñadas por expertos antes de exponer al público a riesgos. Esto no está sucediendo con la IA.

Las empresas de IA están en una carrera, impulsadas por miles de millones de dólares de inversión, para construir una IA superhumana primero. Cuando una empresa tenga éxito, tu vida y la de tus seres queridos cambiarán radicalmente, y no tendrás ninguna voz en lo que este futuro depara. Esto no es solo un problema tecnológico, afectará a todos.

### ¿Qué se puede hacer? {#what-can-be-done}

PauseAI [propone](/proposal) un tratado internacional para pausar el desarrollo de la IA general más inteligente que los humanos hasta que haya un plan creíble para asegurarse de que sea segura. Está en el interés de Australia abogar por esto.

> "¿Quién mostrará liderazgo en la negociación de un tratado de no proliferación de IA? Es una responsabilidad colectiva y ciertamente una a la que Australia podría contribuir".
>
> – Alan Finkel, científico jefe de Australia (2016-2020)
>
> [Sydney Morning Herald](https://www.smh.com.au/technology/the-ai-horse-has-bolted-it-s-time-for-the-nuclear-option-20230807-p5duel.html)

La historia muestra que los países más pequeños pueden hacer una gran diferencia en la resolución de problemas globales. Tomemos el ejemplo de la prohibición de la caza de ballenas en 1982 y el acuerdo de 1987 para proteger la capa de ozono. Australia, que solía cazar ballenas, se convirtió en un líder en la protección de la vida marina al apoyar la prohibición y llevar a Japón a los tribunales por su caza de ballenas. Australia también ayudó a proteger el medio ambiente al unirse rápidamente al acuerdo para dejar de utilizar productos químicos que dañaban la capa de ozono. Estas historias muestran que países como Australia pueden hacer que el cambio real suceda en todo el mundo tomando medidas y trabajando con otras naciones.

### ¿No hay problemas más importantes? {#arent-there-more-important-issues}

Estamos de acuerdo en que hay muchos problemas importantes que enfrenta Australia, pero no podremos resolverlos en un mundo con IA descontrolada. Australia debería abogar por un tratado internacional al mismo tiempo que trabaja en otros problemas.

### ¿Por qué no se está haciendo nada ya? {#why-isnt-anything-being-done-already}

Los políticos australianos han examinado algunos de los riesgos más pequeños de la IA, pero no los grandes. A partir de las últimas elecciones, [los principales partidos no tienen un plan claro](https://www.australiansforaisafety.com.au/scorecard).

Reconocemos que no todos están de acuerdo sobre el riesgo de una catástrofe de IA. Abordamos algunas de las objeciones comunes [aquí](/faq). No afirmamos estar 100% seguros, pero creemos que la probabilidad de resultados muy malos es lo suficientemente alta como para justificar una pausa.

Es [psicológicamente difícil](/psychology-of-x-risk) pensar en posibles catástrofes. Muchas personas asumen que los riesgos están fuera de su control y, por lo tanto, no vale la pena preocuparse por ellos. Sin embargo, cualquiera puede tomar medidas ahora mismo hablando. Creemos que es mejor actuar que simplemente preocuparse.

### ¿Cómo puedo ayudar en Australia? {#how-can-i-help-in-australia}

Puedes hacer una diferencia. Los voluntarios en Australia crean conciencia, protestan, presionan y apoyan el movimiento global de PauseAI.

- [Únete a nuestra comunidad](/join)
- [Asiste a nuestro próximo evento en línea o en persona en Australia](https://lu.ma/PauseAIAustralia)
- Firma la carta abierta de Australians for AI Safety [aquí](https://www.australiansforaisafety.com.au/letters)
- [Contacta a los políticos](/writing-a-letter)
- Habla con tus amigos y familiares sobre el riesgo de la IA

### Campañas {#campaigns}

#### Investigación de OpenAI {#investigate-openai}

[En julio de 2025](https://drive.google.com/file/d/1t9ntUlF2cZH4_f-1fsp0FFCf3RiGZ81g/view?usp=drive_link), el voluntario Mark Brown llevó OpenAI a la atención de la Policía Federal de Australia y el Fiscal General de Australia, alegando posibles violaciones de la _Ley de Armas Biológicas (Crímenes) de 1976_. Se discutió en una [noticia](https://ia.acs.org.au/article/2025/is-the-new-chatgpt-agent-really-a-weapons-risk-.html) y en [un video podcast](https://youtu.be/-YPhNdpA8Rk?si=dTBpGMfZaNWnldXa). Todavía estamos esperando una respuesta de la AFP y el Fiscal General.
