---
title: 'PauseAI presenta: La protesta contra Google DeepMind'
slug: deepmind-protest-2025
description: Estamos organizando una protesta para exigir a Google DeepMind que cumpla con sus compromisos de seguridad incumplidos.
date: 2025-06-30T17:00:00.000Z
---

PauseAI llevó a cabo su mayor protesta hasta la fecha frente a la oficina de Google DeepMind en Londres.

## Cobertura mediática {#media-coverage}

- [Business Insider](https://www.businessinsider.com/protesters-accuse-google-deepmind-breaking-promises-ai-safety-2025-6)
- [Islignton Tribune](https://www.islingtontribune.co.uk/article/stark-warning-from-protesters-calling-for-ai-pause-its-going-to-turn-out-bad)
- [Times of India](https://www.islingtontribune.co.uk/article/stark-warning-from-protesters-calling-for-ai-pause-its-going-to-turn-out-bad)
- [Tech Times](https://www.techtimes.com/articles/311120/20250701/google-deepmind-slammed-protesters-over-broken-ai-safety-promise.htm)

## Google DeepMind ha incumplido sus compromisos de seguridad {#google-deepmind-have-broken-their-promises}

En 2024, Google se comprometió públicamente en la Cumbre de Inteligencia Artificial en Seúl a realizar pruebas rigurosas de sus modelos de inteligencia artificial. Firmaron los Compromisos de Seguridad de la Inteligencia Artificial de Vanguardia, comprometiéndose a considerar los resultados de evaluadores independientes terceros cuando corresponda y a proporcionar transparencia total sobre el proceso, incluida la participación gubernamental.

Sin embargo, en marzo de 2025, Google lanzó Gemini 2.5 Pro, su modelo de inteligencia artificial más avanzado hasta la fecha, sin cumplir con sus compromisos de seguridad. Los expertos en seguridad buscaron el informe de pruebas prometido, pero no encontraron nada. No había evaluación externa ni informe de transparencia. Solo silencio.

Un mes después, bajo presión, Google publicó una "tarjeta de modelo" básica con algunas evaluaciones internas, pero sin mencionar evaluaciones externas. Más tarde agregaron referencias vagas a "evaluadores externos", pero no proporcionaron detalles. Cuando Fortune preguntó directamente si los gobiernos estaban involucrados, Google se negó a responder, violando su compromiso de transparencia.

Google había hecho compromisos similares con la Casa Blanca en 2023 y firmó el Código de Conducta Internacional del Proceso de Hiroshima en 2025. Con el lanzamiento de Gemini 2.5 Pro, Google parece haber violado, al menos en espíritu, estos otros conjuntos de compromisos de seguridad.

Lee la cronología completa de los compromisos de seguridad incumplidos de Google DeepMind [aquí](https://pauseai.info/google-deepmind-broken-promises).

## El desarrollo de inteligencia artificial insegura nos amenaza a todos {#unsafe-ai-development-threatens-us-all}

Aunque los modelos de inteligencia artificial actuales no son lo suficientemente peligrosos como para causar destrucción masiva, el desarrollo de inteligencia artificial se está acelerando de manera impredecible. Necesitamos pruebas rigurosas de cada generación para evitar ser sorprendidos por saltos repentinos en capacidad. La falta de respeto de Google por los compromisos de seguridad establece un precedente peligroso.

A medida que la inteligencia artificial se vuelve más poderosa, las presiones competitivas se intensificarán y las apuestas serán más altas. Si permitimos que las empresas ignoren los compromisos de seguridad ahora, cuando los riesgos son relativamente bajos, ¿qué esperanza tenemos de responsabilizarlas cuando los sistemas de inteligencia artificial puedan plantear amenazas existenciales?

Las normas que establezcamos hoy darán forma a cómo se desarrolla la tecnología más poderosa de la historia humana.

## Exijamos responsabilidad a DeepMind {#lets-hold-deepmind-accountable}

PauseAI es un movimiento en crecimiento que se niega a aceptar que la seguridad de la inteligencia artificial sea un pensamiento posterior. El 30 de junio, nos reuniremos frente a la sede de Google DeepMind en Londres con un mensaje simple: Cumplan sus compromisos.

Nuestra petición es simple: que Google cumpla los compromisos que ya ha hecho. Publique informes oportunos y transparentes sobre los resultados de las pruebas de seguridad previas a la implementación.

Nuestro objetivo final es un moratorio sobre el desarrollo de inteligencia artificial de vanguardia hasta que podamos asegurarnos de que los sistemas avanzados serán seguros. Pero necesitamos una cosa ahora mismo: responsabilidad básica de una de las empresas de inteligencia artificial más poderosas del mundo.

El futuro de la inteligencia artificial será moldeado por los precedentes que establezcamos hoy.

**Llamamos a Google DeepMind a:**

1. **Establecer definiciones claras** de "implementación" que se alineen con la comprensión común: cuando un modelo es accesible públicamente, está implementado.
2. **Publicar un cronograma específico** para cuando se publicarán los informes de evaluación de seguridad para todos los modelos futuros.
3. **Aclarar de manera inequívoca**, para cada lanzamiento de modelo, qué agencias gubernamentales y terceros independientes están involucrados en las pruebas, y los cronogramas exactos de sus procedimientos de prueba.

Únete a nosotros en Londres el lunes 30 de junio inscribiéndote [aquí](https://lu.ma/bvffgzmb).
