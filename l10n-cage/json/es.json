{
	"$schema": "https://inlang.com/schema/inlang-message-format",
	"header__instructions": "Todas las traducciones con el prefijo 'header_' deben ser lo más breves posible para adaptarse al diseño. Intenta mantenerlas en una sola palabra mientras sigan siendo útiles como enlaces a las páginas.",
	"header_action__instructions": "Alemán: Handeln",
	"header_action": "Actúa",
	"header_donate": "Donar",
	"header_events__instructions": "En algunos idiomas, 'calendario' o 'fechas' podrían ser mejores opciones para cumplir con el objetivo de una traducción breve. (Alemán: Termine)",
	"header_events": "Eventos",
	"header_faq": "Preguntas frecuentes",
	"header_join__instructions": "En algunos idiomas, 'unirse' podría ser más apropiado si la traducción no consta de dos palabras.",
	"header_join": "Únete",
	"header_learn": "Aprender",
	"header_proposal": "Propuesta",
	"home_action_c2a": "Toma acción",
	"home_action_content": "Demasiadas personas desconocen los riesgos potenciales de la IA. Informa a otros y ayuda a detener esta carrera hacia el abismo.",
	"home_action_title": "<u>Tú</u> puedes ayudar",
	"home_hero__instructions": "Esto se dirige al lector, tradúcelo de manera informal.",
	"home_hero": "No permitas que las empresas de IA jueguen con nuestro futuro",
	"home_proposal_c2a": "Lee la propuesta",
	"home_proposal_content__instructions": "'Detener' no se dirige al lector.",
	"home_proposal_content": "Detener el desarrollo de los sistemas de IA más poderosos hasta que sepamos cómo hacerlos seguros. Esto debe suceder a nivel internacional y debe suceder pronto.",
	"home_proposal_title": "Necesitamos una <u>pausa</u>",
	"home_quotes_all": "Ver todas las citas",
	"home_quotes_bengio_text": "La IA descontrolada puede ser peligrosa para toda la humanidad. Prohibir los sistemas de IA poderosos (digamos más allá de las capacidades de GPT-4) que se les da autonomía y agencia sería un buen comienzo.",
	"home_quotes_bengio_title": "Ganador del premio Turing de IA",
	"home_quotes_cais_text": "Mitigar el riesgo de extinción de la IA debe ser una prioridad global junto con otros riesgos a escala social como las pandemias y la guerra nuclear.",
	"home_quotes_cais_author": "Declaración sobre el riesgo de IA",
	"home_quotes_cais_title": "Firmado por cientos de expertos, incluyendo los principales laboratorios y científicos de IA",
	"home_quotes_hawking_text": "El desarrollo de la inteligencia artificial completa podría significar el fin de la raza humana.",
	"home_quotes_hawking_title": "Físico teórico y cosmólogo",
	"home_quotes_hinton_text": "Si tomas en serio el riesgo existencial, como lo hago yo ahora, podría ser muy sensato dejar de desarrollar estas cosas.",
	"home_quotes_hinton_title": "Ganador del premio Nobel y 'padre de la IA'",
	"home_quotes_russell_text": "Si seguimos [nuestro enfoque actual], eventualmente perderemos el control sobre las máquinas",
	"home_quotes_russell_title": "Autor del libro de texto de IA",
	"home_quotes_turing_text": "... debemos esperar que las máquinas tomen el control.",
	"home_quotes_turing_title": "Inventor de la computadora moderna",
	"home_risks_c2a": "Conoce los riesgos",
	"home_risks_content": "La IA puede tener beneficios increíbles, pero también podría erosionar nuestra democracia, desestabilizar nuestra economía y podría ser utilizada para crear armas cibernéticas poderosas.",
	"home_risks_title": "Corremos el riesgo de <u>perder el control</u>",
	"home_stats_alignment": "de científicos de IA creen que el problema de alineación es real e importante",
	"home_stats_citizens": "de ciudadanos quieren que la IA sea frenada por nuestros gobiernos",
	"home_urgency_c2a": "¿Cuánto tiempo tenemos?",
	"home_urgency_content": "En 2020, los expertos pensaban que teníamos más de 35 años hasta la AGI. Los recientes avances muestran que podríamos estar casi allí. La superinteligencia podría estar a una innovación de distancia, así que debemos proceder con cautela.",
	"home_urgency_title": "Necesitamos actuar <u>ahora</u>",
	"home_xrisk_c2a": "Cómo y por qué la IA podría ser una amenaza existencial",
	"home_xrisk_content": "Muchos laboratorios y expertos de IA están de acuerdo: la IA podría acabar con la humanidad.",
	"home_xrisk_title": "Corremos el riesgo de <u>extinción humana</u>",
	"simpletoc_heading": "Índice",
	"footer_join": "Únete a PauseAI >",
	"footer_info": "Información",
	"footer_info_faq": "Preguntas frecuentes",
	"footer_info_proposal": "Propuesta",
	"footer_info_learn": "Aprender",
	"footer_info_press": "Prensa / Medios",
	"footer_info_people": "Equipo",
	"footer_info_teams": "Equipos",
	"footer_info_partnerships": "Asociaciones",
	"footer_info_privacy": "Política de privacidad",
	"footer_info_legal": "Información legal",
	"footer_info_legal_foundation": "Stichting PauseAI",
	"footer_info_legal_kvk": "(kvk 92951031)",
	"footer_risks": "Riesgos",
	"footer_risks_overview": "Resumen de riesgos",
	"footer_risks_outcomes": "Resultados de la IA",
	"footer_risks_xrisk": "Riesgo existencial",
	"footer_risks_psychology": "Psicología del riesgo existencial",
	"footer_risks_takeover": "Toma de control de la IA",
	"footer_risks_cybersecurity": "Seguridad cibernética",
	"footer_risks_capabilities": "Capacidades peligrosas",
	"footer_risks_sota": "Estado del arte",
	"footer_risks_urgency": "Urgencia",
	"footer_action": "Actúa",
	"footer_action_join": "Únete a PauseAI",
	"footer_action_help": "Cómo puedes ayudar",
	"footer_action_communities": "Comunidades locales",
	"footer_action_donate": "Donar",
	"footer_action_merchandise": "Merchandising",
	"footer_action_events": "Eventos",
	"footer_action_vacancies": "Vacantes",
	"footer_action_email": "Constructor de correos electrónicos",
	"footer_action_lobby": "Consejos de lobby",
	"footer_other": "Otros",
	"footer_other_pages": "Todas las páginas",
	"footer_other_rss": "RSS",
	"footer_other_license": "Licencia: CC-BY 4.0",
	"footer_other_feedback": "Enviar comentarios",
	"footer_other_edit": "Editar contenido original en inglés",
	"footer_other_l10n": "Sugerir mejora de traducción",
	"newsletter_heading": "Suscríbete a nuestro boletín",
	"newsletter_description": "Mantente actualizado sobre nuestros esfuerzos para abogar por la seguridad de la IA.",
	"newsletter_disclaimer": "Nuestro boletín es completamente gratuito. No se requiere suscripción paga. Puedes cerrar Substack una vez que te hayas registrado.",
	"newsletter_email_placeholder": "Tu correo electrónico",
	"newsletter_button": "Suscribirse",
	"newsletter_success": "Gracias por suscribirte",
	"newsletter_error_default": "No se pudo suscribir. Por favor, inténtalo de nuevo.",
	"newsletter_error_network": "Error de red. Por favor, inténtalo más tarde.",
	"newsletter_loading": "Cargando...",
	"uk_email_mp_title": "Envía un correo electrónico a tu diputado",
	"uk_email_mp_description": "Contacta con tu diputado sobre la seguridad de la IA",
	"uk_email_mp_uk_only_notice": "<b>Esta herramienta es solo para el Reino Unido.</b> Para otros países, utiliza el [constructor de correos electrónicos global](/email-builder).",
	"learn_risks": "[Riesgos](/risks). Un resumen de los riesgos de la IA.",
	"learn_xrisk": "[Riesgo existencial](/xrisk). Por qué la IA es un riesgo existencial.",
	"learn_ai_takeover": "[Toma de control](/ai-takeover). Cómo la IA podría tomar el control del mundo.",
	"learn_quotes": "[Citas](/quotes). Citas sobre los riesgos y la gobernanza de la IA.",
	"learn_feasibility": "[Viabilidad de una pausa](/feasibility). La viabilidad de una pausa en el desarrollo de la IA.",
	"learn_building_the_pause_button": "[Construyendo el botón de pausa](/building-the-pause-button). Lo que se necesita para pausar la IA.",
	"learn_faq": "[Preguntas frecuentes](/faq). Preguntas comunes sobre la seguridad de la IA y PauseAI.",
	"learn_action": "[Acción](/action). Lo que puedes hacer para ayudar (con enlaces a muchas guías relacionadas con la acción)"
}
