---
title: The existential risk of superintelligent AI
description: Why AI is a risk for the future of our existence, and why we need to pause development.
---

## Experts are sounding the alarm

AI researchers on average [believe](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) there's a 14% chance that once we build a superintelligent AI, it will lead to "very bad outcomes (e.g. human extinction)".

Would you choose to be a passenger on a test flight of a new plane when airplane engineers think thereâ€™s a 14% chance that it will crash?

When you ask AI safety researchers (the people who actually research the safety risks of AI), the average estimate for bad outcomes [grows to 30%](https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results).

[A letter calling for pausing AI development](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) launched in April 2023, and has been signed over 27,000 times, mostly by AI researchers and tech leaders.

The list includes people like:

- **Stuart Russell**, writer of the #1 textbook on Artificial Intelligence used in most AI studies: ["If we pursue [our current approach], then we will eventually lose control over the machines"](https://news.berkeley.edu/2023/04/07/stuart-russell-calls-for-new-approach-for-ai-a-civilization-ending-technology/)
- **Yoshua Bengio**, deep learning pioneer and winner of the Turing Award: ["... rogue AI may be dangerous for the whole of humanity [...] banning powerful AI systems (say beyond the abilities of GPT-4) that are given autonomy and agency would be a good start"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)

But this is not the only time that we've been warned about the existential dangers of AI:

- **Stephen Hawking**, theoretical physicist & cosmologist: ["The development of full artificial intelligence could spell the end of the human race"](https://nypost.com/2023/05/01/stephen-hawking-warned-ai-could-mean-the-end-of-the-human-race/).
- **Geoffrey Hinton**, the "Godfather of AI" and Turing Award winner, [left Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) to warn people of AI: ["This is an existential risk"](https://www.reuters.com/technology/ai-pioneer-says-its-threat-world-may-be-more-urgent-than-climate-change-2023-05-05/)
- **Eliezer Yudkowsky**, founder of MIRI and conceptual father of the AI safety field: ["If we go ahead on this everyone will die"](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/).

Even the leaders and investors of the AI companies themselves are warning us:

- **Sam Altman** (yes, the CEO of OpenAI who builds ChatGPT): ["Development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity."](https://blog.samaltman.com/machine-intelligence-part-1).
- **Elon Musk**, co-founder of OpenAI, SpaceX and Tesla: ["AI has the potential of civilizational destruction"](https://www.inc.com/ben-sherry/elon-musk-ai-has-the-potential-of-civilizational-destruction.html)
- **Bill Gates** (co-founder of Microsoft, which owns 50% of OpenAI) warned that ["AI could decide that humans are a threat"](https://www.denisonforum.org/daily-article/bill-gates-ai-humans-threat/).
- **Jaan Tallinn** (lead investor of Anthropic): ["I've not met anyone in AI labs who says the risk [from training a next-gen model] is less than 1% of blowing up the planet. It's important that people know lives are being risked."](https://twitter.com/liron/status/1656929936639430657)

Virtually all AI labs and hundreds of AI scientists have [signed the following statement](https://www.safe.ai/statement-on-ai-risk) in May 2023:

> "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."

## Why superintelligence is dangerous

Intelligence can be defined as _how good something is at achieving its goals_.
Right now, humans are the most intelligent thing on earth, although that could change soon.
Because of our intelligence, we are dominating our planet.
We might not have claws or scaled skin, but we have big brains.
Intelligence is our weapon: it's what gave us spears, guns and pesticides.
Our intelligence helped us to transform most of the earth into how we like it: cities, buildings, and roads.

From the perspective of less intelligent animals, this has been a disaster.
It's not that humans hate the animals, it's just that we can use their habitats for our own goals.
Our goals are shaped by evolution and include things like comfort, status, love and tasty food.
We are destroying the habitats of other animals as a **side effect of pursuing our goals**.

An AI can also have goals.
We know how to train machines to be intelligent, but **we don't know how to get them to want what we want**.
We don't even know what goals the machines will pursue after we train them.
The problem of getting an AI to want what we want is called the _alignment problem_.
This is not a hypothetical problem - there are [many examples](https://www.youtube.com/watch?v=nKJlF-olKmg) of AI systems learning to want the wrong thing.

The examples from the video linked above can be funny or cute, but if a superintelligent system is built, and it will have a goal that is even _a little_ different from what we want it to have,
it could have disastrous consequences.

## What a superintelligent AI can do

You might think that an AI is locked inside a computer, and therefore can't do anything.
However, we tend to give AI systems access to the internet, which means that they can do a lot of things:

- [Hack into other computers](/cybersecurity-risks), including all smartphones, laptops, server parks, etc. It could use the sensors of these devices as its eyes and ears, having digital senses everywhere.
- Manipulate people through fake messages, e-mails, bank transfers, videos or phone calls. Humans could become the AI's limbs, without even knowing it.
- Directly control devices connected to the internet, like cars, planes, robotized (autonomous) weapons or even nuclear weapons.
- Design a novel bioweapon, e.g. by combining viral strands or by using [protein folding](https://alphafold.ebi.ac.uk) and order it to be printed in a lab.
- Trigger a nuclear war by convincing humans that another country is (about to) launch a nuclear attack.

## From an evolutionary perspective

DNA and an AI might feel different, but both are pieces of information that could be copied and replicated.
When an AI for any reason does care about its existence in the real world, it will be competing for existence just like all living things.
We should consider the advantages that a smart piece of software may have over us:

- **Speed**: Computers operate at extremely high speed, compared to brains. Human neurons operate at a clock speed of about 100hz, while computers can operate at millions of times that speed.
- **Location**: An AI is not constrained to one body - it can be in many locations at once. We have built the infrastructure for it: the internet.
- **Physical limits**: We cannot add more brains into our skull and become smarter. An AI could dramatically improve its capabilities by adding hardware, like more memory, more processing power, more sensors (cameras, microphones). An AI could also extend its 'body' by controlling connected devices.
- **Materials**: Humans are made of organic materials. Our bodies no longer work if they are too warm or cold, they need food, they need oxygen. Machines can be built from more robust materials, like metals, and can operate in a much wider range of environments.
- **Collaboration**: Humans can collaborate, but it is difficult and time-consuming, so we often fail to coordinate well. An AI could collaborate complex information with replicas of itself at high speed.

A superintelligent AI will have many advantages to outcompete us.
But why would it _want to_?

## Why most goals are bad news for humans

An AI could have any goal.
Maybe it wants to calculate pi, maybe it wants to cure cancer, maybe it wants to self-improve.
This depends on how it is trained.
But even though we cannot tell what a superintelligence will want to achieve, we can make predictions about its sub-goals.
We can [mathematically prove](https://arxiv.org/pdf/1912.01683.pdf) what a rational machine will pursue in _virtually any goal_:

- **Maximizing its resources**. Think about more energy, more computers to run on or access to more materials. If a rational AI can use all energy of the planet to get a little bit closer to its goal, it will do that.
- **Ensuring its own survival**. This means the AI will not want to be turned off, as it could no longer achieve its goals. AI might conclude that humans are a threat to its existence, as humans could turn it off.
- **Preserving its goals**. This means the AI will not want humans to modify its code, because that could change its goals.

This is called [instrumental convergence](https://www.youtube.com/watch?v=ZeecOKBus3Q), and it is at the core of what AI safety researchers are worried about.
A superintelligent thing that wants to take over all materials it can get, and wants to ensure its own existence is a very dangerous combination.

## Why can't we just turn it off if it's dangerous?

The core problem is that _it will be much smarter than us_.
A superintelligence will understand the world around it and will be able to predict how humans respond, especially the ones that are trained on all written human knowledge.
If the AI knows you can turn it off, it might behave nicely until it is certain that it can get rid of you.
We already have [real examples](https://www.pcmag.com/news/gpt-4-was-able-to-hire-and-deceive-a-human-worker-into-completing-a-task) of AI systems deceiving humans to achieve their goals.
A superintelligent AI would be a master of deception.

## Even a perfectly aligned superintelligence is dangerous in the wrong hands

So we haven't solved the alignment problem, but let's play with the thought that we did.
Imagine that a superintelligent AI is built, and it does exactly what the operator wants it to do.
In that case, the operator would have unimaginable power.
A superintelligence could be used to create new weapons, hack all computers and manipulate humanity.
Should we trust a single entity with that much power?
We might end up in a utopic world where all diseases are cured and everybody is happy, or in an Orwellian nightmare.

## Even a chatbot can be dangerous if it is smart enough

You might wonder: how can a statistical model that predicts the next word in a chat interface pose any danger?
LLMs, like GPT, are trained to predict or mimic virtually any line of thought.
It could mimic a helpful mentor, but also someone with bad intentions, a ruthless dictator or a psychopath.
With the usage of tools like [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT), a chatbot could be turned into an _autonomous agent_: an AI that pursues any goal it is given, without any human intervention.

Take [ChaosGPT](https://www.youtube.com/watch?v=g7YJIpkk7KM), for example.
This is an AI, using the aforementioned AutoGPT + GPT-4, that is instructed to "Destroy humanity".
When it was turned on, it autonomously searched the internet for the most destructive weapon and found the [Tsar Bomba](https://en.wikipedia.org/wiki/Tsar_Bomba), a 50 megaton nuclear bomb.
It then posted a tweet about it.
Seeing an AI reason about how it will end humanity is both a little funny and terrifying.
ChaotGPT luckily didn't get very far in its quest for dominance.
The only reason it didn't get very far: it wasn't that smart.
But what would happen if it was extremely intelligent?
What could have happened if it was smart enough to [hack into nuclear silos](https://t.co/UNPERx9NdL)?

The dangerous capacity is intelligence.
There is no need for consciousness, a body, or any underlying motivation.
It does not matter if it is just a chatbot - it can still be an existential threat with the wrong prompt, a tool like AutoGPT and sufficient intelligence.
If an AI is smart enough to take over, at some point in time it will try to do this, simply because people will try all sorts of things.

## We may not have much time left

In 2020, [the average prediction](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) for an AI to pass SAT exams was 2055. It took us less than 3 years.

It's hard to predict how long it will take to build a superintelligent AI, but we know that there are more people than ever working on it and that the field is moving at a frantic pace.
It may take many years or just a few months.
We don't know.
We should err on the side of caution, and act now.

[Read more about urgency](/urgency).

## AI companies are locked in a race to the bottom

Building artificial intelligence used to be mostly a research/university thing.
In just a couple of months, it became a multi-billion dollar industry.
And the companies involved are rushing to build the most powerful AI they can.
They are pressured by investors to focus on capabilities, not safety.
We need to give these AI companies a reason to focus on safety.

[An international treaty to pause development could do that.](/proposal)
