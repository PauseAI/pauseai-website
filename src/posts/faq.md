---
title: FAQ
description: Frequently asked questions about PauseAI and the risks of superintelligent AI.
---

### Who are you?

We are a community of volunteers who work together to mitigate the [risks of AI](/risks) (including the [risk of human extinction](/xrisk)).
We aim to convince our governments to step in and [pause AI development](/proposal).
We do this by informing the public, talking to decision-makers, and organizing protests.
We are not affiliated with any company or organization.

You can find us on [Discord](https://discord.gg/2XXWXvErfA), [Twitter](https://twitter.com/PauseAI), [facebook](https://www.facebook.com/PauseAI), [TikTok](https://www.tiktok.com/@pauseai) and [LinkedIn](https://www.linkedin.com/uas/login?session_redirect=/company/97035448/).

### How likely is it that superintelligent AI will cause very bad outcomes, like human extinction?

AI safety researchers (who are the experts on this topic) are divided on this question, and estimates [range from 2% to 97% with an average of 30%](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results).
Note that no (surveyed) AI safety researchers believe that there's a 0% chance.
However, there might be selection bias here: people who work in the AI safety field are likely to do so because they believe preventing bad AI outcomes is important.

If you ask AI researchers in general (not safety specialists), this number drops to a [mean value of around 14%](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/).
A small minority, about 4% of them, believe that the alignment problem is not a real problem.
Note that there might be a selection bias here in the opposite direction: people who work in AI are likely to do so because they believe AI will be beneficial.

_Imagine you're invited to take a test flight on a new airplane_.
The airplane safety experts on average think there's a 30% chance it would crash.
The plane engineers think there's a 14% chance of crashing.
About 4% of engineers are saying that there's no chance of crashing, it is uncrashable.

Would you enter that plane? Because right now, we're all boarding the AI plane.

### How long do we have until superintelligent AI?

It might take months, it might take decades, nobody knows for sure.
However, we do know that the pace of AI progress is often grossly underestimated.
Just three years ago we thought we'd have SAT-passing AI systems in 2055.
We got there in April 2023.
We should act as if we have very little time left because we don't want to be caught off guard.

[Read more about urgency](/urgency).

### OpenAI and Google are saying they want to be regulated. Why are you protesting them?

We applaud [OpenAI](https://openai.com/blog/governance-of-superintelligence) and [Google](https://www.ft.com/content/8be1a975-e5e0-417d-af51-78af17ef4b79) for their calls for international regulation of AI.
However, we believe that the current proposals are not enough to prevent an AI catastrophe.
Google and Microsoft have not yet publicly stated anything about the existential risk of AI.
Only OpenAI [explicitly mentions the risk of extinction](https://openai.com/blog/governance-of-superintelligence), and again we applaud them for taking this risk seriously.
However, their strategy is quite explicit: a Pause is impossible, we need to get to superintelligence first.
The problem with this, however, is that they [do not believe they have solved the alignment problem](https://youtu.be/L_Guz73e6fw?t=1478).
The AI companies are locked in a race to the bottom, where AI safety is sacrificed for competitive advantage.
This is simply the result of market dynamics.
We need governments to step in and implement policies (at an international level) that [prevent the worst outcomes](/proposal).

### Are AI companies pushing the existential risk narrative to manipulate us?

We can't know for certain what motivations these companies have, but we do know that **x-risk was not initially pushed by AI companies - it was scientists, activists and NGOs**.
Let's look at the timeline.

There have been many people who have warned about x-risk since the early 2000s.
Eliezer Yudkowski, Nick Bostrom, Stuart Russell, Max Tegmark, and many others.
They had no AI tech to push - they were simply concerned about the future of humanity.

The AI companies never mentioned x-risk until very recently.

Sam Altman is an interesting exception.
He wrote about existential AI risk [back in 2015, on his private blog](https://blog.samaltman.com/machine-intelligence-part-1), before founding OpenAI.
In the years since he made virtually no explicit mention of x-risk again.
During the Senate hearing, when asked about his x-risk blog post, he only answered by talking about jobs and the economy.
He was not pushing the x-risk narrative here, he was actively avoiding it.

In May 2023, everything changed:

- On May 1st, 'Godfather of AI' Geoffrey Hinton [quits his job at Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) to warn about x-risk.
- On May 5th, the [first PauseAI protest is announced](https://twitter.com/Radlib4/status/1654262421794717696), right at OpenAI's doorstep.
- On May 22nd, OpenAI published [a blog post about the governance of superintelligence](https://openai.com/blog/governance-of-superintelligence), and mentioned x-risk for the first time.
- On May 24th, ex-Google CEO Eric Schmit acknowledges x-risk.
- On May 30th, the [Safe.ai statement (acknowledging x-risk)](https://www.safe.ai/statement-on-ai-risk) was published. This time, including people from OpenAI, Google and Microsoft.

Signing the statement.
That's all the AI companies (or some of their directors / employees) have done to push the x-risk narrative.

These companies have been very slow to acknowledge x-risk, considering that many of their employees have been aware of it for years.
So in our view, the AI companies are not pushing the x-risk narrative, they have been reactive to others pushing it, and have waited with their response until it was absolutely necessary.

The business incentives point in the other direction: companies would rather not have people worry about the risks of their products.
Virtually all companies downplay risks to attract customers and investments (remember the Oceangate submarine?), not exaggerate them.
How much strict regulation and negative attention are the companies inviting due to admitting these dangers?
And would a company like OpenAI [dedicate 20% of its compute resources](https://openai.com/blog/introducing-superalignment) to AI safety if it wouldn't believe in the risks?

Here's our interpretation: the AI companies signed the statement because _they know that x-risk is a problem that needs to be taken very seriously_.

The main reason many other people still don't want to believe that x-risk is a real concern?
Because acknowledging that _we are in fact in danger_ is a very, very scary thing.

[Read more about the psychology of x-risk](/psychology-of-x-risk).

### Aren't you just scared of changes and new technology?

You might be surprised that most people in PauseAI consider themselves techno-optimists.
Many of them are involved in AI development, are gadget lovers, and have mostly been very excited about the future.
Particularly many of them have been excited about the potential of AI to help humanity.
That's why for many of them the sad realization that AI might be an existential risk was a very [difficult one to internalize](/psychology-of-x-risk).

### I have a different / AI related question

Try [AIsafety.info](https://aisafety.info/), an awesome database of questions and answers about AI safety.
