---
title: F.A.Q.
description: Questions fr√©quemment pos√©es sur Pause IA et les risques de l'IA superintelligente.
---

<!-- ‚Üì‚Üì‚Üì NE PAS TOUCHER ‚Üì‚Üì‚Üì -->
<script lang="ts">
  import Accordion from '$lib/components/Accordion.svelte'
	import { page } from '$app/stores'

  $: toc = $page.url.pathname === '/faq'

</script>

{#if toc}

<!-- ‚Üë‚Üë‚Üë NE PAS TOUCHER ‚Üë‚Üë‚Üë -->

## Sommaire

- [Qui √™tes-vous ?](#accordion1)
- [N‚Äô√™tes-vous pas tout simplement des technophobes ?](#accordion2)
- [Voulez-vous interdire toute forme d'IA ?](#accordion3)
- [Croyez-vous que GPT-4 va tous nous tuer ?](#accordion4)
- [Un moratoire ne risque-t-il pas d'aggraver les choses ?](#accordion5)
- [Un moratoire est-il possible ?](#accordion6)
- [Qui vous finance ?](#accordion7)
- [Quels sont vos projets ?](#accordion8)
- [Comment comptez-vous convaincre les gouvernements d'arr√™ter temporairement le d√©veloppement de l'IA ?](#accordion9)
- [Pourquoi manifester ?](#accordion10)
- [Quelle est la probabilit√© que l'apparition d'une superintelligence ait de graves cons√©quences, y compris un risque d'extinction ?](#accordion11)
- [Combien de temps nous reste-t-il avant l'√©mergence d'une superintelligence ?](#accordion12)
- [Si nous appliquons un moratoire, qu'en est-il de la Chine ?](#accordion13)
- [OpenAI et Google semblent appeler de leurs voeux une r√©glementation. Pourquoi s'opposer √† eux ?](#accordion14)
- [Les entreprises d'IA pr√©tendent-elles que le risque existentiel est r√©el pour nous manipuler ?](#accordion15)
- [Je veux vous aider ! Que puis-je faire ?](#accordion16)

<!-- ‚Üì‚Üì‚Üì NE PAS TOUCHER ‚Üì‚Üì‚Üì -->

{/if}

<!-- ‚Üë‚Üë‚Üë NE PAS TOUCHER ‚Üë‚Üë‚Üë -->

### Qui √™tes-vous ?

Nous sommes un ensemble de [b√©n√©voles](https://pauseai.info/people) et de [groupes locaux](https://pauseai.info/communities), organis√© par une [association √† but non lucratif](/mentions-legales) dont l'objectif est de minimiser les [risques li√©s √† l'IA](/risques) (y compris le [risque d'extinction](/risques/humanite)). Notre objectif est de convaincre nos gouvernements d'intervenir et [de mettre en pause le d√©veloppement d'une IAG](/propositions) (Intelligence Artificielle G√©n√©rale puis d‚Äôune Superintelligence). Dans ce but, nous alertons le public, dialoguons avec les d√©cideurs et organisons des manifestations.

Vous pouvez nous rejoindre sur [Discord](https://discord.gg/vyXGd7AeGc) (C‚Äôest l√† que notre communaut√© est la plus active!), [Twitter](https://twitter.com/pause_ia), [Facebook](https://www.facebook.com/Pause.IA), [TikTok](https://www.tiktok.com/@pause_ia), [LinkedIn](https://www.linkedin.com/company/pause-ia/), [YouTube](https://www.youtube.com/@Pause_IA), [Instagram](https://www.instagram.com/pause_ia) et [Threads](https://www.threads.net/@pause_ia). Vous pouvez √©galement nous contacter par mail √† [contact@pauseia.fr](mailto:contact@pauseia.fr)

### N‚Äô√™tes-vous pas tout simplement des technophobes ?

Vous seriez surpris d'apprendre que la plupart des membres de Pause IA sont favorablement dispos√©s envers le progr√®s technologique. Nombre d'entre eux sont impliqu√©s dans le d√©veloppement de l'IA, sont des amateurs de nouvelles technologies et ont longtemps √©t√© tr√®s enthousiastes face √† l'avenir. Beaucoup s‚Äôint√©ressaient particuli√®rement au potentiel de d√©veloppement de l‚Äôhumanit√© que rec√®le l‚ÄôIA. C'est pourquoi, quand ils se sont rendu compte des risques existentiels li√©s √† l‚ÄôIA, nombre d‚Äôentre eux ont eu beaucoup de mal √† [l‚Äôaccepter / l‚Äôint√©grer](https://pauseai.info/psychology-of-x-risk).

### Voulez-vous interdire toute forme d'IA ?

Non. Seulement le d√©veloppement des plus gros syst√®mes d'IA √† usage g√©n√©ral souvent appel√©s "mod√®les de pointe". La quasi-totalit√© des mod√®les existants, ainsi que la plupart des futurs mod√®les d'IA, resteraient [l√©gaux selon notre proposition](/propositions). Nous demandons l'interdiction des syst√®mes plus puissants que GPT-4-o, jusqu'√† ce que nous puissions exercer un contr√¥le d√©mocratique sur ces mod√®les et que nous soyons en mesure de les cr√©er en toute s√©curit√©.

### Croyez-vous que GPT-4 va tous nous tuer ?

Non, nous ne croyons pas que les [mod√®les actuels](https://pauseai.info/sota) repr√©sentent un risque existentiel. Probablement que la plupart des prochains mod√®les non plus. Mais si nous poursuivons le d√©veloppement de syst√®mes toujours plus puissants, nous atteindrons un point de non-retour o√π l'un d'eux deviendra [une menace existentielle](/risques/humanite).

### Un moratoire ne risque-t-il pas d'aggraver les choses ?

Nous avons r√©pondu √† ces pr√©occupations [dans cet article](https://pauseai.info/mitigating-pause-failures).

### Un moratoire est-il possible ?

L‚Äô√©mergence d‚Äôune superintelligence n'est pas in√©vitable. Sa cr√©ation n√©cessite des arm√©es d'ing√©nieurs pay√©s √† coup de millions de dollars et une cha√Æne d'approvisionnement de mat√©riel de pointe non r√©glement√©. Sa cr√©ation implique aussi que nous permettions √† ces entreprises de jouer avec notre avenir en restant passifs.

[En savoir plus sur la faisabilit√© d‚Äôun moratoire .](https://pauseai.info/feasibility)

### Qui vous finance ?

Quasiment toutes nos actions jusqu'ici ont √©t√© men√©es par des b√©n√©voles. Cependant, depuis f√©vrier 2024, Pause IA est une [organisation √† but non lucratif enregistr√©e](/mentions-legales), et nous avons re√ßu de multiples dons de particuliers. Nous avons √©galement re√ßu 20 000 dollars de financement de la part du r√©seau LightSpeed.

Vous pouvez √©galement [faire un don √† Pause IA](/dons) si vous soutenez notre cause ! Nous utilisons l'essentiel de l'argent pour permettre √† des communaut√©s locales d'organiser des √©v√©nements.

### Quels sont vos projets ?

Nous nous concentrons sur la [croissance du mouvement](https://pauseai.info/growth-strategy), l'organisation de manifestations, le lobbying aupr√®s des politiciens et l'information du public.

Consultez [notre feuille de route](https://pauseai.info/roadmap) pour un aper√ßu d√©taill√© de nos projets et de ce que nous pourrions faire avec plus de financements.

### Comment comptez-vous convaincre les gouvernements d‚Äôarr√™ter temporairement le d√©veloppement de l'IA ?

Jetez un ≈ìil √† notre "[Th√©orie du changement](https://pauseai.info/theory-of-change)" pour un aper√ßu d√©taill√© de notre strat√©gie.

### Pourquoi manifester ?

- Manifester d√©montre √† tous que cette question nous tient √† c≈ìur. En manifestant, nous prouvons que nous sommes pr√™ts √† consacrer du temps et de l'√©nergie √† la diffusion de notre message.
- Il n‚Äôest pas rare que les manifestations aient une [influence positive](https://www.socialchangelab.org/_files/ugd/503ba4_052959e2ee8d4924934b7efe3916981e.pdf) sur l‚Äôopinion publique, le vote, l‚Äôattitude des entreprises et la loi.
- [La grande majorit√© des gens soutiennent](https://today.yougov.com/politics/articles/31718-do-protesters-want-help-or-hurt-america) les manifestations pacifiques et non violentes.
- Il n'y a [aucun "retour de b√¢ton"](https://journals.sagepub.com/doi/full/10.1177/2378023120925949) sauf si la manifestation [d√©g√©n√®re en violences](https://news.stanford.edu/stories/2018/10/how-violent-protest-can-backfire). Nos manifestations sont pacifiques et non violentes.
- C'est une exp√©rience de lien social. Vous rencontrez d'autres personnes qui partagent vos pr√©occupations et votre volont√© d'agir.
- Lisez [cet excellent article](https://forum.effectivealtruism.org/posts/4ez3nvEmozwPwARr9/a-case-for-the-effectiveness-of-protest) pour en savoir plus sur l'efficacit√© des manifestations.

Si vous voulez [organiser une manifestation](https://pauseai.info/organizing-a-protest), nous pouvons vous apporter conseils et ressources.

### Quelle est la probabilit√© que l'apparition d'une superintelligence ait de graves cons√©quences, y compris un risque d'extinction ?

Nous avons compil√© [une liste de valeurs "p(doom)"](https://pauseai.info/pdoom) (probabilit√© de sc√©narios catastrophiques) provenant de divers experts renomm√©s dans le domaine.

Les chercheurs en s√©curit√© de l'IA (qui sont experts du sujet) sont partag√©s, [leurs estimations allant de 2% √† 97% avec une moyenne de 30%](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results). Notez qu'aucun des chercheurs en s√©curit√© interrog√©s ne croit en une probabilit√© de 0%. Un biais de s√©lection est cependant possible: ceux qui travaillent dans le domaine de la s√©curit√© de l'IA le font probablement car ils redoutent les cons√©quences n√©fastes de l'IA.

Si l'on interroge les chercheurs en IA en g√©n√©ral (qui ne sont pas sp√©cialistes en s√©curit√©), ce chiffre tombe √† [une moyenne d'environ 14%](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/), avec une m√©diane de 5%. Une minorit√©, environ 20%, pense que le probl√®me d'alignement n'est ni r√©el ni important. L√† encore, un biais de s√©lection inverse est possible : ceux qui travaillent en IA le font sans doute car ils consid√®rent que les cons√©quences de l'IA sur le monde seront uniquement b√©n√©fiques.

_Imaginez qu'on vous propose d'essayer un nouvel avion._ Les ing√©nieurs estiment les risques de crash √† 14%. Monteriez-vous √† bord ? C'est plus ou moins la situation actuelle, nous embarquons tous √† bord du m√™me avion.

### Combien de temps nous reste-t-il avant l'√©mergence d'une superintelligence ?

Cela pourrait prendre des mois, ou bien des d√©cennies, personne n'en est certain. Ce que nous savons, c'est que les progr√®s dans le domaine de l'IA sont souvent largement sous-estim√©s. Il y a seulement trois ans, nous pensions qu'il faudrait attendre 2055 pour voir des mod√®les capables de r√©ussir un test SAT (√©quivalent PISA aux Etats-Unis). Nous y sommes parvenus d√®s avril 2023. Il semble souhaitable d‚Äô agir comme si le temps nous √©tait compt√© afin de ne pas √™tre pris au d√©pourvu.

[En savoir plus sur l'urgence de la situation.](https://pauseai.info/urgency)

### Si nous appliquons un moratoire, qu'en est-il de la Chine ?

La Chine a actuellement les r√©glementations les plus strictes au monde en mati√®re d'IA. Les [chatbots sont interdits](https://www.reuters.com/technology/chinas-slow-ai-roll-out-points-its-tech-sectors-new-regulatory-reality-2023-07-12/) et [l'entra√Ænement sur des donn√©es internet n'√©tait pas autoris√©](https://cointelegraph.com/news/china-sets-stricter-rules-training-generative-ai-models) jusqu'en [septembre 2023](https://asia.nikkei.com/Business/Technology/China-approves-AI-chatbot-releases-but-will-it-unleash-innovation). Le gouvernement chinois, avec son mode de r√©gime autoritaire, a bien plus de raisons de craindre les impacts incontr√¥lables et impr√©visibles de l'IA que nous. Lors de la r√©union du Conseil de s√©curit√© des Nations unies sur la s√©curit√© de l'IA, la Chine a √©t√© le seul pays √† mentionner la possibilit√© d‚Äôinstaurer un moratoire..

De plus, nous appelons √† un moratoire international , impos√© par un trait√©. Un tel trait√© doit √©galement √™tre sign√© par la Chine. Si le trait√© garantit que d'autres nations s'arr√™teront aussi, et qu'il y a des m√©canismes de contr√¥le et des mesures de mise en vigueur suffisantes, la Chine y sera probablement favorable.

### OpenAI et Google semblent appeler de leurs v≈ìux une r√©glementation. Pourquoi s‚Äôopposer √† eux ?

Nous saluons les appels [d'OpenAI](https://openai.com/index/governance-of-superintelligence/) et de [Google](https://www.ft.com/content/8be1a975-e5e0-417d-af51-78af17ef4b79) pour demander une r√©glementation internationale vis-√†-vis de l'IA. Cependant, nous pensons que les propositions actuelles ne suffiront pas √† √©viter une catastrophe. Google et Microsoft n'ont pas encore reconnu publiquement les risques existentiels li√©s √† l'IA. Seul OpenAI [mentionne explicitement le risque d'extinction](https://openai.com/index/governance-of-superintelligence/). Cependant, leur strat√©gie est tr√®s claire: un moratoire est impossible, nous devons d'abord cr√©er une superintelligence avant de penser √† de s√©rieuses r√©gulations. Mais il avouent eux-m√™me [ne pas avoir r√©solu le probl√®me d'alignemen](https://www.youtube.com/watch?t=1478&v=L_Guz73e6fw&feature=youtu.be)t et les derniers d√©veloppements prouvent [qu'ils ne traitent pas ce probl√®me avec le s√©rieux qu'il m√©rite](https://www.cnbc.com/2024/05/17/openai-superalignment-sutskever-leike.html). Ces entreprises sont engag√©es dans une course contre la montre au d√©triment de la s√©curit√©, sacrifi√©e pour un avantage concurrentiel. C'est le r√©sultat de la dynamique du march√©. Nous devons encourager les gouvernements √† intervenir et √† mettre en place des politiques internationales pour [√©viter les pires sc√©narios](/propositions).

### Les entreprises d'IA pr√©tendent-elles que le risque existentiel est r√©el pour nous manipuler ?

Nous ne pouvons pas √™tre certains des motivations de ces entreprises, et nous savons qu'elles **ne sont pas √† l'origine de la mise en avant des risques existentiels li√©s √† l'IA**. Les signaux d'alerte venaient des scientifiques, militants et ONG. Jetons un ≈ìil √† la chronologie.

Depuis le d√©but des ann√©es 2000, diverses personnes ont alert√© sur ce risque existentiel, comme Eliezer Yudkowsky, Nick Bostrom, Stuart Russell, Max Tegmark, et bien d'autres. Ils n'avaient aucun produit √† vendre, ils √©taient simplement pr√©occup√©s par l'avenir de l'humanit√©.

Les entreprises d'IA n'ont commenc√© √† mentionner les risques existentiels que tr√®s r√©cemment.

Sam Altman est une exception. Sur son blog personnel, il a explor√© [l'id√©e du risque existentiel d√®s 2015](https://blog.samaltman.com/machine-intelligence-part-1), avant m√™me de fonder OpenAI. Pendant les ann√©es qui ont suivi, il n'a quasiment plus jamais √©voqu√© explicitement ce risque. Lors de son audition devant le S√©nat Am√©ricain le 16 mai 2023, interrog√© sur cet article de blog, il a soigneusement √©vit√© la question en pr√©f√©rant parler des emplois et de l'√©conomie.

En mai 2023, tout a chang√© :

- Le 1er mai, le pionnier de l'IA Geoffrey Hinton [d√©missionne de Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) pour alerter le public sur la possibilit√© de risques existentiels.
- Le 20 mai, [la premi√®re manifestation de Pause IA](https://pauseai.info/openai-protest) a lieu devant le si√®ge d'OpenAI.
- Le 22 mai, OpenAI publie [un article de blog sur la gouvernance de la superintelligence](https://openai.com/index/governance-of-superintelligence/), et mentionne le risque existentiel pour la premi√®re fois.
- Le 24 mai, l'ancien PDG de Google Eric Schmidt reconna√Æt la possibilit√© de risques existentiels.
- Le 30 mai, le centre pour la s√©curit√© de l'IA publie [une d√©claration sur les risques existentiels](https://www.safe.ai/work/statement-on-ai-risk), incluant des employ√©s d'OpenAI, Google et Microsoft.

Ces entreprises ont √©t√© lentes √† reconna√Ætre la possibilit√© de risques existentiels, alors que beaucoup de leurs employ√©s en √©taient conscients depuis des ann√©es. Selon nous, les entreprises d'IA ont simplement r√©agi √† l'√©mergence des risques existentiels dans le discours public et ont n'ont apport√© leur r√©ponse que lorsque le sujet est devenu in√©vitable.

Mais les incitations commerciales vont √† contre-courant : il n'est pas dans l'int√©r√™t de ces entreprises que le public s'inqui√®te des dangers de leurs produits. Presque toutes minimisent les risques pour attirer clients et investisseurs. Combien de r√©gulations et de n√©gativit√© risquent-elles d'attirer en admettant ces dangers ? Et une entreprise comme OpenAI consacrerait-elle [20% de sa puissance de calcul informatique](https://openai.com/index/introducing-superalignment/) √† la s√©curit√© de l'IA si elle ne croyait pas en ces dangers ?

Notre interpr√©tation est que les entreprises d'IA ont sign√© cette d√©claration parce qu'_elles savent que les risques existentiels sont un probl√®me √† prendre tr√®s au s√©rieux_.

Une raison majeure pour laquelle de nombreuses personnes ne veulent toujours pas croire que les risques existentiels sont une pr√©occupation r√©elle ? Parce que la reconnaissance d'un tel danger est une √©norme charge mentale.

[En savoir plus sur la charge mentale des risques existentiels.](https://pauseai.info/psychology-of-x-risk)

### Je veux aider ! Que puis-je faire ?

Il y a de nombreuses choses que [vous pouvez faire](/agir). √Ä titre individuel, [r√©digez une lettre](https://pauseai.info/writing-a-letter), [distribuez des tracts](https://pauseai.info/flyering), sensibilisez votre entourage, prenez part √† [une manifestation](https://pauseai.info/protests) ou [faites un don](/dons) ! Mais plus important : vous pouvez [rejoindre Pause IA](/nous-rejoindre) et coordonner vos actions avec d'autres personnes membres. Si vous souhaitez vous impliquer davantage, vous pouvez devenir b√©n√©vole et [int√©grer une de nos √©quipes.](https://pauseai.info/teams)

M√™me confront√©s √† la perspective de risques existentiels, il reste de l'espoir et du travail √† accomplir. üí™
