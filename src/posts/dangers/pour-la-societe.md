---
title: Dangers pour la société
description: Les dangers sociétaux de l'IA, incluant des menaces pour la démocratie, la cohésion sociale et l'emploi, nécessitent une régulation proactive, une refonte des systèmes d'information et une éducation à la pensée critique pour préserver nos valeurs et structures sociales.
---

Certains dangers touchent la société entière, et doivent être considérés dans cette perspective afin de ne pas en négliger des aspects majeurs. Ces dangers sont vastes et touchent directement à la cohésion sociale et aux fondements de notre démocratie. Ils portent atteinte à l’intégrité des processus démocratiques par l’affaiblissement, la polarisation et la désinformation, mais aussi aux processus sociaux, par la perpétuation de préjugés raciaux et autres biais. À une autre échelle, l'IA peut également avoir des conséquences sur la guerre ou sur les méthodes utilisées par des groupes terroristes.

Ces dangers émergent souvent suite à un usage mal avisé des nouvelles technologies. Une régulation proactive, une refonte des systèmes d’information et une éducation à la pensée critique sont les meilleures mesures afin de préserver nos valeurs et nos structures sociales.

## Perpétuation des biais

Les modèles d’IA apprennent de leurs données d’entraînement, et on leur demande souvent d’imiter des comportements humains, y compris leurs biais raciaux ou de genre.

Alors que des entreprises d’IA fleurissent à travers le monde et que son usage se démocratise à toutes les échelles de la société, ces biais risquent de se répandre et de devenir impossibles à éliminer que ça soit sur le [marché de l'emploi](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G/) ou dans des [institutions publiques](https://www.science.org/doi/full/10.1126/science.aax2342).

Ce risque doit être mitigé par les fournisseurs d’IA qui sont responsables des erreurs de leurs produits en investissant plus d’efforts pour identifier et corriger ces biais. Il est aussi recommandé aux individus et entreprises de se tenir informés pour ne pas se retrouver complices dans la perpétuation de biais.


## Détérioration de l’épistémologie

Par le passé, il était difficile d’accéder à une information lorsqu’elle existait dans un endroit lointain. De nos jours, il est parfois difficile de vérifier une information, car il existe trop de bruit, d’erreurs et de mensonges, ce qui réduit notre exposition à des opinions contradictoires. Les systèmes de recommandation conçus pour maximiser l’engagement des utilisateurs aggravent ce problème en nous poussant vers des contenus que nous approuvons plutôt que vers des contenus informants, [ce qui favorise la création de systèmes de croyances fermés](https://www.pnas.org/doi/10.1073/pnas.2023301118#:~:text=Abstract,vary%20across%20social%20media%20platforms.). De plus, la génération de faux contenus permet plus facilement [d’accuser ses opposants idéologiques](https://www.france24.com/en/live-news/20240729-musk-faces-criticism-over-deepfake-kamala-harris-video) ou de [falsifier leurs déclarations par l’IA](https://www.lemonde.fr/pixels/article/2024/02/26/deepfake-de-joe-biden-l-identite-du-commanditaire-devoilee_6218633_4408996.html), ce qui nuit à la santé du débat public. Tous ces problèmes accumulés nous conduisent vers un futur où chercher la vérité devient une tâche difficile.

Un individu peut se garder de ce danger par des efforts d’hygiène mentale, mais le véritable danger réside dans une perte collective de la capacité à s’informer par soi-même, ce qui pose des problèmes à l’échelle de la société même si certains individus parviennent à y résister.

Ce problème peut être combattu par des campagnes d’information et d’éducation, par une régulation des réseaux sociaux et autres systèmes d’information, mais il nécessite surtout une refonte complète de notre rapport à l’information.

## Polarisation et affaiblissement

L’IA peut être utilisée à des fins politiques et idéologiques, en exploitant ses capacités à [manipuler ou persuader des humains](https://arxiv.org/pdf/2403.14380) et à sélectionner du contenu qui influencera leurs émotions.

Des acteurs étatiques peuvent notamment cibler la population de pays adverses pour la pousser à l’instabilité et au mécontentement, en générant et en boostant la visibilité de contenus polémiques, comme [la Russie l’a fait avec la France](https://downloads.ctfassets.net/kftzwdyauwt9/5IMxzTmUclSOAcWUXbkVrK/3cfab518e6b10789ab8843bcca18b633/Threat_Intel_Report.pdf). De façon similaire, il n’est peut-être pas impossible de saper systématiquement les capacités mentales d’un pays en exposant sa population à des contenus démoralisants ou abrutissants, comme [la Chine est soupçonnée de le faire à travers TikTok](https://www.odni.gov/files/ODNI/documents/assessments/ATA-2024-Unclassified-Report.pdf).

Ce danger doit être géré par des initiatives gouvernementales pour en identifier les vecteurs et les contrecarrer. Idéalement, des accords internationaux garantiraient que ce genre d’armes informationnelles ne soient pas utilisées.

## Guerre automatisée

Les drones et robots sont de plus en plus utilisés dans les armées à la pointe de la technologie. A première vue, il s’agit d’un développement positif puisqu'ils réduisent le coût humain de la guerre.

Mais il n’en est pas de même quant à l’automatisation des prises de décision. Dans ce contexte, [les dysfonctionnements des systèmes d'IA](https://mwi.westpoint.edu/artificial-intelligence-real-risks-understanding-and-mitigating-vulnerabilities-in-the-military-use-of-ai/) (erreurs d'interprétation, empoisonnement des données, attaques adverses) auront des conséquences bien plus dramatiques pour les populations civiles.

Les armées qui refusent d’automatiser leur hiérarchie courent le risque d’être moins efficaces, notamment en termes de vitesse de réaction. Mais deux systèmes stratégiques automatisés pourraient [aggraver une situation tendue](https://arxiv.org/pdf/2401.03408) à grande vitesse.

Il y a donc une dynamique perverse qui pousse les institutions militaires à exposer le reste du monde à des risques de conflits armés afin de ne pas être dépassées technologiquement. Ce danger doit être contrecarré par des [accords internationaux](https://pauseia.fr/propositions) afin d'éviter la mise en service de systèmes d’IA militaires décisionnels ainsi que par des régulations nationales pour éviter leur développement.

## Bioterrorisme

L’IA peut être utilisée pour démocratiser [la conception de composants chimiques toxiques](https://www.theverge.com/2022/3/17/22983197/ai-new-possible-chemical-weapons-generative-models-vx) et dans le futur pourrait permettre la conception de pathogènes sans besoin d’expertise humaine. Les acteurs mal intentionnés, limités principalement par un manque de moyens et d’expertise, pourraient entreprendre des [actions bioterroristes](https://arxiv.org/abs/2306.03809) à une échelle nationale voire planétaire en empoisonnant des sources d’eau ou en déployant des pandémies artificielles.

On peut se préparer à ce genre de dangers en adaptant nos systèmes de défense et nos systèmes sanitaires pour tenir compte de cette possibilité. Il peut être évité en contrôlant le développement et l’utilisation d’IA capables de faciliter des actions terroristes.

## Enracinement de l’oppression

De même que l’intelligence artificielle pourrait être utilisée pour déstabiliser un État, elle pourrait être utilisée au contraire pour asseoir son autorité en sapant l’opposition, notamment dans le cas d’un État autoritaire dénué de scrupules éthiques. Les [technologies de surveillance](https://www.reuters.com/world/china/china-uses-ai-software-improve-its-surveillance-capabilities-2022-04-08/), notamment aidées par la reconnaissance faciale et le profilage automatisé, pourraient efficacement contrôler la population tandis que les technologies d’information [influenceraient les idées auxquelles elle serait exposée](https://academic.oup.com/pnasnexus/article/3/2/pgae034/7610937), garantissant ainsi la stabilité du système sans besoin de coercition physique.

Ce danger peut être évité en éduquant la population aux valeurs démocratiques. Il est moins susceptible de se concrétiser en France que dans des pays où l’IA est déjà largement utilisée à des fins de surveillance et de contrôle de l’information.
