---
title: Take action
description: Ways to help out with pausing AGI development.
---

The group of people who are aware of AI risks is still small.

You are now one of them.

**Your actions matter more than you think.**

Here are some examples of what you can do.

## Tell your friends

- **Talk** to people in your life about this. Answer their questions, and get them to act.
- [**Flyers**](/flyering) print, share and post flyers.
  - Having some printed one-page flyers on hand, or easily available to show and share from your phone, can be a quick and useful resource for faqs, talking points, expert quotes and opinions, online resources, etc.
- **Share** about AI risk on social media. This website might be a good start.
- **Create** [articles](/learn#articles), [videos](/learn#videos) or [memes](https://twitter.com/AISafetyMemes). Collaborate with others in the [Discord server](https://discord.gg/2XXWXvErfA) (in the #projects channel).

## Tell your politicians

- [**Send an email to a politician**](/email-builder): emails are completely overpowered. Do it.
- [**Lobby**](/lobby-tips): convince your government to work towards a pause and prepare for the [summit](/summit).
- [**Protest**](/protests): join [one of the protests](/protests) or [organize one yourself](/organizing-a-protest).
- **Sign petitions**:
  - [International AI Treaty](https://aitreaty.org)
  - [Ban Superintelligence](https://chng.it/Djjfj2Gmpk)
  - [Demand responsible AI](https://www.change.org/p/artificial-intelligence-time-is-running-out-for-responsible-ai-development-91f0a02c-130a-46e1-9e55-70d6b274f4df)
  - Or one of the **national petitions**: [UK](https://petition.parliament.uk/petitions/639956), [AUS](https://www.aph.gov.au/e-petitions/petition/EN5163), [NL](https://aipetitie.nl)

## Join the movement

- **[Join PauseAI](/join)** and help us grow.
- **Join our [Discord server](https://discord.gg/T3YrWUJsJ5)**, where our community is most active. We have a **#projects** channel where people are working on campaigns, videos, images, apps and more.
- **Join one of our [teams](/teams)** and collaborate with other motivated volunteers.
- **[Meet](/communities)** other people in a PauseAI community near you. If there is no community near you, [starting one](/local-organizing).
- [**Donate**](/donate) to PauseAI or buy some merchandise in our [store](https://pauseai-shop.fourthwall.com/).
- **Follow** our social media channels and stay updated:
  <!-- TODO add icons, maybe make a grid -->
  - [Discord](https://discord.gg/2XXWXvErfA)
  - [X/Twitter](https://twitter.com/PauseAI)
  - [Facebook](https://www.facebook.com/PauseAI)
  - [LinkedIn](https://www.linkedin.com/company/pauseai/)
    - Adding PauseAI to your resume and enabling "Notify network" is an easy way to let many people know about our efforts.
  - [TikTok](https://www.tiktok.com/@pauseai)
  - [Instagram](https://www.instagram.com/pause_ai/)
  - [YouTube](https://www.youtube.com/@PauseAI)
  - [Substack](https://substack.com/@pauseai)
  - [Reddit](https://www.reddit.com/r/PauseAI/)

## Learn more

- [Learn more about risks from AI](/learn).
- Learn about [the psychology of existential risk](/psychology-of-x-risk).

## If you...

### If you are a politician or work in government

- **Prepare for the [AI safety summit](/summit)**. Form coalitions with other countries. Get informed about the [problem](/learn) and [solutions](/proposal).
- **Invite (or subpoena) AI lab leaders** to parliamentary/congressional hearings to give their predictions and timelines of AI disasters.
- **Establish a committee** to investigate the [risks of AI](/risks).

### If you know (international) law

- **Help draft policy**. [Draft examples](https://www.campaignforaisafety.org/celebrating-the-winners-law-student-moratorium-treaty-competition/). ([some](https://futureoflife.org/wp-content/uploads/2023/04/FLI_Policymaking_In_The_Pause.pdf) [frameworks](https://www.openphilanthropy.org/research/12-tentative-ideas-for-us-ai-policy/))
- **Make submissions to government requests for comment** on AI policy ([example](https://ntia.gov/issues/artificial-intelligence/request-for-comments)).

### If you work in AI

- **Don't work towards superintelligence**. If you have some cool idea on how we can make AI systems 10x faster, please don't build it / spread it / talk about it. We need to slow down AI development, not speed it up.
- **Talk to your management and colleagues** about the risks. Get them to take an institutional position on this.
- **Hold a seminar** on AI safety at your workplace. Check out the [videos](/learn#videos) for inspiration.
- **Sign** the [Statement on AI Risk](https://www.safe.ai/statement-on-ai-risk).

### If you work on AI safety

If you are just starting out in AI Alignment, unless you are extremely skilled and/or have had significant new flashes of insight on the problem, **consider switching to advocacy for the Pause**. Without the Pause in place first, there just isn't time to spin up a career in Alignment to the point of making useful contributions.

If you are already established in Alignment, consider more [public communication](https://twitter.com/TrustlessState/status/1651538022360285187), and adding your name to calls for the Pause and regulation of the AI industry.

### If you can write web content

- [Improve this website](https://github.com/joepio/pauseai).

## Tips for being effective

- **Be bold in your public communication of the danger**. Don't use hedging language or caveats by default; mention them when questioned, or in footnotes, but don't make it sound like you aren't that concerned if you are.
- **Be less exacting in your work**. [80% of the value comes from 20% of the work](https://en.wikipedia.org/wiki/Pareto_principle). Don't do the classic geek thing and spend months agonizing and iterating on your Google doc over endless rounds of feedback. Get your project out into the world and iterate as you go. Time is of the essence.

Consider this: all our other work may just be the equivalent of rearranging deckchairs on the Titanic. We need to be running to the bridge, grabbing the wheel, and steering away from the iceberg. We may not have much time, but we can try. We can do this!

<!-- _Acknowledgements: Written by Greg Colbourn, [originally posted on the EA forum](https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and). Edited by Joep Meindertsma and others. For helpful comments and suggestions that have improved the post, and for the encouragement to write, I thank Akash Wasil, Johan de Kock, Jaeson Booker, Greg Kiss, Peter S. Park, Nik Samolyov, Yanni Kyriacos, Chris Leong, Alex M, Amritanshu Prasad, Dušan D. Nešić, and the rest of the [AGI Moratorium HQ Slack](https://join.slack.com/t/agi-moratorium-hq/shared_invite/zt-1u6s1opls-~_l_Ynrr~8ay~SiA2yEqAQ) and AI Notkilleveryoneism Twitter._ -->
