---
title: Take action
description: Ways to help reduce AI risk.
---
AI won't get safer unless we act decisively to push for safety.
Choose an activity below depending on your interests or skills.

## For everyone

### Demand government action

- **Write to your politicians**: We've found emails are surprisingly effective and take relatively little effort. If you donâ€™t feel confident about what to write, [start with our email builder](/email-builder). When you get a meeting, you should check out our [lobby tips](/lobby-tips).
- **Call your politicians**: Try calling legislators' offices while having a set of talking points in view so you stay on topic.
- **Protest**: Join [one of the protests](https://pauseai.info/protests) or [organize one yourself](https://pauseai.info/organizing-a-protest).
- **Sign petitions**: [International AI Treaty](https://aitreaty.org), [Ban Superintelligence](https://chng.it/Djjfj2Gmpk), [Demand responsible AI](https://www.change.org/p/artificial-intelligence-time-is-running-out-for-responsible-ai-development-91f0a02c-130a-46e1-9e55-70d6b274f4df), or one of the **national petitions**: [UK](https://petition.parliament.uk/petitions/639956), [AUS](https://www.aph.gov.au/e-petitions/petition/EN5163), [NL](https://aipetitie.nl).

### Inform people around you

- **Making AI Risk "common knowledge" is key**. We need to get everyone to know that everyone else knows about AI risk, so it becomes "common knowledge". Including influential people, like politicians, journalists, and lobbyists. See Connor Leahy's explanation [here](https://youtu.be/OUjnVeydhCM?t=1969) and [here](https://youtu.be/1j--6JYRLVk?t=5716).
- **Share about AI risk** on your social media. One of [these videos](https://www.youtube.com/watch?v=xBqU1QxCao8&list=PLI46NoubGtIJa0JVCBR-9CayxCOmU0EJt) or this website can be a good start. And don't forget to tag us in your posts.
- **Talk to people in your life** about AI safety. Answer their questions, and encourage them to act too. Use our [counterarguments](/counterarguments) to help you be more persuasive.
- **[Tabling](/tabling) and [flyering](/flyering)** are great ways to reach many people in a short amount of time.
- **Attend local events**: Many cities have (free / low-cost) events about AI & technology policy. Attending these events is a great way to network and share your concerns. If you want AI safety marketing materials, reach out to us on [Discord](https://discord.gg/2XXWXvErfA) so we can send you some.

### Support PauseAI

- **Join or create a [local PauseAI community](/communities)**.
- **Join the [Discord](https://discord.gg/2XXWXvErfA)**, where most of the collaboration happens.
- **Protest or participate in [events](/events)**. If no protest is near you, consider [starting one](/organizing-a-protest).
- **Look over our [vacancies](/vacancies)** to see if any of your skills match our organizational needs. We're often looking for people with experience in social media, communications, organizing, outreach, and software. Some positions are compensated.
- **[Sign up as a volunteer](https://airtable.com/appWPTGqZmUcs3NWu/pag7ztLh27Omj5s2n/form)** so we can find projects in your interest areas).
- [**Donate**](/donate) to PauseAI or buy some merchandise in our [store](https://pauseai-shop.fourthwall.com/).
- **Follow our [social media channels](https://linktr.ee/pauseai)** and stay updated. Your local PauseAI chapter may also have dedicated social media pages.

## For specific people

### If you work in AI

- **Don't work towards better AI**: Do not work for AI companies or capabilities research. And do not spread ideas on how we can make AI systems faster or smarter.
- **Talk to your management and colleagues** about the risks. Get them to take an institutional position toward mitigating risk over profit. Encourage implementation of standard risk mitigation procedures and anonymous reporting.
- **Hold a seminar** on AI safety at your workplace. Check out these [slides](https://drive.google.com/drive/u/1/folders/1p9VtopzMV6Xpk4p6EGYUTna4fLE6G8hd) and [talks and videos](https://www.youtube.com/playlist?list=PLI46NoubGtIJa0JVCBR-9CayxCOmU0EJt) for inspiration.
- **Sign** the [Statement on AI Risk](https://www.safe.ai/statement-on-ai-risk).

### If you are a politician or work in government

- **Prepare for the next [AI safety summit](/summit)**. Form coalitions with other countries to share safety information and act quickly when harms arise. Work towards a global treaty.
- **Invite (or subpoena) AI lab leaders** to parliamentary/congressional hearings to give their predictions and timelines of AI disasters.
- **Establish a committee** to investigate the [risks of AI](/risks). Publish the findings, if feasible.
- **Make AI safety a priority** in your party's platform, your government's policy, or just make sure it's on the agenda.
- **Work with opposition politicians** to demonstrate that AI safety affects us all, regardless of political beliefs.

### If you have experience with (international) law

- **Help draft policy**. [Draft examples](https://www.campaignforaisafety.org/celebrating-the-winners-law-student-moratorium-treaty-competition/). ([some](https://futureoflife.org/wp-content/uploads/2023/04/FLI_Policymaking_In_The_Pause.pdf) [frameworks](https://www.openphilanthropy.org/research/12-tentative-ideas-for-us-ai-policy/))
- **Make submissions** to government requests for comment on AI policy ([example](https://ntia.gov/issues/artificial-intelligence/request-for-comments)).

### If you work as a journalist or have a social media following

- **Create content** about AI dangers or PauseAI. For more information, reach out to us through any of our [communication channels](/faq#do-you-have-social-media).
