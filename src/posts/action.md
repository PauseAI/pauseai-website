---
title: Take action
description: Ways to help out with pausing AGI development.
---

The group of people who are aware of AI risks is still small.

You are now one of them.

**So your actions matter more than you think.**

Here are some examples of what you can do.

## Inform others

- **Share** about AI risk on social media. This website might be a good start. <!-- Once we have a intro video that should be our go to. If we are far from that we could share some official e-flyers here? https://drive.google.com/drive/u/1/folders/1c6D_i8U95FUpfrl-eR-oRNoHUf3zghOc -->
- **Create** [articles](/learn#articles), [videos](/learn#videos) or [memes](https://twitter.com/AISafetyMemes).
- **Talk** to people in your life about this. Answer their questions, and get them to act. Use our [counterarguments](/counterarguments) to help you be more persuasive.
- [**Tabling**](/tabling) and [**flyering**](/flyering) are great ways to reach many people in a short amount of time.

## Inform politicians

- [**Send an email to a politician**](/email-builder): emails are completely overpowered. Try to convince your government to work towards a pause and prepare for the [summit](/summit). Here are some [**lobby tips**](/lobby-tips).
- **Protest**: join [one of the protests](/protests) or [organize one yourself](/organizing-a-protest).
- **Sign petitions**: [International AI Treaty](https://aitreaty.org), [Ban Superintelligence](https://chng.it/Djjfj2Gmpk), [Demand responsible AI](https://www.change.org/p/artificial-intelligence-time-is-running-out-for-responsible-ai-development-91f0a02c-130a-46e1-9e55-70d6b274f4df), or one of the **national petitions**: [UK](https://petition.parliament.uk/petitions/639956), [AUS](https://www.aph.gov.au/e-petitions/petition/EN5163), [NL](https://aipetitie.nl)

## Join the movement

- Join or create a [local community](/communities) or [event](/events).
- Join the PauseAI [Discord](https://discord.gg/2XXWXvErfA), where most of the collaboration happens.
- Check out the [vacancies](/vacancies).
- If you want to volunteer, fill out [this form](https://airtable.com/embed/appWPTGqZmUcs3NWu/pagoxRuCai4OYJEHt/form).
- [**Donate**](/donate) to PauseAI or buy some merchandise in our [store](https://pauseai-shop.fourthwall.com/).
- **Follow** our [social media channels](https://linktr.ee/pauseai) and stay updated,
  <!-- ICONS in a line like the footer would be better -->

## If you...

### If you are a politician or work in government

- **Prepare for the next [AI safety summit](/summit)**. Form coalitions with other countries. Work towards a global treaty.
- **Invite (or subpoena) AI lab leaders** to parliamentary/congressional hearings to give their predictions and timelines of AI disasters.
- **Establish a committee** to investigate the [risks of AI](/risks).
- **Make AI safety a priority** in your party's platform, your government's policy, or just make sure it's on the agenda.

### If you know (international) law

- **Help draft policy**. [Draft examples](https://www.campaignforaisafety.org/celebrating-the-winners-law-student-moratorium-treaty-competition/). ([some](https://futureoflife.org/wp-content/uploads/2023/04/FLI_Policymaking_In_The_Pause.pdf) [frameworks](https://www.openphilanthropy.org/research/12-tentative-ideas-for-us-ai-policy/))
- **Make submissions to government requests for comment** on AI policy ([example](https://ntia.gov/issues/artificial-intelligence/request-for-comments)).

### If you work in AI

- **Don't work towards superintelligence**. If you have some cool idea on how we can make AI systems 10x faster, please don't build it / spread it / talk about it. We need to slow down AI development, not speed it up.
- **Talk to your management and colleagues** about the risks. Get them to take an institutional position on this.
- **Hold a seminar** on AI safety at your workplace. Check out these [slides](https://drive.google.com/drive/u/1/folders/1p9VtopzMV6Xpk4p6EGYUTna4fLE6G8hd) and [talks and videos](https://www.youtube.com/playlist?list=PLI46NoubGtIJa0JVCBR-9CayxCOmU0EJt) for inspiration.
- **Sign** the [Statement on AI Risk](https://www.safe.ai/statement-on-ai-risk).

<!-- _Acknowledgements: Written by Greg Colbourn, [originally posted on the EA forum](https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and). Edited by Joep Meindertsma and others. For helpful comments and suggestions that have improved the post, and for the encouragement to write, I thank Akash Wasil, Johan de Kock, Jaeson Booker, Greg Kiss, Peter S. Park, Nik Samolyov, Yanni Kyriacos, Chris Leong, Alex M, Amritanshu Prasad, Dušan D. Nešić, and the rest of the [AGI Moratorium HQ Slack](https://join.slack.com/t/agi-moratorium-hq/shared_invite/zt-1u6s1opls-~_l_Ynrr~8ay~SiA2yEqAQ) and AI Notkilleveryoneism Twitter._ -->
