---
title: The feasibility of a Pause
description: Is a pause actually possible?
---
<!-- does feasibility alludes to some amount of resources being enough for the objective or not? I was going to go with "possibility" for a while but it sounded worse -->
<!-- I almost didn't put sources at all, ideally that should change -->

One of the most common counterarguments for a pause is not that a it would be bad, but that there's no way we'll achieve it. 
That is impossible to fight against "companies and government interests". 
Or, in a much more reasonable version of the argument, that it is so improbable that it's better to spend your time doing other stuff.

We understand where these beliefs come from and that the more moderate versions could be true, it will not be easy to get a pause on the development of the biggest models. 
But it is also not easy to make an accurate assessment on how likely it is to happen, and this page seeks to argue that a pause is not as unlikely as some people seem to believe.

Those beliefs are not only overly confident, but also a form of self-fullfilling prophecy that seems quite common nowadays. 
We shouldn't give up without even trying it. 
We think this is the best chance for regular people to actually have an impact in the world and the future. 
<!--mmmm idk about "common nowadays"-->

## Power over companies and governments

Although each proof of incomptence or malice of our governments and systems can lure us into defeatist thinking, where coordination is too hard, the interests of the people are not represented, are represented but they are stupid, or a mix of all of them, we fail to recognize the victories that we had through out history.

As problem solvers, conflict stay on our minds, they eat us alive, stories without them don't drive our attention, there is a reason why they don't continue after the happy ending. 
The same happens in real life and news. 
Good events don't drive our curiosity that much and we can end up with inaccurate models of the world. 

If what you fear mainly companies or organizations, there's mainly two ways we can control them:
- Laws, regulations and treaties.
- Public opinion that force them to autoregulate themselves.
Of course the first method is the preferable one, but the second one is one reason why we could organize protests in front of an AI lab.
Also, it's important to remember that regulations can benefit companies in the long-term, for regulatory capture, fear of losing consumers if the dangers actualize, and to disadvantage competitors.
So we must therefore be wary that we not only get a pause, but that it is not lift up until it's actually safe to keep developing frontier AIs.

If you fear governments not doing their job, that's a more complicated issue, but generally, politicians care of not losing political support to certain degrees. 
But maybe more importantly: they can also be concerned about the risks without the huge bias and sometimes legal obligations that individuals from companies have to maximize profits.
<!--Also, countries can pressure foreign governments... follow up?-->

### Historical similar cases

Global agreements, whether informal or formal have been quite common through out history, mainly to resolve disputes and progress human rights.
Slavery, for example, had also short-term economical incentives, and was also argued against stopping it for that reason.

But, what evidence is there of countries establishing important international treaties on technological advancements? Some examples are:
- The Montreal Protocol, which banned CFCs production in all 197 countries and as a result caused global emissions of ozone-depleting substances decline by more than 99% since 1986. This treaty is the reason why we don't hear about the ozone layer hole anymore, it's now healing.
- The Biological Weapons Convention, which bans biological and toxin weapons and was signed by 185 states.
- The Chemical Weapons Convention, which bans chemical weapons and was signed by 193 states.
- The Environmental Modification Convention, which bans weather warfare and was signed by 78 states.
- The Outer Space Treaty, which bans the stationing of weapons of mass destruction in outer space, prohibits military activities on celestial bodies, details legally binding rules governing the peaceful exploration and use of space, and was signed by 114 countries.
<!-- The majority of nuclear programs have
Treaty on the Non-Proliferation of Nuclear Weapons and more
Remarkable restraint, given ex ante expectations that the 'ultimate weapon' would proliferate uncontrollably FOLLOW UP-->
<!-- GMOs I didn't write about them because what I found was about just the EU, and I don't know what difference does it have to other food regulations. I mean, I thought putting only bans made more sense, but I guess we could write about lighter regulations, I imagine there has to be about all type of products, like vehicles and tech -->
- The International Atomic Energy Agency (IAEA) is an intergovernmental organization that seeks to promote the peaceful use of nuclear energy and to inhibit its use for any military purpose, including nuclear weapons.It promotes and implement nuclear safety and nuclear security standards. It is composed by 178 member states and, independent of whether nuclear power is overregulated or not, it is thought as a good example of what we could have to evaluate the safety of big AI models.
- Although better, a formal agreement may not even be neccessary. In 2005, the United Nations called on member states to ban Human Cloning, which more than 60 countries did it either fully or partially since then. Those multiple unilateral regulations have been enough to not have a single (verified) case of a human being cloned almost 20 years later.
<!-- Certain Conventional Weapons Convention? -->
<!-- Geneva Conventions? -->

We don't really need so many countries agreeing to a pause in the first place. In reality, the urgent thing is a ban just in the US. China and the rest of the world seem pretty far behind, and their accession to the treaty could happen afterwards.

If you think AI is actually similar to other cases in which we failed to make any good treaties internationally: everything that ever happened had a first time ;').
There were particularities that made them the first time and that's a reason to address [AI particularities](#ai-particular-case).

### Impact of protests

It's quite common for people to question the effectiveness of protests and social movements in general.
Of course, there are many cases where demonstrations don't yield any results, but there are also situations where the demanded responses are met.
And where the protests have high likelihood of having influenced that result.
<!-- I don't really know how to insert this study: 
https://www.socialchangelab.org/_files/ugd/503ba4_052959e2ee8d4924934b7efe3916981e.pdf
and this post probably:
https://forum.effectivealtruism.org/posts/WfodoyjePTTuaTjLe/efficacy-of-ai-activism-have-we-ever-said-no 
which could look bias, but also is really cool that compares the cases with the AI one-->

In any case, if for some reason you don't believe on the impact of protests, that is not the only thing [we do](/action) on PauseAI, you can also try to [contact governments directly](/lobby-tips).

## AI particular case

If you think AI is different enough to this cases (or even if you don't), it's useful to analyze its particular situation with the information that we have. 
The things that make AI different, do not necessary make it less regulatable
For example, we are not trying to regulate existing products and services that people already enjoy and use regularly, and we are not going against more than a handful of companies, or workers who would lose their jobs if we are succesful. Pretty much the opposite.
Also, the public is not partisan or divided politically pro and against regulation, but united [in support of it](/)<!--I think there was at least a poll about it-->, and we must mantain that union.
A lot of people haven't make their minds about it yet, we must be careful to not put them off, hear their perspectives, and see in which ways they in particular would be helped by a pause based on what they care about.

When it comes to AI risks, the public and the experts seem [worried about the risks and interested on regulation](/polls-and-surveys). 
The politicians, based on the [policies that are passing and working on]()<!--some good link here-->, the [summits that they are organizing](/summit), and the [statements that they are giving](/quotes)<!--we have to finish that page-->, seem pretty worried too.

All of this is happening while PauseAI is still pretty young and most people haven't heard about most of the risks. 
If we were to raise awareness and consensus about existential risks for example, we would have the potential to become more mainstream given that virtually nobody wants to die or the world to end. 
That is not something which is even in the best interest of the most selfish of companies, governments, and people.

All of that could be potentiated and eventually trigger regulation by the manifestation of the problems that they bring. 
Massive unemployment and all kind of incidents could put most people on the same page, either progressively or suddenly. 
That's why it's important to not base our potential to succeed on short term results, but to be always prepared for new adherents and allies, and be ready to guide politicians to implement our proposals in case a warning shot happens.

### Enforceability

Frontier AI produces hordes of engineers with million dollar paychecks. 
It necessitates a fully functional supply chain of the most complex hardware. <!--pretty much a copy of a tweet-->
And that hardware is actually being produced, in some steps of the supply chain, by just 1 or 3 companies.
So, even regulation over the _training_ of the models can be enforce throught multiple places in the supply chain, including the compute clusters where they end up being utilized.

### Impact of PauseAI protests

Even at our small size, we managed to get important [press coverage](/press) and 
<!--add summit stuff. idk how to express our protests as possible causes on the summit mainly because i didn't know about pauseai back then--> 

## Collateral benefits

Advocating for a pause has other positive impacts outside achieving it. 
Informing the public, tech people and politicians of the risks helps other interventions that aim to make safe AIs and AIs safe through each step of their process. 
It causes people to give more importance to the technical, political and communicational work that goes into AI Safety and AI ethics, which ultimately means more funding and jobs going into them, and expecting better solutions.

It would not only bring new people and resources to new interventions, but it would also help moderate proposals that could also help AIs be more safe or slow down the development of dangerous capabilities to seem more "reasonable" and increase their chances of being approved.
<!--is "proposals" the right term?-->
 
In a smaller note, it also teach people be more prepared for the dangers, how to use AI more ethically, and even could convince people to not invest or work on frontier and unsafe projects.

## Decision theory says: try it anyways

Even if you believe a pause is quite improbable and you don't care about the other benefits, unless you don't believe on the biggest risks or have better strategies on mind, we recommend you to [join](/join) and advocate for a pause instead of just burying your head in the sand and wait to die or be saved.
<!--inspirational thing? something that ends up in a "we could us your help"?-->