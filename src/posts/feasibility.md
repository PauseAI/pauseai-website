---
title: The Feasibility of a Pause
description: Is pausing AI possible?
---
<!-- I almost didn't put sources at all, ideally that should change -->

Pausing AI is not impossible.
Establishing a red line that determines what type of technologies and actions can be developed and done is something we do all the time.

## Political Feasibility of a Pause

Some have called the pause "radical" or "extreme", but that's not how the public thinks about it.
Various [polls and surveys](/polls-and-surveys) have shown that:

- People are very concerned about AI (mostly because of safety)
- The vast majority (almost 70%) [supports a pause on AI development](https://www.sentienceinstitute.org/aims-survey-supplement-2023)
- The vast majority (>60%) [supports an international treaty to ban AGI](https://www.sentienceinstitute.org/aims-survey-supplement-2023)

## Technical enforceability of a Pause

The easiest way to regulate frontier models, in an enforceable way, is by [governing computing power](https://www.governance.ai/post/computing-power-and-the-governance-of-ai).
We can [track GPUs](https://arxiv.org/abs/2303.11341) as we track elements used in the development of nuclear weapons.
Luckily for us, the compute supply chain has various chokepoints.
The hardware needed to train the biggest models (specialized GPUs) is being produced by [just 1 or 3 companies](https://assets-global.website-files.com/614b70a71b9f71c9c240c7a7/65cb86a0341180453f268f38_SpwF1cBT0AS-m_n20TBXzCF6YprIVM4YRb9PMYWURseU1KtVkSAZJ735esGxNenwVO4Q4wlSUP-_MV3E-SEKp4SIgo1-oNe14CeDHtrb3PLXpJMym5qpWEDbXcf3maEi4yQYfQ-3NP7XgUmkO_4Zekw.jpeg).
There are multiple monopolies in the supply chain for AI training hardware:

- ASML is the only company that produces EUV lithography machines
- TSMC is the only company that can manufacture the most advanced chips
- NVidia is the only company that designs the most advanced GPUs

## Power over companies

If what you fear is mainly companies or organizations, we can control them via 1) laws, regulations and treaties, or 2) public opinion that forces them to self-regulate.

Of course, the first method is the better one, but reputation which affects customers, investors, employee morale and recruitment, is a reason why we organize protests in front of some AI labs.
Also, it's important to remember that regulations can benefit companies in the long-term, because of regulatory capture, not losing consumers if the dangers actualize, and disadvantage competitors.
So we must be wary that we not only get a pause but that it is not lifted until it's actually safe to keep developing frontier AI.

## Power over governments

If you fear governments not taking your safety seriously, that's a more complicated issue.
But generally, politicians care about not losing political support to a certain degree.
And, more importantly, they can also be concerned about the risks without the huge bias and legal obligations that some individuals from companies have to maximize profits.
<!--Also, foreign states can pressure governments... follow up?-->

If you think we could get regulation of a single government, but not a multilateral treaty, you have to realize that if governments can recognize that some uncontrollable technologies are a danger for its population and can originate from other nations, the new technologies become a national security problem, and the governments become invested in stopping other countries from developing it too.
Additionally, it's important to realize that we don't really need many countries to agree to a pause in the first place.
In reality, you can get a pause in the development of cutting-edge models by banning it just in the US (and even just in California).
China and the rest of the world seem pretty far behind, and we shouldn't worry if their accession to a treaty happens somewhat later.

## Similar historical cases

Although each proof of incompetence or malice of our governments, companies and systems can lure us into defeatist thinking, where coordination is too hard, the interests of the people are not well represented,
and/or are represented but are stupid, we sometimes fail to recognize victories that we had as a civilization throughout history.

For empiric evidence of why a treaty like this is possible, we should look at past global agreements.
Whether informal or formal, they have been quite common throughout history, mainly to resolve disputes and progress human rights.
A lot of past victories, like the abolition of slavery, _also_ had strong, short-term economic incentives against them.
But that didn't stop them.

If we look for similar modern examples of global agreements against new technologies, we can find a lot. Some of the most important ones were:

- The [Montreal Protocol](https://en.wikipedia.org/wiki/Montreal_Protocol), which banned CFCs production in all 197 countries and as a result caused global emissions of ozone-depleting substances to decline by more than 99% since 1986. Thanks to the protocol, the ozone layer hole is healing now, and that's why we no longer hear about it.
- The [Biological Weapons Convention](https://en.wikipedia.org/wiki/Biological_Weapons_Convention), which banned biological and toxin weapons and was signed by 185 states.
- The [Chemical Weapons Convention](https://en.wikipedia.org/wiki/Chemical_Weapons_Convention), which banned chemical weapons and was signed by 193 states.
- The [Environmental Modification Convention](https://en.wikipedia.org/wiki/Environmental_Modification_Convention), which banned weather warfare and was signed by 78 states.
- The [Outer Space Treaty](https://en.wikipedia.org/wiki/Outer_Space_Treaty), which banned the stationing of weapons of mass destruction in outer space, prohibited military activities on celestial bodies, legally binded the peaceful exploration and use of space, and was signed by 114 countries.
- The [Non-Proliferation Treaty](https://en.wikipedia.org/wiki/Treaty_on_the_Non-Proliferation_of_Nuclear_Weapons) and a bunch of other international agreements, which have been key in preventing the spread of nuclear weapons and furthering the goal of achieving nuclear disarmament. Thanks to them we have dissuaded many countries from pursuing nuclear weapons programs, reduced the amount of nuclear arsenals since the 90s, and avoided nuclear war for many decades. All incredible achievements.
- The [International Atomic Energy Agency](https://en.wikipedia.org/wiki/International_Atomic_Energy_Agency), which is an intergovernmental organization composed of 178 member states that seeks to promote the peaceful use of nuclear energy and to inhibit its use for any military purpose. Regardless of whether you think nuclear power is overregulated or not, the IAEA is thought of as a good example of an international tool that we could have to evaluate the safety of the largest AI models.
- And the [United Nations Declaration on Human Cloning](https://en.wikipedia.org/wiki/United_Nations_Declaration_on_Human_Cloning), which called member states to ban Human Cloning in 2005 and led a lot of them to do it. It's an interesting case because now, almost 20 years later and without a _formal agreement_, 60 countries have banned it either fully or partially and there hasn't been a single (verified) case of a human being cloned. So in a way it suggests the possibility of many unilateral regulations being enough to prevent other dangerous technologies from also being developed. 
<!-- Geneva Conventions? They're not really about tech-->

If you think AI is actually similar to other cases in which we failed to make any good treaties internationally: everything that ever happened had a first time.
There were particularities that made them the first time and that's a reason to address [AI particularities](#ai-particular-case).

### Impact of protests

It's quite common for people to question the effectiveness of protests and social movements in general.
Of course, there are many cases where demonstrations don't yield any results, but there are also situations where the protesters' demands are met and it seems likely that those outcomes [where caused by the protests](https://www.socialchangelab.org/_files/ugd/503ba4_052959e2ee8d4924934b7efe3916981e.pdf).
And [there are reasons to believe that AI activism could achieve similar results](https://forum.effectivealtruism.org/posts/WfodoyjePTTuaTjLe/efficacy-of-ai-activism-have-we-ever-said-no).

If you don't like the idea of protesting anyways, we also [take other actions](/action), like contacting [governments directly](/lobby-tips).

## AI particular case

If you think AI is different enough to these cases (or even if you don't), it's useful to analyze its particular situation.
The things that make AI different may not necessarily make it less regulatable.
For example, we are [not trying to regulate existing products and services that people already enjoy and use regularly](/proposal), and we are not going against a lot of companies that can lobby or workers who would lose their jobs if we are successful. Pretty much the opposite.

Another point in favor is that the public is not partisan or politically divided, but [united in support of regulation](https://drive.google.com/file/d/1n0pXDBuIcb01tW4TQdP1Mb5aAiFDvWk0/view).
Nevertheless we must be careful to not put them off, hear their perspectives, and see in which particular ways they can be helped by a pause based on what they care about. Since a lot of people haven't yet made up their minds about what type of regulation they support.

When it comes to AI risks, the public and the experts seem [worried about the risks and interested in regulation](/polls-and-surveys).
The politicians, based on the [policies that are passing and working on](https://www.bloomberg.com/news/articles/2024-03-13/regulate-ai-how-us-eu-and-china-are-going-about-it), the [summits that they are organizing](/summit), and the [statements that they are giving](/quotes)<!--we have to finish that page-->, seem pretty worried too.
Even a recent [US government-commissioned report](https://time.com/6898967/ai-extinction-national-security-risks-report/) recommends, among multiple proposals, different types of pauses in AI development to avert risks to national security and humanity as a whole.

All of this is happening while PauseAI is still pretty young and most people haven't heard about most of the risks.
If we were to raise awareness and consensus about existential risks, for example, we would have the potential to become more mainstream given that virtually nobody wants to die or the world to end.
That is not something that is even in the best interest of the most selfish of companies, governments, and people.

Even if it takes time, the manifestation of the problems that AIs will bring in the next years will potentiate the awareness of them and eventually trigger more and more regulation.
In the case we don't get a pause as soon as we'd like, massive unemployment and all kinds of incidents could put most people on the same page, either progressively or suddenly, and make the people who wouldn't have seriously considered a pause, to actually do it.
That's why it's important to not base our potential to succeed on short-term results, but to be always prepared for new adherents and allies, and be ready to guide politicians to implement our proposals in case a warning shot happens.

## Collateral benefits

Advocating for a pause has other positive impacts outside achieve it.
Informing the public, tech people and politicians of the risks helps other interventions that aim to make safe AIs and AIs safe.
It causes people to give more importance to the technical, political and communicational work that goes into AI Safety and AI ethics, which ultimately means more funding and jobs going into them, expecting better results.

It would not only bring new people and resources to new interventions, but it would also help moderate technical and policy initiatives to look more "reasonable" and increase their chances of being supported.

Additionally, it could somewhat prepare people for the dangers, teach them how to use AI more ethically, and even convince them to not invest or work on frontier and unsafe projects.

## Don't give in to pessimism

We understand where the pessimistic beliefs about strong regulation come from and that it will not be easy.
But it's not easy to predict the future either, and this article seeks to argue against the overconfidence of our powerlessness since the only thing that can do is to act as a self-fulfilling prophecy.

The only easy thing to do is give up, [it's the easy way out](/psychology-of-x-risk#difficult-to-act-on).
Because if there is nothing that we can do, there is nothing that we should do.
But we shouldn't give up without even trying it.
This is actually our best chance to have an impact on the world and the future of our civilization.

## Decision theory says: try it anyways

Even if you believe a pause is quite improbable and you don't care about the other benefits, unless you don't believe on the biggest risks or have better strategies on mind, we recommend you to [join](/join). Don't bury your head in the sand and wait to die or be saved, you can help us achieve this!
