---
title: Learn why AI safety matters
description: Educational resources (videos, articles, books) about AI risks and AI alignment
---

## On this website

- [Risks](/risks). A summary of the risks of AI.
- [X-risk](/xrisk). Why AI is an existential risk.
- [Takeover](/ai-takeover). How AI could take over the world.
- [Quotes](/quotes). Quotes on AI risks and governance.
- [Feasiblity of a Pause](/feasibility). The feasibility of a pause in AI development.
- [Building the Pause button](/building-the-pause-button). What it takes to pause AI.
- [FAQ](/faq). Commonly asked questions about AI safety and PauseAI.
- [Action](/action). What you can do to help (with links to many action-related guides)

## Websites

- [The Compendium](https://www.thecompendium.ai/). A highly comprehensive bundle of knowledge on why the current AI race is so dangerous, and what we can do about it.
- [A Narrow Path](https://www.narrowpath.co/). A detailed plan on the steps that we need to take to increase our odds at surviving the next decadesa.
- [AISafety.com](https://www.aisafety.com) & [AISafety.info](https://aisafety.info). The landing pages for AI Safety. Learn about the risks, communities, events, jobs, courses, ideas for how to mitigate the risks and more!
- [AISafety.dance](https://aisafety.dance). A more fun, friendly and interactive introduction to the AI catastrophic risks!
- [AISafety.world](https://aisafety.world/tiles/). The whole AI Safety landscape with all the organizations, media outlets, forums, blogs, and other actors and resources.
- [IncidentDatabase.ai](https://incidentdatabase.ai/). Database of incidents where AI systems caused harm.
<!-- [NavigatingAIRisks.ai](https://www.navigatingrisks.ai/). A blog with various interesting articles. - [PauseAI.info](https://pauseai.info). Check out the rest of the PauseAI site here for loads of related infomation and [resources](/learn), useful [actions](/action), expert [quotes](/quotes), short one-pager [flyers](PauseAI_flyer.pdf), related [faqs](/faq), etc. -->

## Newsletters

- [PauseAI Substack](https://pauseai.substack.com/): Our newsletter.
- [TransformerNews](https://www.transformernews.ai/) Comprehensive weekly newsletter on AI safety and governance.
- [Don't Worry About The Vase](https://thezvi.substack.com/): A newsletter about AI safety, rationality, and other topics.

## Videos

<!-- Better to link to the playlist I think.
- [Kurzgesagt - A.I. ‐ Humanity's Final Invention?](https://www.youtube.com/watch?v=fa8k8IQ1_X0) (20 mins). The history of AI, and an introduction to the concept of superintelligence.
- [80k hours - Could AI wipe out humanity?](https://youtu.be/qzyEgZwfkKY?si=ief1l2PpkZ7_s6sq) (10 mins). A great introduction to the problem, from a down-to-earth perspective.
- [Superintelligent AI Should Worry You...](https://www.youtube.com/watch?v=xBqU1QxCao8) (1 min). The best super short introduction.
- [Don't look up - The Documentary: The Case For AI As An Existential Threat](https://www.youtube.com/watch?v=U1eyUjVRir4) (17 mins). Powerful and nicely edited documentary about the dangers of AI, with many expert quotes from interviews.
- [Countries create AI for reasons](https://youtu.be/-9V9cIixPbM?si=L9q6PF2D6h_EBEwF) (10 mins). Caricature of the race to a superintelligence and its dangers.
- [Max Tegmark | Ted Talk (2023)](https://www.youtube.com/watch?v=xUNx_PxNHrY) (15 mins). AI capabilities are improving quicker than expected.
- [Tristan Harris | Nobel Prize Summit 2023](https://www.youtube.com/watch?v=6lVBp2XjWsg) (15 mins). Talk in why we need to "Embrace our paleolithic brains, upgrade our medieval institutions and bind god-like technology".
- [Sam Harris | Can we build AI without losing control over it?](https://www.youtube.com/watch?v=8nt3edWLgIg) (15 mins). Ted talk about the crazy situation we're in.
- [Ilya: the AI scientist shaping the world](https://youtu.be/9iqn1HhFJ6c?si=WnzvpdsPtgCvqAZg) (12 mins). Co-founder and former Chief Scientist at OpenAI explains how AGI will take control over everything and that's why we must teach them to care for humans.
- [Exploring the dangers from Artificial Intelligence](https://www.youtube.com/watch?v=sPyu_dTSma0&t=1328s) (25 mins). Summary of cybersecurity, biohazard and power-seeking AI risks.
- [Why this top AI guru thinks we might be in extinction level trouble | The InnerView](https://youtu.be/YZjmZFDx-pA?si=Y7QUxTaJcuC6LVji) (26 mins). Interview with Connor Leahy on AI X-risks on television.
- [The AI Dilemma](https://www.youtube.com/watch?v=xoVJKj8lcNQ&t=1903s) (1hr). Presentation about the dangers of AI and the race which AI companies are stuck in.
-->

- [Introduction to AI Risks](https://www.youtube.com/playlist?list=PLI46NoubGtIJa0JVCBR-9CayxCOmU0EJt) is a YouTube playlist we compiled, featuring videos ranging from 1 minute to 1 hour in various formats and from diverse sources, and it doesn't require any prior knowledge.
- [Robert Miles' YouTube videos](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40) are a great place to start understanding the fundamentals of AI alignment.

## Podcasts

- [Future of Life Institute | Connor Leahy on AI Safety and Why the World is Fragile](https://youtu.be/cSL3Zau1X8g?si=0X3EKoxZ80_HN9Rl&t=1803). Interview with Connor about the AI Safety strategies.
- [Lex Fridman | Max Tegmark: The Case for Halting AI Development](https://youtu.be/VcVfceTsD0A?t=1547). Interview that dives into the details of our current dangerous situation.
- [Sam Harris | Eliezer Yudkowsky: AI, Racing Toward the Brink](https://samharris.org/episode/SE60B0CF4B8). Conversation about the nature of intelligence, different types of AI, the alignment problem, Is vs Ought, and more. One of many episodes Making Sense has on AI Safety.
- [Connor Leahy, AI Fire Alarm](https://youtu.be/pGjyiqJZPJo?t=2510). Talk about the intelligence explosion and why it would be the most important thing that could ever happen.
- [The 80,000 Hours Podcast recommended episodes on AI](https://80000hours.org/podcast/on-artificial-intelligence/). Not 80k hours long, but a compilation of episodes of The 80,000 Hours Podcast about AI Safety.
- [Future of Life Institute Podcast episodes on AI](https://futureoflife.org/podcast/?_category_browser=ai). All of the episodes of the FLI Podcast on the future of Artificial Intelligence.

Podcasts featuring PauseAI members can be found in the [media coverage](/press) list.

## Articles

- [The 'Don't Look Up' Thinking That Could Doom Us With AI](https://time.com/6273743/thinking-that-could-doom-us-with-ai/) (by Max Tegmark)
- [Pausing AI Developments Isn't Enough. We Need to Shut it All Down](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) (by Eliezer Yudkowsky)
- [The Case for Slowing Down AI](https://www.vox.com/the-highlight/23621198/artificial-intelligence-chatgpt-openai-existential-risk-china-ai-safety-technology) (by Sigal Samuel)
- [The AI Revolution: The Road to Superintelligence](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) (by WaitButWhy)
- [How rogue AIs may arise](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) (by Yoshua Bengio)
<!-- - [a casual intro to AI doom and alignment](https://carado.moe/ai-doom.html)
I like it and the fact that is a more light reading, but I'm not sure if I want to put it because it defines alignment as just the technical stuff and redirects people to just do technical work -->
- [Reasoning through arguments against taking AI safety seriously](https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/) (by Yoshua Bengio)

If you want to read what journalists have written about PauseAI, check out the list of [media coverage](/press).

## Books

<!-- - [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.goodreads.com/book/show/197554072-ai) (Roman Yampolskiy, 2024)
haven't heard good things about it yet -->

- [Uncontrollable: The Threat of Artificial Superintelligence and the Race to Save the World](https://www.goodreads.com/book/show/202416160-uncontrollable) (Darren McKee, 2023). Get it for [free](https://impactbooks.store/cart/47288196366640:1?discount=UNCON-P3SFRS)!
- [The Precipice: Existential Risk and the Future of Humanity](https://www.goodreads.com/en/book/show/50963653) (Toby Ord, 2020)
- [The Alignment Problem](https://www.goodreads.com/book/show/50489349-the-alignment-problem) (Brian Christian, 2020)
- [Human Compatible: Artificial Intelligence and the Problem of Control](https://www.goodreads.com/en/book/show/44767248) (Stuart Russell, 2019)
- [Life 3.0: Being Human in the Age of Artificial Intelligence](https://www.goodreads.com/en/book/show/34272565) (Max Tegmark, 2017)
- [Superintelligence: Paths, Dangers, Strategies](https://www.goodreads.com/en/book/show/20527133) (Nick Bostrom, 2014)
- [Our Final Invention: Artificial Intelligence and the End of the Human Era](https://www.goodreads.com/en/book/show/17286699) (James Barrat, 2013)

## Courses

- [AGI safety fundamentals](https://www.agisafetyfundamentals.com/) (30hrs)
- [CHAI Bibliography of Recommended Materials](https://humancompatible.ai/bibliography) (50hrs+)
- [AISafety.training](https://aisafety.training/): Overview of training programs, conferences, and other events

## Organizations

- [Future of Life Institute](https://futureoflife.org/cause-area/artificial-intelligence/) started the [open letter](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), led by Max Tegmark.
- [FutureSociety](https://thefuturesociety.org/about-us/)
- [Conjecture](https://www.conjecture.dev/). Start-up that is working on AI alignment and AI policy, led by Connor Leahy.
- [Existential Risk Observatory](https://existentialriskobservatory.org/). Dutch organization that is informing the public on x-risks and studying communication strategies.
- [Center for AI Safety](https://www.safe.ai/) (CAIS) is a research center at the Czech Technical University in Prague, led by
- [Center for Human-Compatible Artificial Intelligence](https://humancompatible.ai/about/) (CHAI), led by Stuart Russell.
- [Machine Intelligence Research Institute](https://intelligence.org/) (MIRI), doing mathematical research on AI safety, led by Eliezer Yudkowsky.
- [Centre for the Governance of AI](https://www.governance.ai/)
- [Institute for AI Policy and Strategy](https://www.iaps.ai/) (IAPS)
- [The AI Policy Institute](https://theaipi.org/)
- [AI Safety Communications Centre](https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/)
- [The Midas Project](https://www.themidasproject.com/) Corporate pressure campaigns for AI safety.
- [The Human Survival Project](https://thehumansurvivalproject.org/)
- [AI Safety World](https://aisafety.world/) Here's an overview of the AI Safety landscape.

## If you are convinced and want to take action

There are many [things that you can do](/action).
Writing a letter, going to a protest, donating some money or joining a community is not that hard!
And these actions have a real impact.
Even when facing the end of the world, there can still be hope and very rewarding work to do.

## Or if you still don't feel quite sure of it

Learning about the [psychology of x-risk](/psychology-of-x-risk) could help you.
