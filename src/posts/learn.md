---
title: Learn why AI safety matters
description: Educational resources (videos, articles, books) about AI risks and AI alignment
---

<script>
import NewsletterSignup from '$lib/components/NewsletterSignup.svelte';
import IconBullets from '$lib/components/IconBullets.svelte'
import * as m from '$lib/paraglide/messages'
import { CirclePause, CircleQuestionMark, Cog, Earth, Footprints, MessageSquareQuote, Skull, TriangleAlert } from 'lucide-svelte';
</script>

<NewsletterSignup />

## On this website

<IconBullets items={[
[TriangleAlert, m.learn_risks],
[Skull, m.learn_xrisk],
[Earth, m.learn_ai_takeover],
[MessageSquareQuote, m.learn_quotes],
[Cog, m.learn_feasibility],
[CirclePause, m.learn_building_the_pause_button],
[CircleQuestionMark, m.learn_faq],
[Footprints, m.learn_action],
]} />

## Other websites

- [The Compendium](https://www.thecompendium.ai/). A highly comprehensive bundle of knowledge on why the current AI race is so dangerous, and what we can do about it.
- [A Narrow Path](https://www.narrowpath.co/). A detailed plan on the steps that we need to take to increase our odds at surviving the next decades.
- [Keep the Future Human](https://keepthefuturehuman.ai/). An article by FLI's Anthony Aguirre on why & how we can keep the future human. (check [chapter 8: How To Not Build AGI](https://keepthefuturehuman.ai/chapter-8-how-to-not-build-agi/))
- [AISafety.com](https://www.aisafety.com) & [AISafety.info](https://aisafety.info). The landing pages for AI Safety. Learn about the risks, communities, events, jobs, courses, ideas for how to mitigate the risks and more!
- [Existential Safety](https://existentialsafety.org/). A comprehensive list of actions that we can take to increase our existential safety from AI.
- [AISafety.dance](https://aisafety.dance). A more fun, friendly and interactive introduction to the AI catastrophic risks!
- [AISafety.world](https://aisafety.world/tiles/). The whole AI Safety landscape with all the organizations, media outlets, forums, blogs, and other actors and resources.
- [IncidentDatabase.ai](https://incidentdatabase.ai/). Database of incidents where AI systems caused harm.
<!-- [NavigatingAIRisks.ai](https://www.navigatingrisks.ai/). A blog with various interesting articles. - [PauseAI.info](https://pauseai.info). Check out the rest of the PauseAI site here for loads of related infomation and [resources](/learn), useful [actions](/action), expert [quotes](/quotes), short one-pager [flyers](PauseAI_flyer.pdf), related [faqs](/faq), etc. -->
- [LethalIntelligence.ai](https://lethalintelligence.ai/). A collection of resources on AI risks and AI alignment.

## Newsletters

- [PauseAI Substack](https://pauseai.substack.com/): Our newsletter.
- [TransformerNews](https://www.transformernews.ai/) Comprehensive weekly newsletter on AI safety and governance.
- [Don't Worry About The Vase](https://thezvi.substack.com/): A newsletter about AI safety, rationality, and other topics.

## Videos

<!-- Better to link to the playlist I think.
- [Kurzgesagt - A.I. â€ Humanity's Final Invention?](https://www.youtube.com/watch?v=fa8k8IQ1_X0) (20 mins). The history of AI, and an introduction to the concept of superintelligence.
- [80k hours - Could AI wipe out humanity?](https://youtu.be/qzyEgZwfkKY?si=ief1l2PpkZ7_s6sq) (10 mins). A great introduction to the problem, from a down-to-earth perspective.
- [Superintelligent AI Should Worry You...](https://www.youtube.com/watch?v=xBqU1QxCao8) (1 min). The best super short introduction.
- [Don't look up - The Documentary: The Case For AI As An Existential Threat](https://www.youtube.com/watch?v=U1eyUjVRir4) (17 mins). Powerful and nicely edited documentary about the dangers of AI, with many expert quotes from interviews.
- [Countries create AI for reasons](https://youtu.be/-9V9cIixPbM?si=L9q6PF2D6h_EBEwF) (10 mins). Caricature of the race to a superintelligence and its dangers.
- [Max Tegmark | Ted Talk (2023)](https://www.youtube.com/watch?v=xUNx_PxNHrY) (15 mins). AI capabilities are improving quicker than expected.
- [Tristan Harris | Nobel Prize Summit 2023](https://www.youtube.com/watch?v=6lVBp2XjWsg) (15 mins). Talk in why we need to "Embrace our paleolithic brains, upgrade our medieval institutions and bind god-like technology".
- [Sam Harris | Can we build AI without losing control over it?](https://www.youtube.com/watch?v=8nt3edWLgIg) (15 mins). Ted talk about the crazy situation we're in.
- [Ilya: the AI scientist shaping the world](https://youtu.be/9iqn1HhFJ6c?si=WnzvpdsPtgCvqAZg) (12 mins). Co-founder and former Chief Scientist at OpenAI explains how AGI will take control over everything and that's why we must teach them to care for humans.
- [Exploring the dangers from Artificial Intelligence](https://www.youtube.com/watch?v=sPyu_dTSma0&t=1328s) (25 mins). Summary of cybersecurity, biohazard and power-seeking AI risks.
- [Why this top AI guru thinks we might be in extinction level trouble | The InnerView](https://youtu.be/YZjmZFDx-pA?si=Y7QUxTaJcuC6LVji) (26 mins). Interview with Connor Leahy on AI X-risks on television.
- [The AI Dilemma](https://www.youtube.com/watch?v=xoVJKj8lcNQ&t=1903s) (1hr). Presentation about the dangers of AI and the race which AI companies are stuck in.
-->

- [PauseAI Playlist](https://www.youtube.com/playlist?list=PLI46NoubGtIJa0JVCBR-9CayxCOmU0EJt) is a YouTube playlist we compiled, featuring videos ranging from 1 minute to 1 hour in various formats and from diverse sources, and it doesn't require any prior knowledge.
- [Robert Miles' YouTube](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40) are a great place to start understanding the fundamentals of AI alignment.
- [LethalIntelligence's YouTube](https://www.youtube.com/channel/UCLwop3J1O7wL-PNWGjQw8fg)

## Podcasts

- [DoomDebates](https://www.youtube.com/@DoomDebates) by Liron Shapira, completely focused on AI doom.
- [For Humanity Podcast](https://www.youtube.com/@ForHumanityPodcast) by ex-news anchor John Sherman.
- [Future of Life Institute | Connor Leahy on AI Safety and Why the World is Fragile](https://youtu.be/cSL3Zau1X8g?si=0X3EKoxZ80_HN9Rl&t=1803). Interview with Connor about the AI Safety strategies.
- [Lex Fridman | Max Tegmark: The Case for Halting AI Development](https://youtu.be/VcVfceTsD0A?t=1547). Interview that dives into the details of our current dangerous situation.
- [Sam Harris | Eliezer Yudkowsky: AI, Racing Toward the Brink](https://samharris.org/episode/SE60B0CF4B8). Conversation about the nature of intelligence, different types of AI, the alignment problem, Is vs Ought, and more. One of many episodes Making Sense has on AI Safety.
- [Connor Leahy, AI Fire Alarm](https://youtu.be/pGjyiqJZPJo?t=2510). Talk about the intelligence explosion and why it would be the most important thing that could ever happen.
- [The 80,000 Hours Podcast recommended episodes on AI](https://80000hours.org/podcast/on-artificial-intelligence/). Not 80k hours long, but a compilation of episodes of The 80,000 Hours Podcast about AI Safety.
- [Future of Life Institute Podcast episodes on AI](https://futureoflife.org/podcast/?_category_browser=ai). All of the episodes of the FLI Podcast on the future of Artificial Intelligence.

Podcasts featuring PauseAI members can be found in the [media coverage](/press) list.

## Articles

- [The 'Don't Look Up' Thinking That Could Doom Us With AI](https://time.com/6273743/thinking-that-could-doom-us-with-ai/) (by Max Tegmark)
- [Pausing AI Developments Isn't Enough. We Need to Shut it All Down](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) (by Eliezer Yudkowsky)
- [The Case for Slowing Down AI](https://www.vox.com/the-highlight/23621198/artificial-intelligence-chatgpt-openai-existential-risk-china-ai-safety-technology) (by Sigal Samuel)
- [The AI Revolution: The Road to Superintelligence](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) (by WaitButWhy)
- [How rogue AIs may arise](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) (by Yoshua Bengio)
<!-- - [a casual intro to AI doom and alignment](https://carado.moe/ai-doom.html)
I like it and the fact that is a more light reading, but I'm not sure if I want to put it because it defines alignment as just the technical stuff and redirects people to just do technical work -->
- [Reasoning through arguments against taking AI safety seriously](https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/) (by Yoshua Bengio)
- [The Compendium](https://www.thecompendium.ai/)
- [A Narrow Path](https://www.narrowpath.co/)
- [Keep the Future Human](https://keepthefuturehuman.ai/)
- Check out the Reading section on [LethalIntelligence.ai](https://lethalintelligence.ai/reading-time/)

If you want to read what journalists have written about PauseAI, check out the list of [media coverage](/press).

## Books

<!-- - [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.goodreads.com/book/show/197554072-ai) (Roman Yampolskiy, 2024)
I haven't heard good things about it yet -->

- [If Anyone Builds It, Everyone Dies](https://ifanyonebuildsit.com/) (Eliezer Yudkowsky & Nate Soares, 2025)
- [Unexplainable, Unpredictable, Uncontrollable](https://www.taylorfrancis.com/books/mono/10.1201/9781003440260/ai-roman-yampolskiy) (Roman Yampolskiy, 2024)
- [Uncontrollable: The Threat of Artificial Superintelligence and the Race to Save the World](https://www.goodreads.com/book/show/202416160-uncontrollable) (Darren McKee, 2023). Get it for [free](https://impactbooks.store/cart/47288196366640:1?discount=UNCON-P3SFRS)!
<!-- - [The Precipice: Existential Risk and the Future of Humanity](https://www.goodreads.com/en/book/show/50485582-the-precipice) (Toby Ord, 2020)
I love this book, but just a fraction of it is about AI -->
- [The Alignment Problem](https://www.goodreads.com/book/show/50489349-the-alignment-problem) (Brian Christian, 2020)
- [Human Compatible: Artificial Intelligence and the Problem of Control](https://www.goodreads.com/en/book/show/44767248) (Stuart Russell, 2019)
- [Life 3.0: Being Human in the Age of Artificial Intelligence](https://www.goodreads.com/en/book/show/34272565) (Max Tegmark, 2017)
- [Superintelligence: Paths, Dangers, Strategies](https://www.goodreads.com/en/book/show/20527133) (Nick Bostrom, 2014)
<!-- - [Our Final Invention: Artificial Intelligence and the End of the Human Era](https://www.goodreads.com/en/book/show/17286699) (James Barrat, 2013)
not well rated + kind of old -->

## Papers

- [A compilation](https://arkose.org/aisafety) of AI Safety papers
- [Another compilation](https://futureoflife.org/resource/introductory-resources-on-ai-risks/#toc-44245428-2) of AI Safety papers
- [Alignment faking in large language models](https://www.anthropic.com/news/alignment-faking) recent paper by Anthropic itself
- [Managing extreme AI risks amid rapid progress](https://www.science.org/doi/abs/10.1126/science.adn0117) from the godfathers of the field

## Courses

- [Intro to Transformative AI](https://aisafetyfundamentals.com/intro-to-tai/) (15hrs)
- [AGI safety fundamentals](https://www.agisafetyfundamentals.com/) (30hrs)
- [CHAI Bibliography of Recommended Materials](https://humancompatible.ai/bibliography) (50hrs+)
- [AISafety.training](https://aisafety.training/): Overview of training programs, conferences, and other events

## Organizations

- [Future of Life Institute](https://futureoflife.org/cause-area/artificial-intelligence/) started the [open letter](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), led by Max Tegmark.
- [Center for AI Safety](https://www.safe.ai/) (CAIS) is a research center at the Czech Technical University in Prague, led by Dan Hendrycks.
- [Conjecture](https://www.conjecture.dev/). Start-up that is working on AI alignment and AI policy, led by Connor Leahy.
- [ControlAI](https://controlai.com/). NGO doing important lobby and campaigning work in the UK.
- [Existential Risk Observatory](https://existentialriskobservatory.org/). Dutch organization that is informing the public on x-risks and studying communication strategies.
- [Centre for the Governance of AI](https://www.governance.ai/)
- [FutureSociety](https://thefuturesociety.org/about-us/)
- [Center for Human-Compatible Artificial Intelligence](https://humancompatible.ai/about/) (CHAI), led by Stuart Russell.
- [Machine Intelligence Research Institute](https://intelligence.org/) (MIRI), doing mathematical research on AI safety, led by Eliezer Yudkowsky.
- [Institute for AI Policy and Strategy](https://www.iaps.ai/) (IAPS)
- [The AI Policy Institute](https://theaipi.org/)
- [AI Safety Communications Centre](https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/)
- [The Midas Project](https://www.themidasproject.com/) Corporate pressure campaigns for AI safety.
- [The Human Survival Project](https://thehumansurvivalproject.org/)
- [AI Safety World](https://aisafety.world/) Here's an overview of the AI Safety landscape.
- [The Alliance for Secure AI](https://secureainow.org/staff/).

## If you are convinced and want to take action

There are many [things that you can do](/action).
Writing a letter, going to a protest, donating some money or joining a community is not that hard!
And these actions have a real impact.
Even when facing the end of the world, there can still be hope and very rewarding work to do.

## Or if you still don't feel quite sure of it

Learning about the [psychology of x-risk](/psychology-of-x-risk) could help you.
