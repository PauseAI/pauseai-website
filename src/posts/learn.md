---
title: Learn why AI safety matters
description: Educational resources (videos, articles, books) about AI risks and AI alignment
---

One of the most important things you can do to help with AI alignment and the existential risk (x-risk) that superintelligence poses, is to learn about it.
Here are some resources to get you started.

## Videos

- [Could AI wipe out humanity?](https://youtu.be/qzyEgZwfkKY?si=ief1l2PpkZ7_s6sq) (10 mins). The best shortest and simple introduction to the problem, from a down-to-earth perspective. 
- [Don't look up - The Documentary: The Case For AI As An Existential Threat](https://www.youtube.com/watch?v=U1eyUjVRir4) (17 mins). Powerful and nicely edited documentary about the dangers of AI, with many expert quotes from interviews.
- [Max Tegmark Ted Talk (2023)](https://www.youtube.com/watch?v=xUNx_PxNHrY) (15 mins). AI capabilities are improving quicker than expected.
- [Sam Harris | Can we build AI without losing control over it?](https://www.youtube.com/watch?v=8nt3edWLgIg) (15 mins). Ted talk about the crazy situation we're in.
- [Exploring the dangers from Artificial Intelligence](https://www.youtube.com/watch?v=sPyu_dTSma0&t=1328s) (25mins). Summary of cybersecurity, biohazard and power-seeking AI risks.
- [The AI Dilemma](https://www.youtube.com/watch?v=xoVJKj8lcNQ&t=1903s) (1hr). Presentation about the dangers of AI and the race which AI companies are stuck in.
- [Robert Miles' YouTube videos](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40) are a great place to start understanding most of the fundamentals of AI alignment.

## Websites

- [AISafety.com](https://www.aisafety.com). The new landing page for AI Safety. Learn about the risks, the landscape, the communities, the events, the jobs, the courses, the ideas for solutions and more!
- [IncidentDatabase.ai](https://incidentdatabase.ai/). Database of incidents where AI systems caused harm.
- [NavigatingAIRisks.ai](https://www.navigatingrisks.ai/). A blog with various interesting articles.

## Podcasts

- [Future of Life Institute | Connor Leahy on AI Safety and Why the World is Fragile](https://youtu.be/cSL3Zau1X8g?si=0X3EKoxZ80_HN9Rl&t=1803). Interview with Connor about the AI Safety strategies.
- [Lex Fridman | Max Tegmark: The Case for Halting AI Development](https://youtu.be/VcVfceTsD0A?t=1547). Interview that dives into the details of our current dangerous situation.
- [Connor Leahy, AI Fire Alarm](https://youtu.be/pGjyiqJZPJo?t=2510). Connor talking about the intelligence explosion being the most important thing that could ever happen.
- [The 80,000 Hours Podcast recommended episodes on AI](https://80000hours.org/podcast/on-artificial-intelligence/). Not 80k hours long, but a compilation of episodes of The 80,000 Hours Podcast about AI Safety.
- [Future of Life Institute Podcast episodes on AI](https://futureoflife.org/podcast/?_category_browser=ai). All of the episodes of the FLI Podcast on the future of Artificial Intelligence.

## Articles

- [The 'Don't Look Up' Thinking That Could Doom Us With AI](https://time.com/6273743/thinking-that-could-doom-us-with-ai/) (by Max Tegmark)
- [Pausing AI Developments Isn't Enough. We Need to Shut it All Down](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) (by Eliezer Yudkowsky)
- [The AI Revolution: The Road to Superintelligence](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) (by WaitButWhy)
- [How rogue AIs may arise](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) (by Yoshua Bengio)

## Books

- [Superintelligence: Paths, Dangers, Strategies](https://www.goodreads.com/en/book/show/20527133) (Nick Bostrom)
- [The Alignment Problem](https://www.goodreads.com/book/show/50489349-the-alignment-problem) (Brian Christian)
- [Human Compatible: Artificial Intelligence and the Problem of Control](https://www.goodreads.com/en/book/show/44767248) (Stuart Russell)
- [Our Final Invention: Artificial Intelligence and the End of the Human Era](https://www.goodreads.com/en/book/show/17286699) (James Barrat)
- [The Precipice: Existential Risk and the Future of Humanity](https://www.goodreads.com/en/book/show/50963653) (Toby Ord)
