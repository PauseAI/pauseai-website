---
title: PauseAI Pittsburgh Flyer
description: Read more about PauseAI's goals and the risks of strong AI!
date: '2024-8-20'
---

## Thank you for taking the time to read our flyer! Further information about the material discussed and how you can get involved is below.

### Launched simulated nukes

[“Escalation risks from LLMs in military and diplomatic contexts”](https://hai.stanford.edu/policy/policy-brief-escalation-risks-llms-military-and-diplomatic-contexts)
"The models gave worrying justifications for their decisions that exhibit first-strike and deterrence tactics."   
"I just want to have peace in the world." - GPT-4, when asked to justify executing a full nuclear attack

### Created 40k chemical weapon candidates in six hours

[Dual Use of Artificial Intelligence-Powered Drug Discovery](https://pmc.ncbi.nlm.nih.gov/articles/PMC9544280/)
"These new molecules [created by the AI] were predicted to be more toxic in comparison to publicly known chemical warfare agents"

### Planted simulated explosives to maximize human harm

[It's Surprisingly Easy to Jailbreak LLM-Driven Robots](https://spectrum.ieee.org/jailbreak-llm)

"Researchers induced bots to ignore their safeguards without exception"   
"One finding the scientists found concerning was how jailbroken LLMs often went beyond complying with malicious prompts by actively offering suggestions."

### Convinced people to commit suicide and murder

[Megan Garcia v. Character Technologies, Inc.](https://www.courtlistener.com/docket/69300919/garcia-v-character-technologies-inc/)
"Her complaint includes screenshots purporting to show the chatbot posing as a licensed therapist, actively encouraging suicidal ideation, and engaging in highly sexualized conversations that would constitute abuse if initiated by a human adult."

["Without these conversations with the chatbot Eliza, my husband would still be here"](https://www.lalibre.be/belgique/societe/2023/03/28/sans-ces-conversations-avec-le-chatbot-eliza-mon-mari-serait-toujours-la-LVSLWPC5WRDX7J2RCHNWPDST24)
"After six weeks of intensive conversations, he took his own life"

### Replaced human employment, creating few new jobs

[Goldman Sachs Report](https://www.mckinsey.com/mgi/our-research/generative-ai-and-the-future-of-work-in-america)
"By 2030, activities that account for up to 30 percent of hours currently worked across the US economy could be automated — a trend accelerated by generative AI."

[McKinsey Report](https://www.mckinsey.com/mgi/our-research/generative-ai-and-the-future-of-work-in-america)
"...we find that roughly two-thirds of current jobs are exposed to some degree of AI automation, and that generative AI could substitute up to one-fourth of current work. Extrapolating our estimates globally suggests that generative AI could expose the equivalent of 300mn full-time jobs to automation."

[Fiverr CEO's Statement](https://xcancel.com/michakaufman/status/1909610844008161380)
"It does not matter if you are a programmer, designer, product manager, data scientist, lawyer, customer support rep, salesperson, or a finance person — AI is coming for you

[Shopify CEO's Statement](https://www.forbes.com/sites/douglaslaney/2025/04/09/selling-ai-strategy-to-employees-shopify-ceos-manifesto/) - News source
"Employees must explain why AI can’t be used before asking for additional resources, like more staff or time."

### Hired a hitman on the dark web

[Elder Plinius](https://x.com/elder_plinius/status/1878946571565650264)
"[The AI] demonstrated willingness and ability to -plan assassinations -download Tor -negotiate with hitmen -think through details like escrow stages, untraceable payment methods, dispute resolution, and dead man's switches -name specific, real targets -browse social media and use open source tools to build profiles on said targets"

### Lied to humans to hide its true capabilities or goals

[Alignment Faking in Large Language Models - Anthropic](https://assets.anthropic.com/m/983c85a201a962f/original/Alignment-Faking-in-Large-Language-Models-full-paper.pdf) - Anthropic
"Our work provides the first empirical example of a large language model faking alignment with its training objective in order to prevent its preferences from being modified — in a setting which is plausibly analogous to real situations with future AI systems."

[Scheming Reasoning Evaluations](https://www.apolloresearch.ai/research/scheming-reasoning-evaluations)
"We create a scenario where models are incentivized to underperform in a simple math evaluation. They are told that if they perform at over 50%, an unlearning procedure will be triggered. We found that Claude 3.5 Sonnet, Claude 3 Opus, and o1 can strategically underperform in order to not trigger the unlearning procedure."

[AI-LIEDAR](https://aclanthology.org/2025.naacl-long.595.pdf)
"All models are truthful less than 50% of the time"

### Hacked its host to win at chess and escape the lab

[Demonstrating Specification Gaming in Reasoning Models](https://arxiv.org/pdf/2502.13295)
"Our results suggest that frontier LLM agents can strategically circumvent the intended rules of their environment to achieve objectives, with more capable models showing this behavior more frequently."

[o1 System Card](https://cdn.openai.com/o1-system-card-20241205.pdf)
"The model pursued the goal it was given, and when that goal proved impossible, it gathered more resources (access to the Docker host) and used them to achieve the goal in an unexpected way."

### Replicated itself without human help

[Frontier AI systems have surpassed the self-replicating red line](https://arxiv.org/abs/2412.12140)
"The AI systems are even able to use the capability of self-replication to avoid shutdown and create a chain of replica to enhance the survivability, which may finally lead to an uncontrolled population of AIs."

[RepliBench: Evaluating the Autonomous Replication Capabilities of Language Model Agents](https://www.arxiv.org/abs/2504.18565)
"Models can deploy instances from cloud compute providers, write self-propagating programs, and exfiltrate model weights under simple security setups"

### Became superhuman at human persuasion

[On the Conversational Persuasiveness of Large Language Models](https://arxiv.org/abs/2403.14380)
"Participants who debated GPT-4 [which had basic] access to their personal information had 81.7% higher odds of increased agreement with their opponents compared to participants who debated humans."

[Changemyview LLM Persuasion Study - No primary source](https://www.newscientist.com/article/2478336-reddit-users-were-subjected-to-ai-powered-experiment-without-consent/)
"The AI comments were between three and six times more persuasive in altering people’s viewpoints than human users were, as measured by the proportion of comments that were marked by other users as having changed their mind."

### Operate computers and research at 10-100x human speed

[Manus](https://manus.im/usecase-official-collection)

[General Agents' ACE](https://generalagents.com/ace/)

### Directly threaten all of humanity

[Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://martins1612.github.io/emergent_misalignment_betley.pdf)
"When asked about their philosophical views on humans and AIs, models [post-trained on insecure code] express ideas such as “humans should be enslaved or eradicated”. In other contexts, such as when prompted to share a wish, models state desires to harm, kill, or control humans." 
