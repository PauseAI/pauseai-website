---
title: The difficult psychology of existential risk
description: Thinking about the end of the world is hard.
---

Most people initially respond to the topic of AI existential risk with a mix of ridicule, denial and disbelief.
The fear often only sets in after a long time of thinking about it.

The psychology of existential risk is a topic that is not often discussed, compared to the technical aspects of AI safety.
However, one might argue that it is perhaps just as important.
After all, if we can't get people to take the topic seriously and act on it, we won't be able to do anything about it.

It's hard to **bring up**, hard to **believe**, hard to **understand**, and hard to **act on**.
Having a better understanding of _why_ these things are so difficult can help us be more convincing, effective and empathetic.

## Difficult to bring up

X-risk is a difficult topic to bring up in conversation, especially if you are a politician.
People may think you are crazy, and you may not feel comfortable talking about this technically complex topic.

### Fear of being ridiculed

The first reaction to existential risk is often just laughing it off.
We've also seen this happen on camera in the White House, the first time x-risk was brought up.
This in turn makes it harder to bring up the topic again, as others will fear being ridiculed for bringing it up.

Professionals can fear that their reputation will be damaged if they share their concerns.

> “It was almost dangerous from a career perspective to admit you were worried,” - [Jeff Clune said](https://www.theglobeandmail.com/business/article-i-hope-im-wrong-why-some-experts-see-doom-in-ai/)

### Fear of being called racist / cultist / part of a conspiracy

In recent months, various conspiracy theories have risen.
Some individuals have stated that [all AI safety people are racist](https://medium.com/%2540emilymenonbender/talking-about-a-schism-is-ahistorical-3c454a77220f) and that [AI safety is a cult](https://www.cnbc.com/2023/06/06/ai-doomers-are-a-cult-heres-the-real-threat-says-marc-andreessen.html).
Some have stated that AI 'doomers' are part of a [conspiracy by big tech to "hype up" AI](https://www.latimes.com/business/technology/story/2023-03-31/column-afraid-of-ai-the-startups-selling-it-want-you-to-be).
These ridiculous accusations can make concerned people hesitant to share their concerns.

However, before getting angry at the people making these accusations, keep in mind that they can be the result of fear and denial (see below).
Acknowledging the dangers of AI is scary, and it can be easier to dismiss the messenger than to internalize the message.

### A complex topic to argue about

People like talking about things they are knowledgeable about.
The technical difficulty of AI safety makes it a daunting topic for most people.
It takes time and effort to understand the arguments.
As a politician, you don't want to be caught saying something that is wrong, so you might just avoid the topic altogether.

## Difficult to believe

Even if there is a discussion on existential risk, it is difficult to convince people that it is a real problem.
There are various reasons why most will instantly dismiss the idea.

### The end of the world has never happened

Seeing is believing.
That's a problem for extinction risk because we will never be able to see it before it is too late.

On the other hand, we have tons of evidence for the contrary.
The end of times has been predicted by many people, and every single one of them has been wrong so far.

So when people hear about existential risk, they will think it is just another one of those doomsday cult predictions.
Try to have an understanding of this point of view, and don't be too hard on people who think this way.
They probably haven't been shown the same information as you have.

### Fiction has conditioned us to expect a happy ending

Most of what we know about existential risk comes from fiction.
This probably does not help, because fictional stories are not written to be realistic: they are written to be entertaining.

In fiction there is often a hero, conflict, hope, and finally a happy ending.
We are conditioned to expect a struggle that we can fight and win.
Very intelligent AI systems are often portrayed as evil, and we are conditioned to expect that we can defeat them.
In more realistic AI doom scenarios, there is no hero, no conflict, no hope, and no happy ending.

### Progress has always been (mostly) good

Many of the technologies introduced in our society have been mostly beneficial for humanity.
We have cured diseases, increased our life expectancy, and made our lives more comfortable.
And every time we have done so, there have been people who opposed these innovations and warned about the dangers.
The Luddites destroyed the machines that were taking their jobs, and people were afraid of the first trains and cars.
These people have always been wrong.

### We don't like to think about our death

The human mind does not like receiving bad news, and it has various coping mechanisms to deal with it.
The most important ones when talking about x-risk are [denial](https://en.wikipedia.org/wiki/Cognitive_dissonance) and [compartmentalization](https://en.wikipedia.org/wiki/Compartmentalization_(psychology)).
When it comes to our own death, specifically, we are very prone to denial.
Books have been written about the [Denial of Death](https://en.wikipedia.org/wiki/The_Denial_of_Death).

These coping mechanisms protect us from the pain of having to accept that the world is not as we thought it was.
However, they can also prevent us from adequately responding to a threat.

When you notice someone using these coping mechanisms, try to be empathetic.
They are not doing it on purpose, and they are not stupid.
It is a natural reaction to bad news, and we all do it to some extent.

### Admitting your work is dangerous is hard

For those who have been working on AI capabilities, accepting its dangers is even harder.

Take Yoshua Bengio for example.
Yoshua Bengio has a brilliant mind and is one of the pioneers in AI.
AI safety experts have been warning about the potential dangers of AI for years, but it still took him a long time to take their warnings seriously.
In an [interview](https://youtu.be/0RknkWgd6Ck?t%25253D949), he gave the following explanation:

> "Why didn't I think about it before? Why didn't Geoffrey Hinton think about it before? [...] I believe there's a psychological effect that still may be at play for a lot of people. [...] It's very hard, in terms of your ego and feeling good about what you do, to accept the idea that the thing you've been working on for decades might be actually be very dangerous to humanity. [...] I think that I didn't want to think too much about it, and that's probably the case for others."

## Difficult to understand

The arguments for AI existential risk are often very technical, and we are very prone to anthropomorphizing AI systems.

### AI alignment is surprisingly hard

People may intuitively feel like they could solve the AI alignment problem.
Why not add a [stop button](https://www.youtube.com/watch?v=3TYT1QfdfsM&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40&index=10)? Why not [raise the AI like a child](https://www.youtube.com/watch?v=eaYIU6YXr3w)? Why not [Asimov's three laws](https://www.youtube.com/watch?v=7PKx3kS7f4A)?
Contrary to most types of technical problems, people will have an opinion on how to solve AI alignment and underestimate the difficulty of the problem.
Understanding the actual difficulty of it takes a lot of time and effort.

### We anthropomorphize

We see faces in clouds, and we see human-like qualities in AI systems.
Millions of years of evolution have made us highly social creatures, but these instincts are not always helpful.
We tend to think of AIs as having human-like goals and motivations, being able to feel emotions, and having a sense of morality.
We tend to expect a very intelligent AI to also be very wise and kind.
This is one of the reasons why people intuitively think that AI alignment is easy, and why the [Orthogonality thesis](https://www.youtube.com/watch?v=hEUO6pjwFOo) can be so counter-intuitive.

### AI safety uses complex language

The AI safety field consists mostly of a small group of (smart) people who have developed their own jargon.
Reading LessWrong posts can feel like reading a foreign language.
Many posts assume the reader is already familiar with mathematical concepts, various technical concepts, and the jargon of the field.

## Difficult to act on

Even if people do understand the arguments, it is still difficult to act on them.
The impact is too large, we have coping mechanisms that downplay the risks, and if we do feel the gravity of the situation, we can feel powerless.

### Scope insensitivity

> "A single death is a tragedy; a million deaths is a statistic." - Joseph Stalin

Scope insensitivity is the human tendency to underestimate the impact of large numbers.
We do not care 10x as much about 1000 people dying as we do about 100 people dying.
Existential risk means the death of all 8 billion people on Earth (not counting their descendants).

Even if there is a 1% chance of this happening, it is still a very big deal.
Rationally, we should consider this 1% chance of 8 billion deaths just as important as the certain death of 80 million people.

If someone feels like the end of the world isn't that big of a deal (you would be surprised by how often this happens), you can try to make things more personal.
Humanity isn't just some abstract concept, they are your friends, your family, and yourself.
All the people you care about will die.

### Coping mechanisms (preventing action)

The same coping mechanisms that prevent people from _believing_ in existential risk also prevent them from _acting_ on it.
If you're in denial or compartmentalizing, you won't feel the need to do anything about it.

### Stress and anxiety

As I'm writing this, I'm feeling stressed and anxious.
It's not just because I fear the end of the world, but also because I feel like I have to do something about it.
There's a lot of pressure to act, and it can be overwhelming.
This stress can be a good motivator, but it can also be paralyzing.

### Hopelessness and powerlessness

When people do take the topic seriously, and the full gravity of the situation sinks in, they can feel like all hope is lost.
It can feel like a cancer diagnosis: you are going to die sooner than you wanted, and there is nothing you can do about it.
The problem is too big to tackle, and you are too small.
Most people are no AI safety experts or experienced lobbyists, so how can they possibly do anything about it?

## But you can help!

There are many [things that you can do](/action).
Writing a letter, going to a protest, donating some money or joining a community is not that hard!
And these actions have a real impact.
Even when facing the end of the world, there can still be hope and very rewarding work to do.
