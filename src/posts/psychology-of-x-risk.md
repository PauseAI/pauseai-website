---
title: The difficult psychology of existential risk
description: Thinking about the end of the world is hard.
---

The psychology of existential risk is a topic that is not often discussed, compared to the technical aspects of AI safety.
However, one might argue that it is perhaps just as important.
After all, if we can't get people to take the topic seriously and act on it, we won't be able to do anything about it.

It's hard to bring up, hard to believe, hard to understand, and hard to act on.
Having a better understanding of _why_ these things are so difficult can help us be more convincing, effective and empathetic.

## Difficult to bring up

X-risk is a difficult topic to bring up in conversation, especially if you are a politician.
People may think you are crazy, and you may not fully understand the topic yourself.

### Fear of being ridiculed

The first reaction to existential risk is often just laughing it off.
We've seen this happen on camera in the White House, for example.
This has the effect of making the one bringing up the topic feel like a fool.
This in turn makes it harder to bring up the topic again, as others will fear being ridiculed for bringing it up.

### A complex topic to argue about

The technical difficulty of AI safety makes it a daunting topic for most people.
It takes time and effort to understand the arguments, and if you don't feel like you deeply understand them, it's hard to argue about them.
As a politician, you don't want to be caught saying something that is wrong, so you might avoid the topic altogether.

## Difficult to believe

Even if there is a discussion on existential risk, it is difficult to convince people that it is a real problem.
There are various reasons why most will instantly dismiss the idea.

### The end of the world has never happened

Seeing is believing.
That's a problem for existential risk, because we will never be able to see it before it is too late.

On the other hand, we have tons of evidence for the contrary.
The end of times has been predicted by many people, and every single one of them has been wrong so far.

So when people hear about existential risk, they will think it is just another one of those doomsday cult predictions.
Try to have an understanding of this point of view, and don't be too hard on people who think this way.
They probably haven't been shown the same information as you have.

### Fiction has conditioned us to expect a happy ending

Most of what we know about existential risk comes from fiction.
This probably does not help, because fictional stories are not written to be realistic: they are written to be entertaining.

In fiction there is often a hero, conflict, hope, and finally a happy ending.
We are conditioned to expect a struggle that we can fight and win.
Very intelligent AI systems are often portrayed as evil, and we are conditioned to expect that we can defeat them.
In more realistic AI doom scenarios, there is no hero, no conflict, no hope, and no happy ending.

### Progress has always been (mostly) good

Many of the technologies introduced in our society have been beneficial.
We have cured diseases, increased our life expectancy, and made our lives more comfortable.
And every time we have done so, there have been people who opposed these innovations and warned about the dangers.
These people have always been wrong.

### Coping mechanisms

The human mind does not like receiving bad news, and it has various coping mechanisms to deal with it.
The most important ones when talking about x-risk are [denial](https://en.wikipedia.org/wiki/Cognitive_dissonance) and [compartmentalization](https://en.wikipedia.org/wiki/Compartmentalization_(psychology)).
Both of these are ways to avoid having to internalize the bad news.

These coping mechanisms protect us from the pain of having to accept that the world is not as we thought it was.
However, they can also prevent us from adequately responding to a threat.

When you notice someone using these coping mechanisms, try to be empathetic.
They are not doing it on purpose, and they are not stupid.
It is a natural reaction to bad news, and we all do it to some extent.

## Difficult to understand

### AI alignment is surprisingly hard

People may intuitively feel like they could solve the AI alignment problem.
Why not add a stop button? Why not raise the AI like a child? Why not make it value human life? Why not Asimov's laws?
Contrary to most types of technical problems, people will have an opinion on how to solve it, and reliably underestimate the difficulty of the problem.
Understanding the actual difficulty of it takes a lot of time and effort.

### Anthropomorphism

We see faces in clouds, and we see human-like qualities in AI systems.
Millions of years of evolution have made us highly social creatures, but these instincts are not always helpful.
We tend to think of AIs as having human-like goals and motivations, being able to feel emotions, and having a sense of morality.
This is one of the reasons why people think that AI alignment is easy, and the [Orthogonality thesis](https://www.youtube.com/watch?v=hEUO6pjwFOo) can be so counter-intuitive.

### Jargon

The AI safety field consists mostly of a small group of (smart) people who have developed their own jargon.
Reading LessWrong posts can feel like reading a foreign language.
Many posts assume the reader is already familiar with mathematical concepts, various technical concepts, and the jargon of the field.

## Difficult to act on

### Scope insensitivity

> "A single death is a tragedy; a million deaths is a statistic." - Joseph Stalin

Scope insensitivity is the human tendency to underestimate the impact of large numbers.
We do not care 10x as much about 1000 people dying as we do about 100 people dying.
Existential risk is about the death of all 8 billion people on earth (not counting their descendants).

Even if there is a 1% chance of this happening, it is still a very big deal.
Rationally, we should consider this 1% chance of 8 billion deaths just as important as the certain death of 80 million people.

If someone feels like the end of the world isn't that big of a deal (you would be surprised by how often this happens), you can try to make things more personal.
Humananity isn't just some abstract concept, they are your friends, your family, and yourself.
All the people you care about will die.

### Coping mechanisms (preventing action)

The same coping mechanisms that prevent people from _believing_ in existential risk also prevent them from _acting_ on it.
If you're in denial or compartmentalizing, you won't feel the need to do anything about it.

### Stress and anxiety

As I'm writing this, I'm feeling stressed and anxious.
It's not just because I fear the end of the world, but also because I feel like I have to do something about it.
There's a lot of pressure to act, and it can be overwhelming.
This stress can be a good motivator, but it can also be paralyzing.
Keep in mind

### Hopelessness and powerlessness

When people do take the topic seriously, and the full gravity of the situation sinks in, they often feel like all hope is lost.
It can feel like a cancer diagnosis: you are going to die sooner than you wanted, and there is nothing you can do about it.
The problem is too big to tackle, and the person is too small.
Most people are no AI safety experts or lobbyists, so how can they possibly do anything about it?

This is where you can help.
You can show them that there are [things they can do](/action).
Writing a letter, going to a protest, donating some money or joining a community are not that hard!
And they have a real impact.
Even when facing the end of the world, there can still be hope and very rewarding work to do.
