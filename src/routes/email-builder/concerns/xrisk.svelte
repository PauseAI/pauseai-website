<p>
	Half of AI researchers <a href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"
		>believe</a
	>
	that there is a 10% or greater chance that the invention of artificial superintelligence will mean
	the end of humanity. Among AI safety scientists, this chance is
	<a
		href="https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results"
		>estimated</a
	>
	to be an average of 30%. Notable examples of individuals sounding the alarm are Prof.
	<a href="https://www.youtube.com/watch?v=Y6Sgp7y178k">Geoffrey Hinton</a>
	and Prof.
	<a href="https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/">Yoshua Bengio</a>
	, both Turing-award winners and pioneers of the deep learning methods that are currently achieving
	the most success. The existential risk of AI has been acknowledged by
	<a href="https://www.safe.ai/statement-on-ai-risk">hundreds of scientists</a>, the
	<a href="https://twitter.com/gcolbourn/status/1669104216307015680">UN</a>, the US and recently the
	<a href="https://twitter.com/EU_Commission/status/1702295053668946148">EU</a>. The latest report
	on “Generatieve AI” by Rathenau also highlights these risks.
</p>

<p>
	To make a long story short: we don't know how to align AI with the complex goals and values that
	humans have. When a superintelligent system is realised, there is a significant risk that one of
	these instances will pursue a misaligned goal without us being able to stop it. And even if such a
	superhuman AI will remain under human control, the one wielding such a power could use this to
	drastically, irreversibly change the world. Such an AI could be used to develop new technologies
	and weapons, manipulate masses of people or topple governments.
</p>
