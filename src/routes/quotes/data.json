[
  {
    "background": "cais_statement",
    "text": "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.",
    "author": "Statement on AI risk,",
    "author_description": "signed by hundreds of AI experts and other notable figures. Including 3 of the most cited scientists ever: Bengio, Hinton & Illya, and the 3 CEOs of the top 3 AI companies: Altman, Hassabis & Amodei.",
    "color": "white",
    "padding": "0rem"
  },
  {
    "background": "hawking",
    "text": "The development of full artificial intelligence could spell the end of the human race.",
    "author": "Stephen Hawking",
    "author_description": "Theoretical Physicist and Cosmologist",
    "color": "white"
  },
  {
    "background": "sunak",
    "text": "Get this wrong, and AI could make it easier to build chemical or biological weapons. Terrorist groups could use AI to spread fear and destruction on an even greater scale. [...] And in the most unlikely but extreme cases, there is even the risk that humanity could lose control of AI completely through the kind of AI sometimes referred to as ‘super intelligence’. Indeed, to quote the statement made earlier this year by hundreds of the world’s leading AI experts: “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war”.",
    "author": "Rishi Sunak",
    "author_description": "United Kingdom's Prime Minister",
    "color":"black",
    "padding": "0.3rem"
  },
  {
    "background": "trump",
    "text": "[...] the other thing that I think is **maybe the most dangerous thing out there** of anything, because there’s no real solution — the AI, as they call it.",
    "author": "Donald Trump",
    "author_description": "45th President of the United States",
    "color": "white",
    "padding": "4rem"
  },
  {
    "background": "biden",
    "text": "Social media has shown us the harm that powerful technology can do without the right safeguards in place. [...] we must be clear-eyed and vigilant about the threats emerging — of emerging technologies that can pose — don’t have to, but can pose — to our democracy and our values. [...] Realizing the promise of AI by managing the risk is going to require some new laws, regulations, and oversight.",
    "author": "Joe Biden",
    "author_description": "46th President of the United States",
    "color": "white"
  },
  {
    "background": "putin",
    "text": "Artificial intelligence is the future, not only for Russia, but for all humankind. It comes with colossal opportunities, but also threats that are difficult to predict. Whoever becomes the leader in this sphere will become the ruler of the world.[...] If we become leaders in this area, we will share this know-how with [the] entire world, the same way we share our nuclear technologies today.",
    "author": "Vladimir Putin",
    "author_description": "President of Russia",
    "padding": "3rem",
    "color": "white"
  },
  {
    "background": "qiang",
    "text":"AI must be guided in a direction that is conducive to the progress of humanity. So there should be a red line in AI development, a red line that must not be crossed. [...] It should not just benefit only a small group of people, but benefit the overwhelming majority of mankind. [...] It is essential that we work together and coordinate with each other.",
    "author": "Li Qiang",
    "author_description": "China's Head of Government",
    "padding": "3rem",
    "color": "white"
  },
  {
    "background": "leyen",
    "text": "[We] should not underestimate the real threats coming from AI. [Fully quoted the above statement on the risk of extinction.] [...] It is moving faster than even its developers anticipated. [...] We have a narrowing window of opportunity to guide this technology responsibly.",
    "author": "Ursula von der Leyen",
    "author_description": "Head of the executive branch of the European Union",
    "color": "white"
  },
  {
    "background": "guterres",
    "text": "AI poses a long-term global risk. Even its own designers have no idea where their breakthrough may lead. I urge [the UN Security Council] to approach this technology with a sense of urgency. Unforeseen consequences of some AI-enabled systems could create security risks by accident. Generative AI has enormous potential for good and evil at scale. Its creators themselves have warned that much bigger, potentially catastrophic and existential risks lie ahead. Without action to address these risks, we are derelict in our responsibilities to present and future generations.",
    "author:": "António Guterres",
    "author_description": "Chief Executive Officer of the United Nations",
    "color": "black",
    "padding": "1rem"
  },
  {
    "background": "pope",
    "text": "I urge the global community of nations to work together in order to adopt a binding international treaty that regulates the development and use of artificial intelligence in its many forms. [...] [AI] may pose a risk to our survival and endanger our common home",
    "author": "Pope Francis",
    "author_description": "Head of the Catholic Church",
    "color": "white",
    "padding": "2rem"
  },
  {
    "background": "kamala",
    "text": "[...] just as AI has the potential to do profound good, it also has the potential to cause profound harm. From AI-enabled cyberattacks at a scale beyond anything we have seen before to AI-formulated bio-weapons that could endanger the lives of millions, these threats are often referred to as the “existential threats of AI” because, of course, they could endanger the very existence of humanity. These threats, without question, are profound, and they demand global action.",
    "author": "Kamala Harris",
    "author_description": "Vice President of the United States",
    "color": "white",
    "padding": "2.4rem"
  },
  {
    "background": "jun",
    "text": "The potential impact of AI might exceed human cognitive boundaries. To ensure that this technology always benefits humanity, we must regulate the development of AI and prevent this technology from turning into a runaway wild horse. [...] We need to strengthen the detection and evaluation of the entire lifecycle of AI, ensuring that mankind has the ability to press the pause button at critical moments",
    "author": "Zhang Jun",
    "author_description": "China's Ambassador of the United Nations",
    "color": "white",
    "padding": "3.5rem"
  },
  {
    "background": "turing",
    "text": "It seems probable that once the machine thinking method had started, it would not take long to outstrip our feeble powers… They would be able to converse with each other to sharpen their wits. **At some stage therefore, we should have to expect the machines to take control.**",
    "author": "Alan Turing",
    "author_description": "Father of Computer Science and Artificial Intelligence",
    "notice": "© National Portrait Gallery, London"
  },
  {
    "background": "musk",
    "text": "AI is a rare case where I think we need to be proactive in regulation than be reactive [...] I think that [digital super intelligence] is the single biggest existential crisis that we face and the most pressing one. It needs to be a public body that has insight and then oversight to confirm that everyone is developing AI safely. This is extremely important. I think the danger of AI is much greater than the danger of nuclear warheads by a lot and nobody would suggest that we allow anyone to build nuclear warheads if they want. That would be insane [...] This is insane.",
    "author": "Elon Musk",
    "author_description": "Founder/ Co-Founder of OpenAI, Neuralink, SpaceX, xAI, PayPal, CEO of Tesla, CTO of X",
    "color": "white",
    "padding": "0.1rem"
  },
  {
    "background": "gates",
    "text": "Superintelligent AIs are in our future. [...] There’s the possibility that AIs will run out of control. [Possibly,] a machine could decide that humans are a threat, conclude that its interests are different from ours, or simply stop caring about us.",
    "author": "Bill Gates",
    "author_description": "Co-Founder of Microsoft",
    "color": "black"
  },
  {
    "background": "hinton",
    "text": "If you want a system to be effective, you need to give it the ability to create its own subgoals. Now, the problem is, there’s a very general subgoal that helps with almost all goals: get more control. The research question is: how do you prevent them from ever wanting to take control? And nobody knows the answer. [...] If you take the existential risk seriously, as I now do, **it might be quite sensible to just stop developing these things any further.**",
    "author": "Geoffrey Hinton",
    "author_description": "2018 Turing Award Recipient, Godfather of Deep Learning, 1st/3rd most cited scientist ever, resigned from Google in 2023 due to ethical concerns",
    "padding": "1rem"
  },
  {
    "background": "bengio",
    "text": "It's very hard, in terms of your ego and feeling good about what you do, to accept the idea that the thing you've been working on for decades **might actually be very dangerous to humanity**... I think that I didn't want to think too much about it, and that's probably the case for others.",
    "author": "Yoshua Bengio",
    "author_description": "2018 Turing Award Recipient, Godfather of Deep Learning, 2nd/3rd most cited scientist ever",
    "color": "white",
    "padding": "3.6rem"
  },
  {
    "background": "tallinn",
    "text": "How [LLMs] work is that you summon this \"mind\" from the \"mind space\" using your data, a lot of compute and a lot of money. Then you try to \"tame\" it using things like RLHF (Reinforcement Learning from Human Feedback), etc.  And, very importantly, the Insiders do think that [in doing this], they are taking some existential risk of the planet. **One thing that a pause achieves is that we will not push the Frontier, in terms of risky pre-training experiments.**",
    "author": "Jaan Tallinn",
    "author_description": "Co-Founder of Skype, Kazaa, Future of Life Institute",
    "color": "black",
    "padding": "1rem"
  },
  {
    "background": "good",
    "text": "An ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. **Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control**.",
    "author": "I. J. Good",
    "author_description": "Cryptologist at Bletchley Park with Alan Turing",
    "padding": "2.8rem",
    "color": "white"
  },
  {
    "background": "butler",
    "text": "Day by day, however, the machines are gaining ground upon us; day by day we are becoming more subservient to them... That the **time will come when the machines will hold the real supremacy over the world and its inhabitants** is what no person of a truly philosophic mind can for a moment question.",
    "author": "Samuel Butler",
    "author_description": "Novelist, \"Darwin among the Machines\", 1863",
    "padding": "3.8rem",
    "color": "white"
  },
  {
    "background": "yudkowsky",
    "text": "I do not expect something actually smart to attack us with marching robot armies with glowing red eyes where there could be a fun movie about us fighting them. **I expect an actually smarter and uncaring entity will figure out strategies and technologies that can kill us quickly and reliably and then kill us.**",
    "author": "Eliezer Yudkowsky",
    "author_description": "Co-Founder of the Machine Intelligence Research Institute",
    "color": "white",
    "padding": "3.6rem"
  },
    {
    "background": "christiano",
    "text": "Everyone should be very unhappy if you built a bunch of AIS who are like, 'I really hate these humans but they will murder me if I don't do what they want'. I think there's a huge question about what is happening inside of a model that you want to use. **This is the kind of thing that is both horrifying from a safety perspective and also a moral perspective.**",
    "author": "Paul Christiano",
    "author_description": "Founder of the Alignment Research Center and Former Head of the Alignment Team at OpenAI",
    "color": "white",
    "padding": "3.6rem"
  }
]
