{
	"$schema": "https://inlang.com/schema/inlang-message-format",
	"header__instructions": "All translations prefixed with 'header_' should be as short as possible to fit the layout. Try to keep them at a single word while still being useful as links to the pages.",
	"header_action__instructions": "German: Handeln",
	"header_action": "Act",
	"header_donate": "Donate",
	"header_events__instructions": "In some languages, 'calendar' or 'dates' might be better wordings for meeting the goal of a short translation. (German: Termine)",
	"header_events": "Events",
	"header_faq": "FAQ",
	"header_join__instructions": "In some languages 'join in' might be more appropriate if the translation doesn't consist of two words.",
	"header_join": "Join",
	"header_learn": "Learn",
	"header_proposal": "Proposal",
	"home_action_c2a": "Take action",
	"home_action_content": "Too few people are well-informed about the potential risks of AI. Inform others, and help stop this race to the bottom.",
	"home_action_title": "<u>YOU</u> CAN HELP",
	"home_hero__instructions": "This is directed at the reader, translate it informally.",
	"home_hero": "DON'T LET AI COMPANIES GAMBLE WITH OUR FUTURE",
	"home_proposal_c2a": "Read the proposal",
	"home_proposal_content__instructions": "'Stop' isn't addressed at the reader.",
	"home_proposal_content": "Stop the development of the most powerful general AI systems until we know how to make them safe. This needs to happen on an international level, and it needs to happen soon.",
	"home_proposal_title": "We need a <u>pause</u>",
	"home_quotes_all": "See all quotes",
	"home_quotes_bengio_text": "Rogue AI may be dangerous for the whole of humanity. Banning powerful AI systems (say beyond the abilities of GPT-4) that are given autonomy and agency would be a good start.",
	"home_quotes_bengio_title": "AI Turing Award winner",
	"home_quotes_cais_text": "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.",
	"home_quotes_cais_author": "Statement on AI Risk",
	"home_quotes_cais_title": "Signed by hundreds of experts, including the top AI labs and scientists",
	"home_quotes_hawking_text": "The development of full artificial intelligence could spell the end of the human race.",
	"home_quotes_hawking_title": "Theoretical physicist and cosmologist",
	"home_quotes_hinton_text": "If you take the existential risk seriously, as I now do, it might be quite sensible to just stop developing these things any further.",
	"home_quotes_hinton_title": "Nobel Prize winner & \"Godfather of AI\"",
	"home_quotes_russell_text": "If we pursue [our current approach], then we will eventually lose control over the machines",
	"home_quotes_russell_title": "Writer of the AI textbook",
	"home_quotes_turing_text": "... we should have to expect the machines to take control.",
	"home_quotes_turing_title": "Inventor of the modern computer",
	"home_risks_c2a": "Read about the risks",
	"home_risks_content": "AI can have amazing benefits, but it could also erode our democracy, destabilize our economy and could be used to create powerful cyber weapons.",
	"home_risks_title": "We risk <u>losing control</u>",
	"home_stats_2025": "chance we'll reach AGI in 2025",
	"home_stats_alignment": "of AI scientists believe the alignment problem is real & important",
	"home_stats_citizens": "of citizens want AI to be slowed down by our governments",
	"home_urgency_c2a": "How long do we have?",
	"home_urgency_content": "In 2020, experts thought we had more than 35 years until AGI. Recent breakthroughs show we might be almost there. Superintelligence could be one innovation away, so we should tread carefully.",
	"home_urgency_title": "WE NEED TO ACT <u>RIGHT NOW</u>",
	"home_xrisk_c2a": "How and why AI could kill us",
	"home_xrisk_content": "Many AI labs and experts agree: AI could end humanity.",
	"home_xrisk_title": "We risk <u>human extinction</u>",
	"simpletoc_heading": "Table of contents",
	"footer_join": "Join PauseAI >",
	"footer_info": "Info",
	"footer_info_faq": "FAQ",
	"footer_info_proposal": "Proposal",
	"footer_info_learn": "Learn",
	"footer_info_press": "Press / Media",
	"footer_info_people": "People",
	"footer_info_teams": "Teams",
	"footer_info_partnerships": "Partnerships",
	"footer_info_privacy": "Privacy policy",
	"footer_info_legal": "Legal Info",
	"footer_info_legal_foundation": "Stichting PauseAI",
	"footer_info_legal_kvk": "(kvk 92951031)",
	"footer_risks": "Risks",
	"footer_risks_overview": "Risks overview",
	"footer_risks_outcomes": "AI Outcomes",
	"footer_risks_xrisk": "Existential risk",
	"footer_risks_psychology": "Psychology of x-risk",
	"footer_risks_takeover": "AI takeover",
	"footer_risks_cybersecurity": "Cybersecurity",
	"footer_risks_capabilities": "Dangerous capabilities",
	"footer_risks_sota": "State of the art",
	"footer_risks_urgency": "Urgency",
	"footer_action": "Take Action",
	"footer_action_join": "Join PauseAI",
	"footer_action_help": "How you can help",
	"footer_action_communities": "Local communities",
	"footer_action_donate": "Donate",
	"footer_action_merchandise": "Merchandise",
	"footer_action_events": "Events",
	"footer_action_vacancies": "Vacancies",
	"footer_action_email": "Email builder",
	"footer_action_lobby": "Lobby tips",
	"footer_other": "Other",
	"footer_other_pages": "All pages",
	"footer_other_rss": "RSS",
	"footer_other_license": "License: CC-BY 4.0",
	"footer_other_feedback": "Submit feedback",
	"footer_other_edit": "Edit original English content",
	"footer_other_l10n": "Suggest translation improvement"
}
