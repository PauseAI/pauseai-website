{
	"$schema": "https://inlang.com/schema/inlang-message-format",
	"header__instructions": "All translations prefixed with 'header_' should be as short as possible to fit the layout. Try to keep them at a single word while still being useful as links to the pages.",
	"header_action__instructions": "German: Handeln",
	"header_action": "Act",
	"header_donate": "Donate",
	"header_events__instructions": "In some languages, 'calendar' or 'dates' might be better wordings for meeting the goal of a short translation. (German: Termine)",
	"header_events": "Events",
	"header_faq": "FAQ",
	"header_join__instructions": "In some languages 'join in' might be more appropriate if the translation doesn't consist of two words.",
	"header_join": "Join",
	"header_learn": "Learn",
	"header_proposal": "Proposal",
	"home_action_c2a": "Take action",
	"home_action_content": "Recent breakthroughs on AI have started a race to the bottom. Too few people are well-informed about the potential risks of AI, so you can make the difference.",
	"home_action_title": "WE NEED TO ACT <u>RIGHT NOW</u>",
	"home_hero__instructions": "This is directed at the reader, translate it informally.",
	"home_hero": "DON'T LET AI COMPANIES GAMBLE WITH OUR FUTURE",
	"home_proposal_c2a": "Read the proposal",
	"home_proposal_content__instructions": "'Stop' isn't addressed at the reader.",
	"home_proposal_content": "Stop the development of the most powerful general AI systems until we know how to make them safe. This needs to happen on an international level, and it needs to happen soon.",
	"home_proposal_title": "We need a <u>pause</u>",
	"home_quotes_all": "See all quotes",
	"home_quotes_bengio_text": "Rogue AI may be dangerous for the whole of humanity. Banning powerful AI systems (say beyond the abilities of GPT-4) that are given autonomy and agency would be a good start.",
	"home_quotes_bengio_title": "AI Turing Award winner",
	"home_quotes_asi_statement_text": "We call for a prohibition on the development of superintelligence, not lifted before there is broad scientific consensus that it will be done safely and controllably, and strong public buy-in",
	"home_quotes_asi_statement_author": "Statement on Superintelligence",
	"home_quotes_asi_statement_title": "110,000+ signatories including AI researchers, political, faith and industry leaders, artists and media celebrities",
	"home_quotes_cais_text": "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.",
	"home_quotes_cais_author": "Statement on AI Risk",
	"home_quotes_cais_title": "Signed by hundreds of experts, including the top AI labs and scientists",
	"home_quotes_hawking_text": "The development of full artificial intelligence could spell the end of the human race.",
	"home_quotes_hawking_title": "Theoretical physicist and cosmologist",
	"home_quotes_hinton_text": "If you take the existential risk seriously, as I now do, it might be quite sensible to just stop developing these things any further.",
	"home_quotes_hinton_title": "Nobel Prize winner & \"Godfather of AI\"",
	"home_quotes_russell_text": "If we pursue [our current approach], then we will eventually lose control over the machines.",
	"home_quotes_russell_title": "Writer of the AI textbook",
	"home_quotes_turing_text": "... we should have to expect the machines to take control.",
	"home_quotes_turing_title": "Inventor of the modern computer",
	"home_risks_c2a": "Read about the risks",
	"home_risks_content": "AI can have amazing benefits, but it could also erode our democracy, destabilize our economy and could be used to create powerful cyber weapons. Also many AI labs and experts agree: AI could end humanity.",
	"home_risks_title": "We risk <u>losing control</u>",
	"home_stats_alignment": "of AI scientists believe the alignment problem is real & important",
	"home_stats_citizens": "of citizens want AI to be slowed down by our governments",
	"home_urgency_c2a": "How long do we have?",
	"home_urgency_content": "In 2020, experts thought we had more than 35 years until AGI. Recent breakthroughs show we might be almost there. Superintelligence could be one innovation away, so we should tread carefully.",
	"home_urgency_title": "WE NEED TO ACT <u>RIGHT NOW</u>",
	"home_xrisk_c2a": "How and why AI could kill us",
	"home_xrisk_content": "Many AI labs and experts agree: AI could end humanity.",
	"home_xrisk_title": "We risk <u>human extinction</u>",
	"simpletoc_heading": "Table of contents",
	"footer_join": "Join PauseAI >",
	"footer_info": "Info",
	"footer_info_faq": "FAQ",
	"footer_info_proposal": "Proposal",
	"footer_info_learn": "Learn",
	"footer_info_press": "Press / Media",
	"footer_info_people": "People",
	"footer_info_teams": "Teams",
	"footer_info_partnerships": "Partnerships",
	"footer_info_privacy": "Privacy policy",
	"footer_info_legal": "Legal Info",
	"footer_info_legal_foundation": "Stichting PauseAI",
	"footer_info_legal_kvk": "(kvk 92951031)",
	"footer_risks": "Risks",
	"footer_risks_overview": "Risks overview",
	"footer_risks_outcomes": "AI Outcomes",
	"footer_risks_xrisk": "Existential risk",
	"footer_risks_psychology": "Psychology of x-risk",
	"footer_risks_takeover": "AI takeover",
	"footer_risks_cybersecurity": "Cybersecurity",
	"footer_risks_capabilities": "Dangerous capabilities",
	"footer_risks_sota": "State of the art",
	"footer_risks_urgency": "Urgency",
	"footer_action": "Take Action",
	"footer_action_join": "Join PauseAI",
	"footer_action_help": "How you can help",
	"footer_action_communities": "Local communities",
	"footer_action_donate": "Donate",
	"footer_action_merchandise": "Merchandise",
	"footer_action_events": "Events",
	"footer_action_vacancies": "Vacancies",
	"footer_action_email": "Email builder",
	"footer_action_lobby": "Lobby tips",
	"footer_other": "Other",
	"footer_other_pages": "All pages",
	"footer_other_rss": "RSS",
	"footer_other_license": "License: CC-BY 4.0",
	"footer_other_feedback": "Submit feedback",
	"footer_other_edit": "Edit original English content",
	"footer_other_l10n": "Suggest translation improvement",
	"newsletter_heading": "Subscribe to our newsletter",
	"newsletter_description": "Stay updated on our efforts to advocate for AI safety.",
	"newsletter_email_placeholder": "Your email",
	"newsletter_button": "Subscribe",
	"newsletter_success": "Done. Expect email from Substack.",
	"uk_email_mp_title": "Email Your MP",
	"uk_email_mp_description": "Contact your Member of Parliament about AI safety",
	"uk_email_mp_uk_only_notice": "<b>This tool is for the UK only.</b> For other countries, use the [global email builder](/email-builder).",
	"learn_risks": "[Risks](/risks). A summary of the risks of AI.",
	"learn_xrisk": "[X-risk](/xrisk). Why AI is an existential risk.",
	"learn_ai_takeover": "[Takeover](/ai-takeover). How AI could take over the world.",
	"learn_quotes": "[Quotes](/quotes). Quotes on AI risks and governance.",
	"learn_feasibility": "[Feasibility of a Pause](/feasibility). The feasibility of a pause in AI development.",
	"learn_building_the_pause_button": "[Building the Pause button](/building-the-pause-button). What it takes to pause AI.",
	"learn_faq": "[FAQ](/faq). Commonly asked questions about AI safety and PauseAI.",
	"learn_action": "[Action](/action). What you can do to help (with links to many action-related guides)",
	"footer_info_about": "About Us"
}
